{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T18:21:06.214825Z","iopub.execute_input":"2021-07-10T18:21:06.215333Z","iopub.status.idle":"2021-07-10T18:21:06.22544Z","shell.execute_reply.started":"2021-07-10T18:21:06.215287Z","shell.execute_reply":"2021-07-10T18:21:06.224137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lire le datset de (Valenzuela et al, 2015)\ndataset=pd.read_csv('../input/mydata/ValenzuelaAnnotations.csv')\n\n#Visualiser notre dataset\ndataset.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:21:07.574979Z","iopub.execute_input":"2021-07-10T18:21:07.575383Z","iopub.status.idle":"2021-07-10T18:21:07.749694Z","shell.execute_reply.started":"2021-07-10T18:21:07.575351Z","shell.execute_reply":"2021-07-10T18:21:07.748593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Récupérer les codes d'articles présents dans le dataset (les columns Paper et Cited-by)\nunion = pd.concat([dataset['Paper'], dataset['Cited-by']])\nunion=union.drop_duplicates()\n#print(union.values)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:32:41.384999Z","iopub.execute_input":"2021-07-10T19:32:41.385398Z","iopub.status.idle":"2021-07-10T19:32:41.397042Z","shell.execute_reply.started":"2021-07-10T19:32:41.385366Z","shell.execute_reply":"2021-07-10T19:32:41.39586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# web scrapping pour recuperer les liens des fichiers PDF pour les telecharger par la suite\n\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Récupérer l'URL, l'auteur et l'année de l'article à partir de son code de l'anthologie ACL\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n}\n\nurl=\"https://www.aclweb.org/anthology/\"\nr1 = requests.get(url, headers=headers)\ncoverpage = r1.content\nsoup1 = BeautifulSoup(coverpage, 'html.parser')\n#print(str(soup1))\nurl_list =[]\nauthor_list =[]\nyears_list =[]\nfor i in range(len(union.values)):\n    r1 = requests.get(url+str(union.values[i]), headers=headers)\n    coverpage = r1.content\n    soup1 = BeautifulSoup(coverpage, 'html.parser')\n    #z = re.findall(r'url = \\\"(.+)\\\"',str(soup1))\n    urls=re.findall(r'url =( &quot;| \\\")(.+)(&quot|\\\")',str(soup1))\n    if len(urls)!=0: url_list.append(urls[0][1])\n    else: print(union.values[[i]])\n    authors=re.findall(r'<meta content=\\\"([a-zA-Zéłńčøš\\. ]+)\\\" name=\"citation_author\"/>',str(soup1))\n    if len(authors)!=0:author_list.append(authors)\n    else: print(union.values[[i]])\n    year = re.findall(r'year =( &quot;| \\\")([\\d]+)(&quot;|\\\")', str(soup1))\n    if len(year)!=0: years_list.append(year[0][1])\n    else: print(union.values[[i]])","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:29:15.952453Z","iopub.execute_input":"2021-07-10T18:29:15.95293Z","iopub.status.idle":"2021-07-10T18:42:27.410663Z","shell.execute_reply.started":"2021-07-10T18:29:15.95289Z","shell.execute_reply":"2021-07-10T18:42:27.409843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#L'article dont le code a été affiché précédemment [W07-2058] n'a pas pu être accédée \n#car le fichier n'existe pas dans l'anthologie ACL","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:42:55.592617Z","iopub.execute_input":"2021-07-10T18:42:55.59328Z","iopub.status.idle":"2021-07-10T18:42:55.597293Z","shell.execute_reply.started":"2021-07-10T18:42:55.593221Z","shell.execute_reply":"2021-07-10T18:42:55.596242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Afficher nombre d'articles unique auxquels on a pu accéder\nprint(len(url_list))\nprint(len(author_list))\nprint(len(years_list))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:42:58.331088Z","iopub.execute_input":"2021-07-10T18:42:58.331479Z","iopub.status.idle":"2021-07-10T18:42:58.338141Z","shell.execute_reply.started":"2021-07-10T18:42:58.331448Z","shell.execute_reply":"2021-07-10T18:42:58.336929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Télécharger les articles  auxquels on a pu accéder précédemment\nimport urllib\n\nfor url in url_list:\n    if '.pdf' not in url:\n          url=url+'.pdf'\n    try:\n      urllib.request.urlretrieve(url, url[-12:])\n    except :\n      print(\"Variable \"+ str(url) +\" is not defined\")\n      pass","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:43:02.13611Z","iopub.execute_input":"2021-07-10T18:43:02.136537Z","iopub.status.idle":"2021-07-10T18:54:15.792497Z","shell.execute_reply.started":"2021-07-10T18:43:02.136494Z","shell.execute_reply":"2021-07-10T18:54:15.791677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Fonction pour récupérer l'auteur et l'année de l'article\ndef get_authors_year(code):\n\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n    }\n    # Récupérer les articles de l'anthologie ACL\n    url=\"https://www.aclweb.org/anthology/\"\n    r1 = requests.get(url+str(code), headers=headers)\n    coverpage = r1.content\n    soup1 = BeautifulSoup(coverpage, 'html.parser')\n    years_list =[]\n    author_list =[]\n    soup1 = BeautifulSoup(coverpage, 'html.parser')\n    authors=re.findall(r'<meta content=\\\"([a-zA-Zéłńčøš\\. ]+)\\\" name=\"citation_author\"/>',str(soup1))\n    \n    if len(authors)!=0:\n        author_list.append(authors)\n    else: \n        print('author non found')\n    \n    year = re.findall(r'year =( &quot;| \\\")([\\d]+)(&quot;|\\\")', str(soup1))\n    if len(year)!=0: \n        years_list.append(year[0][1])\n    else: \n        print('year non found')\n\n\n    return author_list[0],years_list[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:57.074701Z","iopub.execute_input":"2021-07-10T19:36:57.075201Z","iopub.status.idle":"2021-07-10T19:36:57.085169Z","shell.execute_reply.started":"2021-07-10T19:36:57.07516Z","shell.execute_reply":"2021-07-10T19:36:57.083855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exemple d'exécution de la fonction get_authors_year\nprint(((get_authors_year('D07-1009'))))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:59.658105Z","iopub.execute_input":"2021-07-10T19:36:59.65869Z","iopub.status.idle":"2021-07-10T19:37:01.339933Z","shell.execute_reply.started":"2021-07-10T19:36:59.658652Z","shell.execute_reply":"2021-07-10T19:37:01.338817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Après avoir analysé notre ensemble d'articles, on a remarqué que certains fichiers se convertissent mal \n#on a récupéré leurs codes dans la liste suivante\nremoved_files = ['N06-1038','D08-1070','D12-1092','E09-1091','P12-1076','W10-2923','W10-2420','P00-1061','D09-1057','S13-1014','P00-1061',\n                 'P11-2123','P09-2030','E06-1011','W07-2058','L08-1267','L08-1328','L08-1584','E06-1012',\n                 'D09-1057','W11-0112','I08-6004','S13-1021','W13-3616','I08-4013','D09-1016','W11-0135','D07-1019','W08-0504','D13-1159']","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:37:07.521516Z","iopub.execute_input":"2021-07-10T19:37:07.521968Z","iopub.status.idle":"2021-07-10T19:37:07.528117Z","shell.execute_reply.started":"2021-07-10T19:37:07.521932Z","shell.execute_reply":"2021-07-10T19:37:07.526859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Supprimer les articles dont le code est dans la liste removed_files\ndf = dataset[~dataset['Cited-by'].isin(removed_files)]\ndf = df.reset_index(drop=True)\nprint(\"Nombre d'articles apres la suppression : \"+str(len(df)))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:37:10.071011Z","iopub.execute_input":"2021-07-10T19:37:10.071405Z","iopub.status.idle":"2021-07-10T19:37:10.081249Z","shell.execute_reply.started":"2021-07-10T19:37:10.071372Z","shell.execute_reply":"2021-07-10T19:37:10.079774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fonction pour recuperer le contenu des articles sans la partie abstract et sans la partie des references \n# pour garder le contenu essentielle\ndef get_pdf_content(string):\n    \n    if re.search('(\\nintroduction\\s*?\\n)',string,re.I):\n        deb=re.search('(\\nintroduction\\s*?\\n)',string,re.I).span()[0]\n    elif re.search('(\\nabstract\\s*?\\n)',string,re.I): \n        deb=re.search('(\\nabstract\\s*?\\n)',string,re.I).span()[0]  \n    else: \n        deb = 0  \n      \n   \n    if re.search('(\\n\\s*s?ecnerefer\\n)',string[::-1],re.I):\n        fin = len(string) - re.search('(\\n\\s*?s?ecnerefer\\n)',string[::-1],re.I).span()[0]\n    else : \n        fin = len(string)\n\n    return string[deb:fin]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:37:16.040662Z","iopub.execute_input":"2021-07-10T19:37:16.041101Z","iopub.status.idle":"2021-07-10T19:37:16.049256Z","shell.execute_reply.started":"2021-07-10T19:37:16.041067Z","shell.execute_reply":"2021-07-10T19:37:16.048169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pour la conversion des pdf vers text on a utilsé la bibliotheque tika\n#apres on a creer une fonction pour nettoyer le texte du entetes et figures et tableux et les urls\ndef clean_pdf(all_text):\n    #supprimer les tableaux et figures\n    regex_for_num =r'(\\d{1,}[.,]?\\d{1,}%?|\\b[a-zA-Z]{1,3}\\b)'\n    regex_for_urls =r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n\n    list_paragraphs = re.split(r'\\n\\n',all_text)  \n    \n    for i in range(len(list_paragraphs)):\n        if re.search(r'^((Figure|Table) \\d{1,2}[:.]\\s+?[A-Z])',list_paragraphs[i]):\n            #print('haha')\n            list_paragraphs[i]=''\n            list_paragraphs[i-1]=''\n        list_paragraphs[i] = re.sub(regex_for_urls,'',list_paragraphs[i])\n      \n    \n    cleaned = [paragraph for paragraph in list_paragraphs if len(paragraph)>70]\n    \n    txt = '\\n\\n'.join(cleaned)\n    return txt","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:37:20.82618Z","iopub.execute_input":"2021-07-10T19:37:20.826837Z","iopub.status.idle":"2021-07-10T19:37:20.83532Z","shell.execute_reply.started":"2021-07-10T19:37:20.826773Z","shell.execute_reply":"2021-07-10T19:37:20.834096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fonction pour extraire d'un article les citations et leur contexte qui contient 3 phrases \nimport re\nfrom tika import parser\nimport signal\nclass TimeoutException(Exception):   # Custom exception class\n    pass\n\ndef timeout_handler(signum, frame):   # Custom signal handler\n    raise TimeoutException\n\n# Change the behavior of SIGALRM\nsignal.signal(signal.SIGALRM, timeout_handler)\n\n#Foonction pour extraire les citations et leurs contextes (3 phrases)\ndef get_docsss_cita(document_code):    \n    \n    parsed_pdf = parser.from_file(document_code+'.pdf')\n    bana = re.split(r'\\n\\n',clean_pdf(get_pdf_content(parsed_pdf['content'])))\n    \n    sentences = re.findall(r'(([A-Z])(.|\\n)+?(\\.)(?=(\\s*[A-Z]|$)))',clean_pdf(get_pdf_content(parsed_pdf['content'])))\n    sentences = [sen[0] for sen in sentences]\n\n    pattern1 = \"([([](e.g[,.]+\\s)?[A-Z][^\\s0-9]+(page?|-|\\d{2}(\\d{2})?[a-z]?|[;,]|etc|with|&|avec|et|and|al|[A-Z][^\\s0-9]+|\\.|\\s|-\\s+[^A-Z\\s0-9]+)*\\d{2}(\\d{2})?[a-z]?(pages?\\s\\d{1,4}-?\\d{1,4}?|-|\\s|:|,|;|\\d{4}|\\d{2})*(etc|(among|and)\\sothers)?[])])\"\n    pattern2 = \"([A-Z][^\\s0-9,;]+(etc|with|&|avec|et|and|al|[A-Z][^\\s0-9]+|\\.|\\s|-\\s+[^A-Z\\s0-9]+)*?\\(\\d{2}(\\d{2})?[a-z]?(pages?\\s\\d{1,4}-?\\d{1,4}?|-|\\s|:|,|;|\\d{4}|\\d{2})*(etc|(among|and)\\sothers)?\\))\"\n    pattern3 = \"\\[\\d{1,2}\\]\"\n    all_cita = []\n    all_con = []\n\n    for sen_index,sentence in zip(range(len(sentences)),sentences):\n        sen_contex=''\n        signal.alarm(1)\n        try :\n            if sen_index <1:\n                sen_contex =sentences[sen_index]+' '+sentences[sen_index+1]\n            elif sen_index == len(sentences)-1:\n                sen_contex =sentences[sen_index-1]+' '+sentences[sen_index]\n            else:\n                #print(sen_index,len(sentences))\n                sen_contex =sentences[sen_index-1]+' '+sentences[sen_index]+' '+sentences[sen_index+1]    \n            \n            hamida=re.findall(pattern1,re.sub('\\s*\\n\\s*',' ', re.sub('-\\s*\\n\\s*','',sentence)))\n            style_1 = [i[0] for i in hamida]\n            hamid= re.findall(pattern2,re.sub('\\s*\\n\\s*',' ', re.sub('-\\s*\\n\\s*','',sentence)))\n            style_2 = [i[0] for i in hamid]\n            salim= re.findall(pattern3,re.sub('\\s*\\n\\s*',' ', re.sub('-\\s*\\n\\s*','',sentence)))\n            style_3 = [i[0] for i in salim]\n            sen_cita = style_1+style_2+style_3\n            context = [re.sub('\\s*\\n\\s*',' ', re.sub('-\\s*\\n\\s*','',sen_contex)) for i in sen_cita]\n    \n            all_cita = all_cita +sen_cita\n            all_con = all_con + context\n    \n        except TimeoutException:\n            continue # continue the for loop if function A takes more than 5 second\n        else:\n            # Reset the alarm\n            signal.alarm(0)\n    \n        data_frame = pd.DataFrame(list(zip(all_cita, all_con)),\n                                  columns=['Citation','Context'])\n    return data_frame\n\n#voici les citations avec des differents styles et leurs contexte d'un article W11-810\nget_docsss_cita(\"W11-0810\")","metadata":{"execution":{"iopub.status.busy":"2021-07-10T20:46:58.861663Z","iopub.execute_input":"2021-07-10T20:46:58.86206Z","iopub.status.idle":"2021-07-10T20:46:59.513534Z","shell.execute_reply.started":"2021-07-10T20:46:58.862028Z","shell.execute_reply":"2021-07-10T20:46:59.512564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Construction de notre dataset\ncounter = 0\ncitations_list = []\nContext_list = []\nfiles_no_citations = []\nfiles_time_prblm = []\n\n\nfor i in range(len(df)):\n\n    authors,year = get_authors_year(df['Paper'][i])\n    data_frame = get_docsss_cita(df['Cited-by'][i])\n    \n    for j in range(len(data_frame)):\n            if re.search(authors[0].split()[-1] , re.sub('-?\\s?\\n','',data_frame['Citation'][j]),re.I ) and (year in data_frame['Citation'][j] or year[2:4] in data_frame['Citation'][j]) :\n                citations_list.append(data_frame['Citation'][j])\n                Context_list.append(data_frame['Context'][j])\n                #print(j)\n                break\n    else:\n        citations_list.append(None)\n        Context_list.append(None)\n        counter+=1\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:39:13.781791Z","iopub.execute_input":"2021-07-10T19:39:13.782225Z","iopub.status.idle":"2021-07-10T19:58:13.331926Z","shell.execute_reply.started":"2021-07-10T19:39:13.782191Z","shell.execute_reply":"2021-07-10T19:58:13.330902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creer une fonction pour supprimer les noms des entites\nimport spacy\n\ndef remove_NE(text_data):\n    \n    nlp = spacy.load('en_core_web_sm')\n    document = nlp(text_data)\n\n    text_no_namedentities = []\n    ents = [e.text for e in document.ents]\n    \n    for item in document:\n        if item.text in ents:\n            pass\n        else:\n            text_no_namedentities.append(item.text)\n    return \" \".join(text_no_namedentities)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:58:30.073542Z","iopub.execute_input":"2021-07-10T19:58:30.074007Z","iopub.status.idle":"2021-07-10T19:58:30.752539Z","shell.execute_reply.started":"2021-07-10T19:58:30.073963Z","shell.execute_reply":"2021-07-10T19:58:30.751303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exemple d'exécution de la fonction remove_NE\ntext_data = 'Mohamed has achieved good results using Stanford POS tagger'\nremove_NE(text_data)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T20:37:07.942085Z","iopub.execute_input":"2021-07-10T20:37:07.942503Z","iopub.status.idle":"2021-07-10T20:37:08.948144Z","shell.execute_reply.started":"2021-07-10T20:37:07.942461Z","shell.execute_reply":"2021-07-10T20:37:08.946943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fonction pour nettoyer le contexte  \nimport nltk\ndef clean_context(pd_frame):\n    list_clean = []\n    pattern1 = \"([([](e.g[,.]+\\s)?[A-Z][^\\s0-9]+(page?|-|\\d{2}(\\d{2})?[a-z]?|[;,]|etc|with|&|avec|et|and|al|[A-Z][^\\s0-9]+|\\.|\\s|-\\s+[^A-Z\\s0-9]+)*\\d{2}(\\d{2})?[a-z]?(pages?\\s\\d{1,4}-?\\d{1,4}?|-|\\s|:|,|;|\\d{4}|\\d{2})*(etc|(among|and)\\sothers)?[])])\"\n    pattern2 = \"([A-Z][^\\s0-9,;]+(etc|with|&|avec|et|and|al|[A-Z][^\\s0-9]+|\\.|\\s|-\\s+[^A-Z\\s0-9]+)*?\\(\\d{2}(\\d{2})?[a-z]?(pages?\\s\\d{1,4}-?\\d{1,4}?|-|\\s|:|,|;|\\d{4}|\\d{2})*(etc|(among|and)\\sothers)?\\))\"\n    \n    for i in range(len(pd_frame)):\n        list_clean.append(pd_frame['Context'][i].replace(pd_frame['Citation'][i],' '))\n        #Supprimer les citations\n        list_clean[i]=re.sub(pattern1,'', list_clean[i])\n        list_clean[i]=re.sub(pattern2,'', list_clean[i])\n        #Supprimer ce qui se trouve entre parenthèses et entre crochets\n        list_clean[i]=re.sub(\"([\\[\\(][^\\(\\)\\[\\]]*?[\\]\\)])\",' ', list_clean[i])\n        # part-of-speech ---> part of speech\n        list_clean[i]= re.sub('-',' ',list_clean[i])\n        #Supprimer les mots qui ont des caractères spéciaux et des         \n        list_clean[i]=' '.join([i for i in  nltk.word_tokenize(list_clean[i]) if not re.search(\"[^a-zA-Z]+\",i)  and len(re.findall(r'[A-Z]',i)) < len(re.findall(r'[a-z]',i)) and len(i)>1])\n        #Supprimer les entités nommées \n        list_clean[i] = remove_NE(list_clean[i])\n    return list_clean\nprint('Context avant nettoyage :')\nprint(get_docsss_cita(\"W11-0810\")['Context'][3])\nprint('\\n\\n')\nprint('Context apres nettoyage :')\nprint(clean_context(get_docsss_cita(\"W11-0810\"))[3])","metadata":{"execution":{"iopub.status.busy":"2021-07-10T20:49:04.914233Z","iopub.execute_input":"2021-07-10T20:49:04.914636Z","iopub.status.idle":"2021-07-10T20:49:20.36908Z","shell.execute_reply.started":"2021-07-10T20:49:04.914603Z","shell.execute_reply":"2021-07-10T20:49:20.367934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2= df\n\ndf2['Citation'] = citations_list\ndf2['Context'] = Context_list\n\ndf2 = df2.dropna()\ndf2= df2.reset_index(drop=True)\nprint(len(df2))\nclean_con = clean_context(df2)\ndf2['Cleaned_context'] = clean_con","metadata":{"execution":{"iopub.status.busy":"2021-07-10T20:00:51.712546Z","iopub.execute_input":"2021-07-10T20:00:51.713002Z","iopub.status.idle":"2021-07-10T20:11:07.259135Z","shell.execute_reply.started":"2021-07-10T20:00:51.712955Z","shell.execute_reply":"2021-07-10T20:11:07.258004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ceci une autre liste des articles qui se sont mal convertie\nremove2 = ['P02-1057', 'C04-1077', 'P01-1008','P05-1006','P10-1036', 'W03-0424', 'P02-1055', 'N13-1059','P06-1089', 'N10-1072', 'P12-3013']\n\ndf2 = df2[~df2['Cited-by'].isin(remove2)]\ndf2 = df2.reset_index(drop=True)\n#print(len(df2))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T20:50:36.283335Z","iopub.execute_input":"2021-07-10T20:50:36.283758Z","iopub.status.idle":"2021-07-10T20:50:36.288114Z","shell.execute_reply.started":"2021-07-10T20:50:36.283708Z","shell.execute_reply":"2021-07-10T20:50:36.286846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualisation du partie du notre dataset contexte\n#on remarque qu'ils sont bien nettoyé du bruit\nfor i in range(240,245):\n    print(df2['Cleaned_context'][i]+'\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T20:50:48.723532Z","iopub.execute_input":"2021-07-10T20:50:48.724135Z","iopub.status.idle":"2021-07-10T20:50:48.731877Z","shell.execute_reply.started":"2021-07-10T20:50:48.724079Z","shell.execute_reply":"2021-07-10T20:50:48.730817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reconnaitre les citations étiquetées 2 et 3 mais qui n'existent pas dans notre dataset\nfor i in range(len(dataset)):\n    if dataset['Follow-up'][i]>1:\n        for j in range(len(df2)):\n            if dataset['Cited-by'][i]==df2['Cited-by'][j] and dataset['Paper'][i]==df2['Paper'][j] and dataset['Annotator'][i]==df2['Annotator'][j]:\n                break\n            \n        else : \n            print(get_authors_year(dataset['Paper'][i]))\n            print(dataset['Annotator'][i]+'    '+dataset['Paper'][i]+'    '+dataset['Cited-by'][i]+'   '+str(dataset['Follow-up'][i]))  \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Récupérer le dataset qu'on a construit dans un fichier Csv pour l'implementation des algorithmes\ndf2.to_csv('three_sentences.csv')","metadata":{},"execution_count":null,"outputs":[]}]}
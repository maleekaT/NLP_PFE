,Annotator,Paper,Cited-by,Follow-up,Citation,Context,Cleaned_context
0,A,A00-1043,C00-2140,0,"(Jing, 2000)","We shorten the output of the summarizer to a \telegraphic style""; that way, more information can be included in a summary of k words (or n bytes). Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in (Jing, 2000). Our method simply excludes words occurring in the stop list from the summary, except for some highly informative words such as \I"" or \not"".",shorten the output of the summarizer to style that way more information can be included in summary of words Since we only use shallow methods for textual analysis that do not generate dependency structure we can not use complex methods for text reduction as described in Our method simply excludes words occurring in the stop list from the summary except for some highly informative words such as or
1,A,A97-1011,W09-1118,1,"(Tapanainen and Järvinen, 1997)","The task is approached using the IOB tagging scheme proposed by, e.g., Ramshaw and Marcus (1995), turning the original 7-class task into a 15-class task. Each token is represented using a fairly standard menagerie of features, including such stemming from the surface appearance of the token (e.g.,Contains dollar? Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar (Tapanainen and Järvinen, 1997) (e.g.,Case, Part-of-speech), fetched from precompiled lists of information (e.g.,Is first name?), and features based on predictions concerning the context of the token (e.g,Class of previous token). The decision committee is made up from 10 boosted decision trees using MultiBoostAB (Webb, 2000) (cf.",The task is approached using the tagging scheme proposed by turning the original class task into class task Each is represented using fairly standard menagerie of features including such stemming from the surface appearance of the calculated based on linguistic pre processing made with the Functional Dependency Grammar fetched from precompiled lists of information and features based on predictions concerning the context of the The decision committee is made up from boosted decision trees using MultiBoostAB cf
2,A,A97-1011,A00-2017,1,"(Tapanainen and Jrvinen, 1997)","The corpus was divided into 80% training and 20% test. The training and the test data  were processed by the FDG parser (Tapanainen and Jrvinen, 1997). Only verbs that  occur at least 50 times in the corpus were chosen.",The corpus was divided into training and test The training and the test data were processed by the parser Only verbs that occur at least times in the corpus were chosen
3,A,A97-1011,C00-2099,0,"(Tapanainen and J�arvinen, 1997)","The model has been realized as a type of probabilistic chart parser. The only other high-�delity computational rendering of Tesni�ere's dependency syntax that we are aware of is that of (Tapanainen and J�arvinen, 1997), which is neither generative nor statistical. The stochastic model generating dependency trees is very similar to other statistical dependency models, e.g., to that of (Alshawi, 1996).",The model has been realized as type of probabilistic chart parser The only other high computational rendering of dependency syntax that we are aware of is that of which is neither generative nor statistical The stochastic model generating dependency trees is very similar to other statistical dependency models to that of
4,A,A97-1011,W04-1505,0,"(Lin, 1998; Tapanainen and J�arvinen, 1997)","Participles may function as adjectives (Western industrialized/VBN countries), again obviating the need for an empty subject. Most successful deep-linguistic Dependency Parsers (Lin, 1998; Tapanainen and J�arvinen, 1997) do not have a statistical base. But one DG advantage is precisely that it o�ers simple but powerful statistical Maximum Likelihood Estimation (MLE) models.",Participles may function as adjectives again obviating the need for an empty subject Most successful deep linguistic Dependency Parsers do not have statistical base But advantage is precisely that it simple but powerful statistical Maximum Likelihood Estimation models
5,A,A97-1011,P99-1033,0,"(Tapanainen and J~irvinen, 1997; J£rvinen and Tapanainen, 1998)","Computational approaches to dependency syntax have recently become quite popular (e.g., a workshop dedicated to computational approaches to dependency grammars has been held at COLING/ACL'98 Conference). J~irvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and J~irvinen, 1997; J£rvinen and Tapanainen, 1998). The work of Sleator and Temperley(1991) on link grammar, an essentially lexicalized variant of dependency grammar, has also proved to be interesting in a number of aspects.",Computational approaches to dependency syntax have recently become quite popular and have demonstrated an efficient wide coverage dependency parser for The work of on link grammar an essentially lexicalized variant of dependency grammar has also proved to be interesting in number of aspects
6,A,A97-1011,W06-0202,3,"(Tapanainen and Järvinen, 1997)","There are 3911 instances of binary relations in all corpora. Three dependency parsers were used for these experiments:MINIPAR3 (Lin, 1999), the Machinese Syntax4 parser from Connexor Oy (Tapanainen and Järvinen, 1997) and the Stanford5 parser (Klein and Manning, 2003). These three parsers represent a cross-section of approaches to producing dependency analyses:MINIPAR uses a constituency grammar internally before converting the result to a dependency tree, Machinese Syntax uses a functional dependency grammar, and the Stanford Parser is a lexicalized probabilistic parser.",There are instances of binary relations in all corpora dependency parsers were used for these experiments the parser from and the parser These parsers represent cross section of approaches to producing dependency analyses uses constituency grammar internally before converting the result to dependency tree Syntax uses functional dependency grammar and the is lexicalized probabilistic parser
7,A,A97-1011,P01-1006,3,"(Tapanainen and Järvinen, 1997)","One of the very few systems3 that is fully automatic is MARS, the latest version of Mitkov’s knowledge-poor approach implemented by Evans. Recent work on this project has demonstrated that fully automatic anaphora resolution is more difficult than previous work has suggested (Orăsan et al., 2000). workbench employs one of the high performance ”super-taggers” for English - Conexor’s FDG Parser (Tapanainen and Järvinen, 1997). This super-tagger gives morphological information and the syntactic roles of words (in most of the cases).",of the very few that is fully automatic is the latest version of knowledge poor approach implemented by Recent work on this project has demonstrated that fully automatic anaphora resolution is more difficult than previous work has suggested workbench employs of the high performance super taggers for Conexor Parser This super tagger gives morphological information and the syntactic roles of words
8,A,A97-1011,E12-1072,0,"(Tapanainen and Järvinen, 1997)","Except for AnCora-ES, with 10,791 elliptic pronouns, our corpus is larger than the ones used in previous approaches: about 1,830 verbs including zero and explicit subjects in (Ferrández and Peral, 2000) (the exact number is not mentioned in the paper) and 1,202 zero subjects in (Rello and Illisei, 2009b). The corpus was parsed by Connexor’s Machinese Syntax (Connexor Oy, 2006), which returns lexical and morphological information as well as the dependency relations between words by employing a functional dependency grammar (Tapanainen and Järvinen, 1997). To annotate our corpus we created an annotation tool that extracts the finite clauses and the annotators assign to each example one of the defined annotation tags.",Except for AnCora with elliptic pronouns our corpus is larger than the ones used in previous approaches about verbs including and explicit subjects in and subjects in The corpus was parsed by Connexor Machinese Syntax which returns lexical and morphological information as well as the dependency relations between words by employing functional dependency grammar annotate our corpus we created an annotation tool that extracts the finite clauses and the annotators assign to each example of the defined annotation tags
9,A,C00-1072,C02-1130,2,"(Lin and Hovy, 2000)","We therefore created features that use topic signatures for each of the person subcategories. A topic signature, as described in (Lin and Hovy, 2000), is a list of terms that can be used to signal the membership of a text in the relevant topic or category. Each term in a text is given a topic signature score that indicates its ability to signal that the text is in a relevant category (the higher the score, the more that term is indicative of that category).",therefore created features that use topic signatures for each of the person subcategories topic signature as described in is list of terms that can be used to signal the membership of text in the relevant topic or category Each term in text is given topic signature score that indicates its ability to signal that the text is in relevant category
10,A,C00-1072,C08-1021,1,"(Lin and Hovy, 2000)","Section 5 describes the results when evaluating different versions of KnowNet and finally, section 6 presents some concluding remarks and future work. Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Topic Signatures are built by retrieving context words of a target topic from a large corpora.",Section describes the results when evaluating different versions of and finally section presents some concluding remarks and future work Signatures are word vectors related to particular topic Signatures are built by retrieving context words of target topic from large corpora
11,A,C00-1072,C08-1124,1,"(Lin and Hovy, 2000)","They observed that sentences located at the document head most likely contained important information. Recently, content features were also well studied, including centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold.",They observed that sentences located at the document head most likely contained important information Recently content features were also well studied including centroid signature terms and high frequency words defined centroid words as those whose average score were higher than threshold
12,A,C00-1072,D08-1080,0,"(Lin & Hovy, 2000; Harabagiu, 2004)","Amini & Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin & Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan & Radev, 2004; Mihalcea & Tarau, 2004).",use the documents to be summarized themselves to cluster terms and thus expanding the query internally More advanced methods for query expansion use topic signatures words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not Graph based methods for text summarization work usually at the level of sentences
13,A,C00-1072,D09-1032,1,"(Lin, 2004; Lin and Hovy, 2003)","The best feature, JensenShannon divergence, leads to a correlation as high as 0.88 with manual pyramid and 0.73 with responsiveness evaluations. The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (Lin, 2004; Lin and Hovy, 2003). ROUGE compares system summaries against one or more model summaries by computing n-gram word overlaps between the two.",The best feature divergence leads to correlation as high as with manual pyramid and with responsiveness evaluations The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric compares system summaries against or more model summaries by computing gram word overlaps between the
14,A,C00-1072,E09-1062,0,"(Lin and Hovy, 2000; Conroy et al., 2006)","Information-theoretic measures Entropy of the input word distribution and KL divergence between the input and a large document collection. 1Evaluations from later years did not include generic summarization, but introduced new tasks such as topic-focused and update summarization. Log-likelihood ratio for words in the input Number of topic signature words (Lin and Hovy, 2000; Conroy et al., 2006) and percentage of signature words in the vocabulary. Document similarity in the input set These features apply to multi-document summarization only.",Information theoretic measures Entropy of the input word distribution and divergence between the input and large document collection from later years did not include generic summarization but introduced new tasks such as topic focused and update summarization likelihood ratio for words in the input Number of topic signature words and percentage of signature words in the vocabulary Document similarity in the input set These features apply to multi document summarization only
15,A,C00-1072,I08-1016,0,"(Lin and Hovy, 2002; Schiffman et al., 2002; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Daumé III and Marcu, 2006)","Two of the key components of effective summarizations are the ability to identify important points in the text and to adequately reword the original text in order to convey these points. Automatic text summarization approaches have offered reasonably well-performing approximations for identifiying important sentences (Lin and Hovy, 2002; Schiffman et al., 2002; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Daumé III and Marcu, 2006) but, not surprisingly, text (re)generation has been a major challange despite some work on sub-sentential modification (Jing and McKeown, 2000; Knight and Marcu, 2000; Barzilay and McKeown, 2005). An additional drawback of extractive approaches is that estimates for the importance of larger text units such as sentences depend on the length of the sentence (Nenkova et al., 2006).",of the key components of effective summarizations are the ability to identify important points in the text and to adequately reword the original text in order to convey these points text summarization approaches have offered reasonably well performing approximations for identifiying important sentences but not surprisingly text generation has been major challange despite some work on sub sentential modification additional drawback of extractive approaches is that estimates for the importance of larger text units such as sentences depend on the length of the sentence
16,A,C00-1072,J10-1003,0,(Lin and Hovy 2000; Marcu 2000),"To give the reader an idea of variability of the answers, Table 4 reports standard deviation from the mean. continuous in nature: the quality of a summary. That is why we interpret the level of agreement as sufficient for the purpose of evaluating the quality of the summaries. 6.3 Comparing the Machine-Made Summaries and the Manually Created Extracts Measuring sentence co-selection between extractive summaries created by humans and those created by automatic summarizers has a long tradition in the text summarization community (Lin and Hovy 2000; Marcu 2000), but this family of measures has a number of well-known shortcomings. As many have remarked on previous occasions (Mani 2001; Radev et al. 2003), co-selection measures do not provide a complete assessment of the quality of a summary.",give the reader an idea of variability of the answers Table reports standard deviation from the mean continuous in nature the quality of summary That is why we interpret the level of agreement as sufficient for the purpose of evaluating the quality of the summaries Comparing the Machine Made Summaries and the Manually Created Extracts Measuring sentence co selection between extractive summaries created by humans and those created by automatic summarizers has long tradition in the text summarization community but this family of measures has number of well known shortcomings many have remarked on previous occasions co selection measures do not provide complete assessment of the quality of summary
17,A,C00-1072,J11-1001,0,(Lin and Hovy 2000),"In TAC 2008, ROUGE was used for automatic evaluation. ROUGE (Lin and Hovy 2000) compares any summary to any other (typically human-generated) summary using a recall-oriented approach. ROUGE-1 and -2 are based on unigrams and bigrams, respectively; ROUGE-SU4 uses bigrams with a maximum skip distance of 4 between bigrams; ROUGE-BE (Hovy, Lin, and Zhou 2005) is an n-gram approach based on basic elements, computed via parsing or automatic entity recognition.",was used for automatic evaluation compares any summary to any other summary using recall oriented approach and are based on and respectively uses with maximum skip distance of between is an gram approach based on basic elements computed via parsing or automatic entity recognition
18,A,C00-1072,P02-1058,2,(Lin and Hovy 2000),"Figure 1 shows the top 5 concepts with their relevancy scores (-2λ) for the topic “Slovenia Secession from Yugoslavia” in the DUC-2001 test collection. This is similar to the idea of topic signature introduced in (Lin and Hovy 2000). With the individual key concepts available, we proceed to cluster these concepts in order to identify major subtopics within the main topic.",Figure shows the top concepts with their relevancy scores for the topic Slovenia Secession from in the test collection This is similar to the idea of topic signature introduced in With the individual key concepts available we proceed to cluster these concepts in order to identify major subtopics within the main topic
19,A,C00-1072,P05-1026,0,"(Lin and Hovy, 2000)","In order to model this structure, the topic representation that we create considers separate topic signatures for each sub-topic. The notion of topic signatures was first introduced in (Lin and Hovy, 2000). For each subtopic in a scenario, given (a) documents relevant to the sub-topic and (b) documents not relevant to the subtopic, a statistical method based on the likelihood ratio is used to discover a weighted list of the most topic-specific concepts, known as the topic signature.",order to model this structure the topic representation that we create considers separate topic signatures for each sub topic The notion of topic signatures was first introduced in For each subtopic in scenario given documents relevant to the sub topic and documents not relevant to the subtopic statistical method based on the likelihood ratio is used to discover weighted list of the most topic specific concepts known as the topic signature
20,A,C00-1072,P06-1015,0,(Lin and Hovy 2000),"With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting semantic relations between entities, such as entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2006), and other relations.",With seemingly endless amounts of textual data at our disposal we have tremendous opportunity to automatically grow semantic term banks and ontological resources date researchers have harvested with varying success several resources including concept lists topic signatures facts and word similarity lists Many recent efforts have also focused on extracting semantic relations between entities such as entailments is part of and other relations
21,A,C00-1072,P06-4007,0,"(Lin and Hovy, 2000)","First introduced in (Harabagiu et al., 2005b), a predictive questioning approach to automatic question-answering assumes that Q/A systems can use the set of documents relevant to a user’s query in order to generate sets of questions – known as predictive questions – that anticipate a user’s information needs. Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a user’s domain of interest. Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question generation module, which uses a set of syntactic rewrite rules in order to create natural language questions from the original passage.",introduced in predictive questioning approach to automatic question answering assumes that systems can use the set of documents relevant to user query in order to generate sets of questions known as predictive questions that anticipate user information needs Under this approach topic representations like those introduced in and are used to identify set of text passages that are relevant to user domain of interest Topic relevant passages are then semantically parsed and submitted to question generation module which uses set of syntactic rewrite rules in order to create natural language questions from the original passage
22,A,C00-1072,P07-1070,0,"(Hovy and Lin, 1997; Lin and Hovy, 2000)","Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and O’Leary, 2001; Amini and Gallinari, 2002; Shen et al., 2007).",Extraction based methods usually assign saliency score to each sentence and then rank the sentences in the document The scores are usually computed based on combination of statistical and linguistic features including term frequency sentence position cue words stigma words topic signature etc Machine learning methods have also been employed to extract sentences including unsupervised methods and supervised methods
23,A,C00-1072,P08-1090,0,"(Lin and Hovy, 2000)","While previous work hasn’t focused specifically on learning narratives1, our work draws from two lines of research in summarization and anaphora resolution. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios.",While previous work hasn focused specifically on learning our work draws from lines of research in summarization and anaphora resolution summarization topic signatures are set of terms indicative of topic They are extracted from hand sorted sets of documents using log likelihood ratios
24,A,C00-1072,P08-1092,0,"(Lin and Hovy, 2003)","Finally, we run our reference rewriting component on each and trim the output to 665 bytes. We evaluate first using the ROUGE-L metric (Lin and Hovy, 2003) with a 95% (ROUGE computed) confidence interval for all systems and compared these to the ROUGE-L score of the best-performing DUC2004 system.5 The higher the ROUGE score, the closer the summary is to the DUC2004 human reference summaries. As shown in Figure 1, our best performing system is the multinomial naı̈ve Bayes classifier (MNB) using the classifier confidence scores to order the sentences in the biography.",Finally we run our reference rewriting component on each and trim the output to bytes evaluate using the metric with confidence interval for all systems and compared these to the score of the best performing The higher the score the closer the summary is to the human reference summaries shown in Figure our best performing system is the multinomial classifier using the classifier confidence scores to order the sentences in the biography
25,A,C00-1072,P09-1023,0,"(Lin and Hovy, 2000)","The outline of a wiki article using inner links will render the structure of its definition. In addition, infobox could be considered as topic signature (Lin and Hovy, 2000) or keywords about the topic. Since keywords and summary of a document can be mutually boosted (Wan et al., 2007), infobox is capable of summarization instruction.",The outline of wiki article using inner links will render the structure of its definition addition infobox could be considered as topic signature or keywords about the topic Since keywords and summary of document can be mutually boosted infobox is capable of summarization instruction
26,A,C00-1072,P10-1094,0,"(Luhn 1969; Lin and Hovy, 2000)","Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004).",Extractionbased summarization methods usually assign each sentence saliency score and then rank the sentences in document or document set For single document summarization the sentence score is usually computed by empirical combination of number of statistical and linguistic feature values such as term frequency sentence position cue words stigma words topic signature The summary sentences can also be selected by using machine learning methods or graph based methods
27,A,C00-1072,P11-1155,0,"(Luhn 1969; Lin and Hovy, 2000)","We focus on extraction-based methods in this study, and the methods directly extract summary sentences from a document or document set by ranking the sentences in the document or document set. In the task of single document summarization, various features have been investigated for ranking sentences in a document, including term frequency, sentence position, cue words, stigma words, and topic signature (Luhn 1969; Lin and Hovy, 2000). Machine learning techniques have been used for sentence ranking (Kupiec et al., 1995; Amini and Gallinari, 2002).",focus on extraction based methods in this study and the methods directly extract summary sentences from document or document set by ranking the sentences in the document or document set the task of single document summarization various features have been investigated for ranking sentences in document including term frequency sentence position cue words stigma words and topic signature Machine learning techniques have been used for sentence ranking
28,A,C02-1025,C10-2104,0,"(Chieu and Ng, 2002)","For example, a word like “Arkansas” may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help. To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus.",For example word like may not appear in the training set and in the test set there may not be enough context to infer its tag such cases neither global features nor aggregated contexts can help overcome this deficiency we employed the following unsupervised procedure the baseline is applied to the target annotated corpus
29,A,C02-1025,P03-1028,3,"(Soderland, 1999; Ciravegna, 2001; Chieu and Ng, 2002a)","IE from semi-structured texts is easier than from free texts, since the layout and format of a semi-structured text provide additional useful clues to aid in extraction. Several benchmark data sets have been used to evaluate IE approaches on semistructured texts (Soderland, 1999; Ciravegna, 2001; Chieu and Ng, 2002a). For the task of extracting information from free texts, a series of Message Understanding Conferences (MUC) provided benchmark data sets for evaluation.",from semi structured texts is easier than from free texts since the layout and format of semi structured text provide additional useful clues to aid in extraction Several benchmark data sets have been used to evaluate approaches on semistructured texts For the task of extracting information from free texts series of Message Understanding Conferences provided benchmark data sets for evaluation
30,A,C02-1025,P05-1045,0,Chieu and Ng (2002),"This approach is quite effective for enforcing label consistency in many NLP tasks, however, it permits a forward flow of information only, which is not sufficient for all cases of interest. Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document. This approach has the added advantage of allowing the training procedure to automatically learn good weightings for these “global” features relative to the local ones.",This approach is quite effective for enforcing label consistency in many tasks however it permits forward flow of information only which is not sufficient for all cases of interest propose solution to this problem for each token they define additional features taken from other occurrences of the same token in the document This approach has the added advantage of allowing the training procedure to automatically learn good weightings for these global features relative to the local ones
31,A,C02-1025,P05-1051,0,"(Chieu and Ng, 2002)","People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token. Recently, in (Ji and Grishman, 2004) we proposed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.",People have spent considerable effort in engineering appropriate features to improve performance most of these involve internal name structure or the immediate local context of the name Some other named entity systems have explored global information for name tagging made tagging pass which uses information on token sequences tagged in the pass used as features information about features assigned to other instances of the same token Recently in we proposed name tagging method which applied an based on coreference information to filter the names with low confidence and used coreference rules to correct and recover some names
32,A,C02-1025,P06-1141,1,"(Finkel et al., 2005; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004)","We use this intuition to approximate the aggregate information about labels assigned to other occurrences of the entity by the nonlocal model, with the aggregate information about labels assigned to other occurrences of the entity by the sequence model. This intuition enables us to learn weights for non-local dependencies in two stages; we first get predictions from a regular sequential CRF and in turn use aggregate information about predictions made by the CRF as extra features to train a second CRF. • Most work has looked to model non-local dependencies only within a document (Finkel et al., 2005; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004). Our model can capture the weaker but still important consistency constraints across the whole document collection, whereas previous work has not, for reasons of tractability.",use this intuition to approximate the aggregate information about labels assigned to other occurrences of the entity by the nonlocal model with the aggregate information about labels assigned to other occurrences of the entity by the sequence model This intuition enables us to learn weights for non local dependencies in stages we get predictions from regular sequential and in turn use aggregate information about predictions made by the as extra features to train Most work has looked to model non local dependencies only within document Our model can capture the weaker but still important consistency constraints across the whole document collection whereas previous work has not for reasons of tractability
33,A,H05-1079,C08-1028,0,Bos and Markert (2005),"Of these, CCG has received the most zealous computational attention. Impressive results have been achieved culminating in the state-of-the-art parser of Clark and Curran (2004) which has been used as the parser for the Pascal Rich Textual Entailment Challenge entry of Bos and Markert (2005). The appeal of CCG can be attributed to the existence of efficient parsing algorithms for it and the fact that it recognizes a mildly context-sensitive language class (Joshi et al., 1989), a language class more powerful than the context free languages (CFLs) that has been argued to be necessary for natural language syntax.",these has received the most zealous computational attention results have been achieved culminating in the state of the art parser of which has been used as the parser for the Pascal Rich Textual Entailment Challenge entry of The appeal of can be attributed to the existence of efficient parsing algorithms for it and the fact that it recognizes mildly context sensitive language class language class more powerful than the context free languages that has been argued to be necessary for natural language syntax
34,A,H05-1079,C08-2001,0,"(Zaenen et al., 2005; Bos, 2008)","However, the RTE evaluation framework has the disadvantage of being a “blackbox” type of evaluation. It makes very difficult to isolate the semantic task from the task of retrieving the necessary background knowledge (Zaenen et al., 2005; Bos, 2008). Furthermore, it is not designed to measure performance on specific semantic phenomena, and it is therefore difficult to know why a system is working correctly or incorrectly.",However the evaluation framework has the disadvantage of being blackbox type of evaluation makes very difficult to isolate the semantic task from the task of retrieving the necessary background knowledge Furthermore it is not designed to measure performance on specific semantic phenomena and it is therefore difficult to know why system is working correctly or incorrectly
35,A,H05-1079,D10-1074,0,"(Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005)","There is a large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006). Supervised learning approaches have also been applied to measure the similarities between training and testing pairs (Zanzotto and Moschitti, 2006).",There is large body of work on textual entailment initiated by the Pascal Recognizing Textual Entailment Challenges Different approaches have been developed for example based on logic proving and graph match Supervised learning approaches have also been applied to measure the similarities between training and testing pairs
36,A,H05-1079,J07-3004,0,Bos (2005),"In this article and in CCGbank, we approximate such semantic interpretations with dependency graphs that include most semantically relevant non-anaphoric local and long-range dependencies. Although certain decisions taken by the builders of the original Penn Treebank mean that the syntactic derivations that can be obtained from the Penn Treebank are not always semantically correct (as we will discuss), subsequent work by Bos et al. (2004) and Bos (2005) has demonstrated that the output of parsers trained on CCGbank can also be directly translated into logical forms such as Discourse Representation Theory structures (Kamp and Reyle 1993), which can then be used as input to a theorem prover in applications like question answering and textual entailment recognition. Translating the Treebank into this more demanding formalism has revealed certain sources of noise and inconsistency in the original annotation that have had to be corrected in order to permit induction of a linguistically correct grammar.",this article and in we approximate such semantic interpretations with dependency graphs that include most semantically relevant non anaphoric local and long range dependencies Although certain decisions taken by the builders of the original Penn mean that the syntactic derivations that can be obtained from the Penn are not always semantically correct subsequent work by and has demonstrated that the output of parsers trained on can also be directly translated into logical forms such as Discourse Representation Theory structures which can then be used as input to theorem prover in applications like question answering and textual entailment recognition Translating the into this more demanding formalism has revealed certain sources of noise and inconsistency in the original annotation that have had to be corrected in order to permit induction of linguistically correct grammar
37,A,H05-1079,N06-1005,2,"(Bos and Markert, 2005)","The motivation for this formulation was to isolate and evaluate the application-independent component of semantic inference shared across many application areas, reflected in the division of the PASCAL RTE dataset into seven distinct tasks: Information Extraction (IE), Comparable Documents (CD), Reading Comprehension (RC), Machine Translation (MT), Information Retrieval (IR), Question Answering (QA), and Paraphrase Acquisition (PP). 1 The examples given throughout this paper are from the first PASCAL RTE dataset, described in Section 6. The RTE problem as presented in the PASCAL RTE dataset is particularly attractive in that it is a reasonably simple task for human annotators with high inter-annotator agreement (95.1% in one independent labeling (Bos and Markert, 2005)), but an extremely challenging task for automated systems. The highest accuracy systems on the RTE test set are still much closer in performance to a random baseline accuracy of 50% than to the inter-annotator agreement.",The motivation for this formulation was to isolate and evaluate the application independent component of semantic inference shared across many application areas reflected in the division of the dataset into distinct tasks Information Extraction Comparable Documents Reading Comprehension Machine Translation Information Retrieval Question Answering and Paraphrase Acquisition The examples given throughout this paper are from the dataset described in Section The problem as presented in the dataset is particularly attractive in that it is reasonably simple task for human annotators with high inter annotator agreement but an extremely challenging task for automated systems The highest accuracy systems on the test set are still much closer in performance to random baseline accuracy of than to the inter annotator agreement
38,A,H05-1079,N06-1006,1,"(Akhmatova, 2005; Fowler et al., 2005; Bos and Markert, 2005)","Then the theorem prover may generate intermediate forms in the proof, but, nevertheless, individual terms are resolved locally without reference to global context. Finally, a few efforts (Akhmatova, 2005; Fowler et al., 2005; Bos and Markert, 2005) have tried to translate sentences into formulas of first-order logic, in order to test logical entailment with a theorem prover. While in principle this approach does not suffer from the limitations we describe below, in practice it has not borne much fruit.",Then the theorem prover may generate intermediate forms in the proof but nevertheless individual terms are resolved locally without reference to global context Finally few efforts have tried to translate sentences into formulas of order logic in order to test logical entailment with theorem prover While in principle this approach does not suffer from the limitations we describe below in practice it has not borne much fruit
39,A,H05-1079,N10-1146,0,"(Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)","Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005).",Recognizing Textual Entailment is rather challenging as effectively modeling syntactic and semantic for this task is difficult Early deep semantic models as well as more recent ones rely on specific world knowledge encoded in rules for drawing decisions Shallower models exploit matching methods between graphs of texts and hypotheses
40,A,H05-1079,P06-1051,0,"(Bos and Markert, 2005)","The entailment between T andH is detected when there is a transformation r ∈ T so thatsim(r(T ),H) > α. These transformations are logical rules in (Bos and Markert, 2005) or sequences of allowedrewrite rulesin (de Salvo Braz et al., 2005). The disadvantage is that such rules have to be manually designed.",The entailment between is detected when there is transformation so thatsim These transformations are logical rules in or sequences of allowedrewrite rulesin The disadvantage is that such rules have to be manually designed
41,A,H05-1079,P06-2105,0,"(Bos and Markert, 2005)","The following sections of the paper shall detail the logic proving methodology, our logical representation of text and the various types of axioms that the prover uses. To our knowledge, there are few logical approaches to RTE. (Bos and Markert, 2005) represents � and lation of the DRS language used in Discourse Representation Theory (Kamp and Reyle, 1993) and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts. (de Salvo Braz et al., 2005) proposes a Description Logic-based knowledge representation language used to induce the representations of � and sumption algorithm to check if any of � ’s representations obtained through equivalent transformations entails Our system uses COGEX (Moldovan et al., 2003), a natural language prover originating from OTTER (McCune, 1994). Once its set of support is loaded with � and the negated hypothesis ( � ate inferences, COGEX begins to search for proofs.",The following sections of the paper shall detail the logic proving methodology our logical representation of text and the various types of axioms that the prover uses our knowledge there are few logical approaches to represents and lation of the language used in Discourse Representation Theory and uses theorem prover and model builder with some generic lexical and geographical background knowledge to prove the entailment between the texts proposes Description Logic based knowledge representation language used to induce the representations of and sumption algorithm to check if any of representations obtained through equivalent transformations entails Our system uses natural language prover originating from Once its set of support is loaded with and the negated hypothesis ate inferences begins to search for proofs
42,A,H05-1079,P11-1059,0,"(Bos and Markert, 2005)","Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts.",present supervised scope detector using their own annotation Some applications deal indirectly with negation machine translation text classification and recognizing entailments Regarding corpora the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts
43,A,H05-1079,Q13-1015,0,"(Bobrow et al., 2007; Bos and Markert, 2005)","Semantic operators, such as determiners, negation, conjunctions, modals, tense, mood, aspect, and plurals are ubiquitous in natural language, and are crucial for high performance on many practical applications— but current distributional models struggle to capture even simple examples. Conversely, computational models of formal semantics have shown low recall on practical applications, stemming from their reliance on ontologies such as WordNet (Miller, 1995) to model the meanings of content words (Bobrow et al., 2007; Bos and Markert, 2005). For example, consider what is needed to answer a question like Did Google buy YouTube? from the following sentences: All of these require knowledge of lexical semantics (e.g. that buy and purchase are synonyms), but some also need interpretation of quantifiers, negatives, modals and disjunction.",Semantic operators such as determiners negation conjunctions modals tense mood aspect and plurals are ubiquitous in natural language and are crucial for high performance on many practical but current distributional models struggle to capture even simple examples Conversely computational models of formal semantics have shown low recall on practical applications stemming from their reliance on ontologies such as to model the meanings of content words For example consider what is needed to answer question like Did Google buy from the following sentences All of these require knowledge of lexical semantics but some also need interpretation of quantifiers negatives modals and disjunction
44,A,H05-1079,S12-1082,0,"(Rinaldi et al., 2003; Bos and Markert, 2005)","STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006). Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word ngram hit rates.",is closely related to the problems of paraphrasing which is bidirectional and based on semantic equivalence and textual entailment which is directional and based on relations between semantics Related methods incorporate measurements of similarity at various levels lexical syntactic and semantic Measures from machine translation evaluation are often used to evaluate lexical level approaches including metric based on word ngram hit rates
45,A,H05-1079,S12-1108,0,"(Rinaldi et al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 2005, 2007)","For example, they may treat the input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different levels. Logic-based approach is to map the language expressions to logical meaning representations, and then rely on logical entailment checks, possibly by invoking theorem provers (Rinaldi et al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 2005, 2007). An alternative to use logical meaning representations is to start by mapping each word of the input language expressions to a vector that shows how strongly the word co-occurs with particular other words in corpora (Lin, 1998b), possibly also taking into account syntactic information, for example requiring that the co-occurring words participate in particular syntactic dependencies (Pad´o & Lapata, 2007).",For example they may treat the input expressions simply as surface strings they may operate on syntactic or semantic representations of the input expressions or on representations combining information from different levels Logic based approach is to map the language expressions to logical meaning representations and then rely on logical entailment checks possibly by invoking theorem provers alternative to use logical meaning representations is to start by mapping each word of the input language expressions to vector that shows how strongly the word co occurs with particular other words in corpora possibly also taking into account syntactic information for example requiring that the co occurring words participate in particular syntactic dependencies
46,A,H05-1079,S13-1002,3,Bos and Markert (2005),"RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference.",directly tests whether system can construct semantic representations that allow it to draw correct inferences existing approaches the closest to ours is by who employ purely logical approach that uses to convert both the premise and hypothesis into order logic and then checks for entailment using theorem prover contrast our approach uses logic with probabilistic inference
47,A,I05-2038,C10-1076,1,"(Tateisi et al., 2005)","For detailed statistics about the three subcorpora, please see Morante and Daelemans (2009). For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser2 (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al., 2005)3, which is a bracketed corpus in (almost) PTB style. 10-fold cross-validation on GTB1.0 shows that the parser achieves the performance of 86.57 in F1-measure. It is worth noting that the GTB1.0 corpus includes all the sentences in the abstracts subcorpus of the Bioscope corpus.",For detailed statistics about the subcorpora please see For preprocessing all the sentences in the Bioscope corpus are tokenized and then parsed using the trained on the which is bracketed corpus in style fold cross validation on shows that the parser achieves the performance of in measure is worth noting that the corpus includes all the sentences in the abstracts subcorpus of the Bioscope corpus
48,A,I05-2038,C10-1088,1,"(Tateisi et al., 2005)","Likewise, a number of evaluation of parser outputs against gold standard corpora have been performed in the domain, but the broader implications of the results of such intrinsic evaluations are rarely considered. The BioNLP’09 shared task involved documents contained also in the GENIA treebank (Tateisi et al., 2005), creating an opportunity for direct study of intrinsic and task-oriented evaluation results. As the treebank can be converted into various dependency formats using existing format conversion methods, evaluation can further be extended to cover the effects of different representations.",Likewise number of evaluation of parser outputs against gold standard corpora have been performed in the domain but the broader implications of the results of such intrinsic evaluations are rarely considered The shared task involved documents contained also in the treebank creating an opportunity for direct study of intrinsic and task oriented evaluation results the treebank can be converted into various dependency formats using existing format conversion methods evaluation can further be extended to cover the effects of different representations
49,A,I05-2038,D09-1157,1,"(Tateisi et al., 2005)","We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task.",used and for the experiments structured parsers split sentence into syntactic constituents such as noun phrases or verb phrases used the parser and also variant of the parser which was trained on the treebank for biomedical text parsers aim to compute in depth syntactic and semantic structures based on syntactic theories such as and used the parser and variant of adapted for the biomedical domain There were number of practical issues to consider when using parsers for this task
50,A,I05-2038,N10-1004,1,"(Tateisi et al., 2005)","While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2004)). Our biomedical data comes from theGENIA treebank8 (Tateisi et al., 2005), a corpus of abstracts from the Medline database.9 We downloaded additional sentences from Medline for our self-trainedMEDLINE corpus. Unlike the other two self-trained corpora, we include two versions ofMEDLINE.",While the original trees include disfluency information we assume our speech corpora have had speech repairs excised Our biomedical data comes from corpus of abstracts from the downloaded additional sentences from for our self corpus Unlike the other self trained corpora we include versions
51,A,I05-2038,P06-1128,1,"(Tateisi et al., 2005)","We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs).",parsed all sentences using an parser to obtain their predicate argument structures Because our target is biomedical texts we re trained parser with the and also applied bidirectional part ofspeech tagger trained with the as preprocessor Because parsing speed is still unrealistic for parsing the entire on single machine we used geographically separated computer clusters having nodes
52,A,I05-2038,P06-4005,2,"(Tateisi et al., 2005)","In the table, ‘HPSG-PTB’ means that the statistical model was trained on Penn Treebank. ‘HPSG-GENIA’ means that the statistical model was trained on both Penn Treebank and GENIA treebank as described in (Hara et al., 2005). The GENIA treebank (Tateisi et al., 2005) consists of 500 abstracts (4,446 sentences) extracted from MEDLINE. ture structure for the sentence “NASA officials vowed to land Discovery early Tuesday at one of three locations after weather conditions forced them to scrub Monday’s scheduled return.” Figure 2 shows the top page of the MEDIE. MEDIE is an intelligent search engine for the accurate retrieval of relational concepts from MEDLINE 2 (Miyao et al., 2006).",the table means that the statistical model was trained on Penn Treebank means that the statistical model was trained on both Penn Treebank and treebank as described in The treebank consists of abstracts extracted from ture structure for the sentence officials vowed to land early Tuesday at of locations after weather conditions forced them to scrub scheduled Figure shows the top page of the is an intelligent search engine for the accurate retrieval of relational concepts from
53,A,J02-3001,C04-1018,0,Gildea and Jurafsky’s (2002),"An advantage over our work, however, is that Bethard et al. (2004) do not require separate solutions to pse identification and the identification of their direct sources. Automatic identification of sources has also been addressed indirectly by Gildea and Jurafsky’s (2002) work on semantic role identification in that finding sources often corresponds to finding the filler of the agent role for verbs. Their methods then might be used to identify sources and associate them with pse’s that are verbs or portions of verb phrases.",advantage over our work however is that do not require separate solutions to pse identification and the identification of their direct sources Automatic identification of sources has also been addressed indirectly by work on semantic role identification in that finding sources often corresponds to finding the filler of the agent role for verbs Their methods then might be used to identify sources and associate them with pse that are verbs or portions of verb phrases
54,A,J02-3001,D08-1094,0,"(Gildea and Jurafsky, 2002)","In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Padó et al., 2007). that integrates lexical information with selectional preferences.",linguistics expectations in the form of selectional restrictions and selectional preferences have long been used in semantic theories and more recently induced from corpora Attention has mostly been limited to selectional preferences of verbs which have been used for example for syntactic disambiguation word sense disambiguation and semantic role labeling Recently vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments that integrates lexical information with selectional preferences
55,A,J02-3001,E03-1040,0,"(Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002)","Wide-coverage language processing systems require large amounts of knowledge about individual words, leading to a lexical acquisition bottleneck. Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification.",Wide coverage language processing systems require large amounts of knowledge about individual words leading to lexical acquisition bottleneck Because verbs play central role in the syntactic and semantic interpretation of sentence much research has focused on automatically learning properties of verbs from text corpora such as their subcategorization argument roles selectional preferences and lexical semantic classification Our work aims to extend the applicability of the latter by developing general feature space for automatic verb classification
56,A,J02-3001,P04-1043,1,"(Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003)","Arg. 1 Figure 1: A predicate argument structure in a parse-tree representation. Several machine learning approaches for argument identification and classification have been developed (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003). Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat representation.",Arg Figure predicate argument structure in parse tree representation Several machine learning approaches for argument identification and classification have been developed Their common characteristic is the adoption of feature spaces that model predicate argument structures in flat representation
57,A,J02-3001,P07-1071,2,"(Pradhan et al., 2004; Gildea and Jurafsky, 2002)","We describe a novel neural network architecture for the problem of semantic role labeling. Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chunker.",describe novel neural network architecture for the problem of semantic role labeling Many current solutions are complicated consist of several stages and handbuilt features and are too slow to be applied as part of real applications that require such semantic labels partly because of their use of syntactic parser Our method instead learns direct mapping from source sentence to semantic tags for given predicate without the aid of parser or chunker
58,A,J96-1002,A00-1019,0,"(Brown et al., 1993; Berger et al., 1996; Och and Weber, 98; Wang and Waibel, 98; Wu and Wong, 98)","For example, the translation model could have a higher weight at the start of a sentence but the contribution of the language model might become more important in the middle or the end of the sentence• A study of the weightings for these two models is described elsewhere• In the work described here we did not use the contribution of the language model (that is, a( t ' ,  s) = O, V t', s). Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work (Brown et al., 1993; Berger et al., 1996; Och and Weber, 98; Wang and Waibel, 98; Wu and Wong, 98). These studies report improvements on some specific tasks (task-oriented limited vocabulary) which by nature are very different from the task TRANSTYPE is devoted to.",For example the translation model could have higher weight at the start of sentence but the contribution of the language model might become more important in the middle or the end of the study of the weightings for these models is described the work described here we did not use the contribution of the language model that is Techniques for weakening the independence assumptions made by the models and have been proposed in recent work These studies report improvements on some specific tasks which by nature are very different from the task is devoted to
59,A,J96-1002,A00-2026,2,"(Berger et al., 1996)","These packages require linguistic sophistication in order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text. The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998). The MT systems of (Berger et al., 1996) learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation.",These packages require linguistic sophistication in order to write the abstract semantic representation but they are flexible because minor changes to the input can accomplish major changes to the generated text The only trainable approaches to surface generation are the purely statistical machine translation systems such as and the corpus based generation system described in The systems of learn to generate text in the target language straight from the source language without the aid of an explicit semantic representation
60,A,J96-1002,A00-2031,0,"(Berger et al., 1996)","Features can be fairly simple and easily read off the tree (e.g. 'this node's label is X', 'this node's parent's label is Y'), or slightly more complex ('this node's head's partof-speech is Z'). This is concordant with the usage in the maximum entropy literature (Berger et al., 1996). When using a number of known features to guess an unknown one, the usual procedure is to calculate the value of each feature, and then essentially look up the empirically most probable value for the feature to be guessed based on those known values.",Features can be fairly simple and easily read off the tree or slightly more complex This is concordant with the usage in the maximum entropy literature When using number of known features to guess an unknown one the usual procedure is to calculate the value of each feature and then essentially look up the empirically most probable value for the feature to be guessed based on those known values
61,A,J96-1002,A97-1056,0,"(Berger et al., 1996)","Decomposable models are those graphical models that express the joint distribution as the product of the marginal distributions of the variables in the maximal  cliques of the graphical representation, scaled by the marginal distributions of variables common to two or more of these maximal sets. Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; (Berger et al., 1996)). 3. Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power (Whittaker, 1990).",Decomposable models are those graphical models that express the joint distribution as the product of the marginal distributions of the variables in the maximal cliques of the graphical representation scaled by the marginal distributions of variables common to two or more of these maximal sets Because their joint distributions have such closed form expressions the parameters can be estimated directly from the training data without the need for an iterative fitting procedure Although there are far fewer decomposable models than log linear models for given set of feature variables it has been shown that they have substantially the same expressive power
62,A,J96-1002,C00-1060,0,"(Berger et al., 1996)","Our statistical model is called the Triplet/Quadruplet Model, which was named after the number of constituents in the conditional parts of the equations. We report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar, SLUNG (Mitsuishi et al., 1998) and the maximum entropy method (Berger et al., 1996). Moreover, the resulting parse trees generated by our hybrid parser are legitimate trees in terms of given hand-crafted grammars, and we are expecting that we can enjoy advantages provided by high-level grammar formalisms, such as construction of semantic structures.",Our statistical model is called the which was named after the number of constituents in the conditional parts of the equations report that our parsing framework achieved high accuracy in dependency analysis of with combination of an underspecified based grammar and the maximum entropy method Moreover the resulting parse trees generated by our hybrid parser are legitimate trees in terms of given hand crafted grammars and we are expecting that we can enjoy advantages provided by high level grammar formalisms such as construction of semantic structures
63,A,J96-1002,C00-1061,0,"(Berger et al., 1996)","In order to solve a sparseness problem, Maximum Entropy Model, Back-o�, and Linear interpolation methods are used. They combine di�erent statistical estimators. (Tae-il Kim, 2000) use up to �ve phonemes in feature function(Berger et al., 1996). Nine feature functions are combined with Maximum Entropy Method.",order to solve sparseness problem Maximum Entropy Model Back and interpolation methods are used They combine statistical estimators use up to phonemes in feature function feature functions are combined with Maximum Entropy Method
64,A,J96-1002,C00-1064,1,"(Brown et al., 1993; Berger et al., 1996; Melamed, 1997)","Aligned texts have been used for derivation of bilingual dictionaries and terminology databases which are useful for machine translation and cross languages information retrieval. Thus, a lot of alignment techniques have been suggested at the sentence (Gale et al., 1993), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), word (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), collocation (Smadja et al., 1996) and terminology level. Some work has used lexical association measures for word alignments.",Aligned texts have been used for derivation of bilingual dictionaries and terminology databases which are useful for machine translation and cross languages information retrieval Thus lot of alignment techniques have been suggested at the sentence phrase noun phrase word collocation and terminology level Some work has used lexical association measures for word alignments
65,A,J96-1002,C00-2124,0,"(Berger et al., 1996)","During evaluation it is tested which features are active (i.e. a feature is active when the context meets the requirements given by the feature). For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al., 1996). For the classi�er built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence.",During evaluation it is tested which features are active For every class the weights of the active features are combined and the best scoring class is chosen For the built here the surrounding words their tags and tags predicted for the previous words are used as evidence
66,A,J96-1002,C02-1064,1,"(Berger et al., 1996; Ristad, 1997; Ristad, 1998)","The goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences. We implemented these models within an maximum entropy framework (Berger et al., 1996; Ristad, 1997; Ristad, 1998). 4.1 Keyword-Production Models This section describes five keyword-production models which are represented by P (K|M,D,T ) in Eq. (4). In these models, we define the set of headwords whose frequency in the corpus is over a certain threshold as a set of keywords, KS, and we restrict the bunsetsus to those generated by the generation rules represented in form (5).",The goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences implemented these models within an maximum entropy framework Keyword Production Models This section describes keyword production models which are represented by in these models we define the set of headwords whose frequency in the corpus is over certain threshold as set of keywords and we restrict the bunsetsus to those generated by the generation rules represented in form
67,A,J96-1002,D07-1051,1,"(Berger et al., 1996)","Using disagreement exclusively for selection requires only one parameter,viz. the batch sizeb, to be specified. For our AL framework we decided to employ a Maximum Entropy (ME) classifier (Berger et al., 1996). We employ a rich set of features (see Table 1) which are general enough to be used in most (sub)domains for entity recognition.",Using disagreement exclusively for selection requires only one parameter viz the batch sizeb to be specified For our framework we decided to employ Maximum Entropy classifier employ rich set of features which are general enough to be used in most domains for entity recognition
68,A,J96-1002,C10-3004,0,"(Berger et al., 1996)","Named Entity Recognition (NER): LTP can identify six sorts of named entity: Person, Loc, Org, Time, Date and Quantity. A maximum entropy model (Berger et al., 1996) is adopted here. We still used the People’s Daily corpus. 4.",Named Entity Recognition can identify sorts of named entity Person Loc Org Time Date and Quantity maximum entropy model is adopted here still used the People Daily corpus
69,A,J96-1002,P05-1020,0,"(Berger et al., 1996)","Learning algorithms. We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al., 1996). The classification model induced by each of these learners returns a number between 0 and 1 that indicates the likelihood that the two NPs under consideration are coreferent.",Learning algorithms consider learning algorithms namely the decision tree induction system the rule learning algorithm and maximum entropy classification The classification model induced by each of these learners returns number between and that indicates the likelihood that the under consideration are coreferent
70,A,J96-1002,P10-1142,0,"(Berger et al., 1996)","Decision tree induction systems (e.g., C5 (Quinlan, 1993)) are the first and one of the most widely used learning algorithms by coreference researchers, although rule learners (e.g., RIPPER (Cohen, 1995)) and memory-based learners (e.g., TiMBL (Daelemans and Van den Bosch, 2005)) are also popular choices, especially in early applications of machine learning to coreference resolution. In recent years, statistical learners such as maximum entropy models (Berger et al., 1996), voted perceptrons (Freund and Schapire, 1999), 3In this paper, we use the termanaphoricto describe any NP that is part of a coreference chain but is not the head of the chain. Hence, proper names can be anaphoric under this overloaded definition, but linguistically, they are not. and support vector machines (Joachims, 1999) have been increasingly used, in part due to their ability to provide a confidence value (e.g., in the form of a probability) associated with a classification, and in part due to the fact that they can be easily adapted to train recently proposed rankingbased coreference models (see Section 3.3).",Decision tree induction systems are the and one of the most widely used learning algorithms by coreference researchers although rule and memory based are also popular choices especially in early applications of machine learning to coreference resolution recent years statistical such as maximum entropy models voted perceptrons this paper we use the termanaphoricto describe any that is part of coreference chain but is not the head of the chain Hence proper names can be anaphoric under this overloaded definition but linguistically they are not and support vector machines have been increasingly used in part due to their ability to provide confidence value associated with classification and in part due to the fact that they can be easily adapted to train recently proposed rankingbased coreference models
71,A,J96-1002,P98-2214,2,"(Della Pietra et al., 1997; Berger et al., 1996)","First, we introduce a model of generating a collocation of a verb and argument /adjunct  nouns (section 2) and then view the model as a probability model (section 3). As a model learning method, we adopt the maximum entropy model learning method (Della Pietra et al., 1997; Berger et al., 1996). Case dependencies and noun class generalization are represented as features in the maximum entropy approach.",we introduce model of generating collocation of verb and argument nouns and then view the model as probability model model learning method we adopt the maximum entropy model learning method Case dependencies and noun class generalization are represented as features in the maximum entropy approach
72,A,J96-1002,P99-1069,0,Berger et al. (1996),"Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly ) are as useful (if not more useful) as the full probabilities P0(w), at least in those cases for which the ultimate goal is syntactic analysis. Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. The problem of estimating parameters for log-linear models is not new.",Regardless of the pragmatic motivation one could perhaps argue that the conditional probabilities are as useful as the full probabilities at least in those cases for which the ultimate goal is syntactic analysis and make this same point and arrive at the same estimator albeit through maximum entropy argument The problem of estimating parameters for log linear models is not new
73,A,J96-1002,P02-1002,0,"(Berger et al., 1996)","Rather than attempting to train all model parameters simultaneously, the algorithm trains them sequentially. The algorithm is easy to implement, typically uses only slightly more memory, and will lead to improvements for most maximum entropy problems. 1 Introduction Conditional Maximum Entropy models have been used for a variety of natural language tasks, including Language Modeling (Rosenfeld, 1994), partof-speech tagging, prepositional phrase attachment, and parsing (Ratnaparkhi, 1998), word selection for machine translation (Berger et al., 1996), and finding sentence boundaries (Reynar and Ratnaparkhi, 1997). Unfortunately, although maximum entropy (maxent) models can be applied very generally, the typical training algorithm for maxent, Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972), can be extremely slow.",Rather than attempting to train all model parameters simultaneously the algorithm trains them sequentially The algorithm is easy to implement typically uses only slightly more memory and will lead to improvements for most maximum entropy problems Introduction Conditional Maximum Entropy models have been used for variety of natural language tasks including Language Modeling partof speech tagging prepositional phrase attachment and parsing word selection for machine translation and finding sentence boundaries Unfortunately although maximum entropy models can be applied very generally the typical training algorithm for maxent Generalized Iterative Scaling can be extremely slow
74,A,J96-1002,P05-1066,0,"(Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004)","Our approach involves a preprocessing step, where sentences in the language being translated are modified before being passed to an existing phrasebased translation system. A number of other researchers (Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods. (Berger et al., 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’intérêt). This was a relatively limited study, concentrating on this one syntactic phenomenon which involves relatively local transformations (a parser was not required in this study). (Niessen and Ney, 2004) describe a method that combines morphologically–split verbs in German, and also reorders questions in English and German.",Our approach involves preprocessing step where sentences in the language being translated are modified before being passed to an existing phrasebased translation system number of other researchers have described previous work on preprocessing methods describe an approach that targets translation of phrases of the form de This was relatively limited study concentrating on this syntactic phenomenon which involves relatively local transformations describe method that combines verbs in and also reorders questions in and
75,A,J96-1002,W00-0707,1,"(Berger et al., 1996)","However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I described a Maximum Entropy/Minimum Divergence (MEMD) model (Berger et al., 1996) for p(w[hi, s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (Brown et al., 1993). This model significantly outperforms an equivalent linear combination of a trigram and model 1 in testcorpus perplexity, despite using several orders of magnitude fewer translation parameters.",However this appears to be weak technique even when is allowed to depend on various features of the context previous work described Maximum Divergence model for which incorporates trigram language model and translation component which is an analog of the well known translation model This model significantly outperforms an equivalent linear combination of trigram and model in testcorpus perplexity despite using several orders of magnitude fewer translation parameters
76,A,J96-1002,W02-0813,1,"(Berger et al., 1996)","We developed an automatic WSD system that uses a maximum entropy framework to combine linguistic contextual features from corpus instances of each verb to be tagged. Under the maximum entropy framework (Berger et al., 1996), evidence from different features can be combined with no assumptions of feature independence. The automatic tagger estimates the conditional probability that a word has sensex given that it occurs in contexty, where y is a conjunction of features.",developed an automatic system that uses maximum entropy framework to combine linguistic contextual features from corpus instances of each verb to be tagged Under the maximum entropy framework evidence from different features can be combined with no assumptions of feature independence The automatic tagger estimates the conditional probability that word has sensex given that it occurs in contexty where is conjunction of features
77,A,J96-1002,W96-0213,0,"(Berger et al., 1996)","A Maximum Entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data. Previous uses of this model include language modeling(Lau et al., 1993), machine translation(Berger et al., 1996), prepositional phrase attachment(Ratnaparkhi et al., 1994), and word morphology(Della Pietra et al., 1995). This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St.",Maximum Entropy model is well suited for such experiments since it cornbines diverse forms of contextual information in principled manner and does not impose any distributional assumptions on the training data Previous uses of this model include language modeling machine translation prepositional phrase attachment and word morphology This paper briefly describes the maximum entropy and maximum likelihood properties of the model features used for tagging and the experiments on the Penn Treebank Wall
78,A,N03-1028,C04-1081,0,"(Malouf, 2002; Sha and Pereira, 2003)","M} is written Traditional maximum entropy learning algorithms, such as GIS and IIS (della Pietra et al., 1995), can be used to train CRFs. However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003). The gradient of the likelihood is∂PΛ(y|x)/∂λk = CRFs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum.",is written Traditional maximum entropy learning algorithms such as and can be used to train However our implementation uses quasi gradient climber for optimization which has been shown to converge much faster The gradient of the likelihood share many of the advantageous properties of standard maximum entropy classifiers including their convex likelihood function which guarantees that the learning procedure converges to the global maximum
79,A,N03-1028,C08-1094,1,Sha and Pereira (2003),"Word classes are straightforwardly extracted from the treebank trees, by measuring the span of constituents starting and ending at each word position. We trained log linear models with the perceptron algorithm (Collins, 2002) using features similar to those used for NP chunking in Sha and Pereira (2003), including surrounding POStags (provided by a separately trained log linear POS-tagger) and surrounding words, up to 2 before and 2 after the current word position. Table 2 presents classification accuracy on the development set for both of these classification tasks.",Word classes are straightforwardly extracted from the treebank trees by measuring the span of constituents starting and ending at each word position trained log linear models with the algorithm using features similar to those used for chunking in including surrounding and surrounding words up to before and after the current word position Table presents classification accuracy on the development set for both of these classification tasks
80,A,N03-1028,C08-1106,2,(McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001),"As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license ( Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure.",the representative problem in shallow parsing noun phrase chunking has received much attention with the development of standard evaluation datasets and with under the Creative Commons Attribution Noncommercial Share Alike Unported license Some rights reserved extensive comparisons among methods contexts often have complex underlying structure
81,A,N03-1028,C08-1113,0,"(Sha and Pereira, 2003)","Then the maximum likelihood estimator for this model can be obtained by maximizing the log likelihood function: This modeling naturally embraces label ambiguities in the incomplete annotation.4 Unfortunately, equation (3) is not a concave function5 so that there are local maxima in the objective function. Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete annotations using gradient ascent iterations (Sha and Pereira, 2003). Gradient ascent methods require the partial derivative of equation (3): Equations (3) and (4) include the summations of all of the label sequences in Y or YL.",Then the maximum likelihood estimator for this model can be obtained by maximizing the log likelihood function This modeling naturally embraces label ambiguities in the incomplete Unfortunately equation is not concave so that there are local maxima in the objective function Although this non concavity prevents efficient global maximization of equation it still allows us to incorporate incomplete annotations using gradient ascent iterations Gradient ascent methods require the partial derivative of equation Equations and include the summations of all of the label sequences in or
82,A,N03-1028,C10-1082,0,"(Sha and Pereira, 2003; McCallum and Li, 2003)","To extend the problem to event classification, the alphabet γ must be extended with the event classes (state, aspectual, etc.). CRFs have been successfully applied to many sequence labeling tasks (Sha and Pereira, 2003; McCallum and Li, 2003). From our point of view, the task addressed in this paper is well suited for this ML technique.",extend the problem to event classification the alphabet must be extended with the event classes have been successfully applied to many sequence labeling tasks From our point of view the task addressed in this paper is well suited for this technique
83,A,N03-1028,P03-1064,0,"(ShaandPereira,2003)","In this paper, we will focus on the NP chunking task, and use it as an applicationof supertagging. (Abney, 1991) proposeda two-phaseparsing modelwhich includeschunkingandattaching. (Ramshaw and Marcus, 1995) approachedchucking by usingTransformationBasedLearning(TBL). Many machinelearningtechniqueshave beensuccessfullyappliedto chunkingtasks,suchasRegularizedWinnow (Zhangetal.,2001),SVMs(Kudoand Matsumoto,2001),CRFs(ShaandPereira,2003), MaximumEntropy Model (Collins,2002),Memory BasedLearning(Sang,2002)andSNoW(Muñozet al., 1999). Thepreviousbestresulton chunkingin literature wasachievedby RegularizedWinnow (Zhangetal., 2001),which took someof theparsingresultsgiven by anEnglishSlotGrammar-basedparserasinputto thechunker.",this paper we will focus on the chunking task and use it as an applicationof supertagging proposeda phaseparsing modelwhich includeschunkingandattaching approachedchucking by usingTransformationBasedLearning Many machinelearningtechniqueshave beensuccessfullyappliedto chunkingtasks suchasRegularizedWinnow MaximumEntropy Model Memory BasedLearningandSNoW Thepreviousbestresulton chunkingin literature wasachievedby RegularizedWinnow which took someof theparsingresultsgiven by anEnglishSlotGrammar basedparserasinputto thechunker
84,A,N03-1028,P10-1038,1,"(Sha and Pereira, 2003; Culotta and McCallum, 2004)","The standard Viterbi algorithm for making predictions from a trained CRF is not tuned to minimize false positives. To address this difficulty, we use the forward-backward algorithm (Sha and Pereira, 2003; Culotta and McCallum, 2004) to estimate separately for each position the probability of a hyphen at that position. Then, we only allow a hyphen if this probability is over a high threshold such as 0.9.",The standard algorithm for making predictions from trained is not tuned to minimize false positives address this difficulty we use the forward backward algorithm to estimate separately for each position the probability of hyphen at that position Then we only allow hyphen if this probability is over high threshold such as
85,A,N03-1028,W05-0622,0,"(Sha and Pereira, 2003)","These models allow for the use of very large sets of arbitrary, overlapping and non-independent features. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003; Cohn et al., 2005), part-of-speech (PoS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of table data (Pinto et al., 2003), among other tasks. While CRFs have not been used to date for SRL, their close cousin, the maximum entropy model has been, with strong generalisation performance (Xue and Palmer, 2004; Lim et al., 2004).",These models allow for the use of very large sets of arbitrary overlapping and non independent features have been applied with impressive empirical results to the tasks of named entity recognition part of speech tagging noun phrase chunking and extraction of table data among other tasks While have not been used to date for their close cousin the maximum entropy model has been with strong generalisation performance
86,A,N03-1028,W06-1655,0,"(Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003)","We have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds. There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003). The current work indicates that these systems might be improved by moving to a semi-CRF model.",have presented the intuitive argument that the log odds may be advantageous because it does not exhibit the asymmetry of the log probability but it would be satisfying to justify the choice on more theoretical grounds There is significant volume of work exploring the use of for variety of chunking tasks including named entity recognition gene prediction shallow parsing and others The current work indicates that these systems might be improved by moving to semi model
87,A,N04-4028,C04-1081,1,"(Culotta and McCallum, 2004)","A confidence threshold of 0.9 is determined by cross-validation. Segment confidence is estimated using constrained forward-backward (Culotta and McCallum, 2004). The standard forward-backward algorithm (Rabiner, 1990) calculatesZx, the total likelihood of all label sequencesy given a sequencex.",confidence threshold of is determined by cross validation Segment confidence is estimated using constrained forward backward The standard forward backward algorithm calculatesZx the total likelihood of all label sequencesy given sequencex
88,A,N04-4028,D07-1068,0,"(Culotta and McCallum, 2004)","As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al. (2004) has applied linear-chain CRFs to Chinese word segmentation. It is calculated by constrained forwardbackward algorithm (Culotta and McCallum, 2004), and confident segments are added to the dictionary in order to improve segmentation accuracy. In this paper, we proposed a method for categorizing NEs in Wikipedia.",for the marginal probability to use as confidence measure shown in Figure has applied linear chain to word segmentation is calculated by constrained forwardbackward algorithm and confident segments are added to the dictionary in order to improve segmentation accuracy this paper we proposed method for categorizing in Wikipedia
89,A,N04-4028,D10-1095,0,"(Culotta and McCallum, 2004)","Here, the random method performing the worst, while KD-PC and KD-Fixed are the best, and as shown in (Dredze and Crammer, 2008), MinConfMargin outperforming MinMargin. Related Work: Most previous work has focused on confidence estimation for an entire example or some fields of an entry (Culotta and McCallum, 2004) using CRFs. (Kristjansson et al., 2004) show the utility of confidence estimation is extracted fields of an interactive information extraction system by high-lighting low confidence fields for the user. (Scheffer et al., 2001) estimate confidence of single token label in HMM based information extraction system by a method similar to the Delta method we used. (Ueffing and Ney, 2007) propose several methods for word level confidence estimation for the task of machine translation. One of the methods they use is very similar to the weighted and non-weighted K-best Viterbi methods we used with the proper adjustments to the machine translation task.",Here the random method performing the worst while and Fixed are the best and as shown in MinConfMargin outperforming MinMargin Related Work Most previous work has focused on confidence estimation for an entire example or some fields of an entry using show the utility of confidence estimation is extracted fields of an interactive information extraction system by high lighting low confidence fields for the user estimate confidence of single token label in based information extraction system by method similar to the method we used propose several methods for word level confidence estimation for the task of machine translation One of the methods they use is very similar to the weighted and non weighted best methods we used with the proper adjustments to the machine translation task
90,A,N04-4028,P08-1004,1,"(Culotta and McCallum, 2004)","H-CRF uses the probability distribution over the set of possible labels according to each O-CRF and R1-CRF as features. To obtain the probability at each position of a linear-chain CRF, the constrained forward-backward technique described in (Culotta and McCallum, 2004) is used. H-CRF also computes the Monge Elkan distance (Monge and Elkan, 1996) between the relations predicted by O-CRF and R1CRF and includes the result in the feature set.",uses the probability distribution over the set of possible labels according to each and as features obtain the probability at each position of linear chain the constrained forward backward technique described in is used also computes the Monge distance between the relations predicted by and and includes the result in the feature set
91,A,N04-4028,P09-1015,0,"(Abe and Mamitsuka, 1998; Miller et al., 2004; Culotta and McCallum, 2005)","The latter attempts to select its training set intelligently by requesting the labels of only those examples that are judged to be the most useful or informative. Numerous studies have demonstrated that active learners can make more efficient use of unlabelled data than do passive learners (Abe and Mamitsuka, 1998; Miller et al., 2004; Culotta and McCallum, 2005). However, relatively few researchers have applied active learning techniques to the L2P domain.",The latter attempts to select its training set intelligently by requesting the labels of only those examples that are judged to be the most useful or informative Numerous studies have demonstrated that active learners can make more efficient use of unlabelled data than do passive learners However relatively few researchers have applied active learning techniques to the domain
92,A,N04-4028,P10-1038,1,"(Sha and Pereira, 2003; Culotta and McCallum, 2004)","The standard Viterbi algorithm for making predictions from a trained CRF is not tuned to minimize false positives. To address this difficulty, we use the forward-backward algorithm (Sha and Pereira, 2003; Culotta and McCallum, 2004) to estimate separately for each position the probability of a hyphen at that position. Then, we only allow a hyphen if this probability is over a high threshold such as 0.9.",The standard algorithm for making predictions from trained is not tuned to minimize false positives address this difficulty we use the forward backward algorithm to estimate separately for each position the probability of hyphen at that position Then we only allow hyphen if this probability is over high threshold such as
93,A,N04-4028,P11-1149,0,"(Culotta and McCallum, 2004)","This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection.",This selection criterion provides an additional view different than the one used by the prediction model Multi view learning is well established idea implemented in methods such as co training explored by many previous works and applied to several processing tasks such as syntactic parsing machine translation speech relation extraction and dialog systems addition to sample selection we use confidence estimation as way to approximate the overall quality of the model and use it for model selection
94,A,N06-1040,C08-5001,0,"(McDonald et al., 2005; Mohri and Roark, 2006)","For the hypergraph case, the REA algorithm has been adapted for k-best derivations (Jiménez and Marzal, 2000; Huang and Chiang, 2005). Applications of this algorithm include k-best parsing (McDonald et al., 2005; Mohri and Roark, 2006) and machine translation (Chiang, 2007). It is also implemented as part of Dyna (Eisner et al., 2005), a generic langauge for dynamic programming.",For the hypergraph case the algorithm has been adapted for best derivations Applications of this algorithm include best parsing and machine translation is also implemented as part of generic langauge for dynamic programming
95,A,N06-1040,C10-1007,0,"(Mohri and Roark, 2006)","Note that the rules tend to use common tags with well-defined roles. By focusing on weighted loss as opposed to arc frequency, the classifier discovers structural zeros (Mohri and Roark, 2006), events which could have been observed, but were not. We consider this an improvement over the frequencybased length thresholds employed previously in tag-specific vine parsing.",Note that the rules tend to use common tags with well defined roles focusing on weighted loss as opposed to arc frequency the classifier discovers structural events which could have been observed but were not consider this an improvement over the frequencybased length thresholds employed previously in tag specific vine parsing
96,A,N06-1040,D08-1091,0,"(Mohri and Roark, 2006)","First, because multi-scale grammars are most effective when many productions share the same weight, sparsity is very desirable. In the present work, we exploit L1-regularization, though other techniques such as structural zeros (Mohri and Roark, 2006) could also potentially be used. Second, training requires repeated parsing, so we use coarse-to-fine chart caching to greatly accelerate each iteration.",because multi scale grammars are most effective when many productions share the same weight sparsity is very desirable the present work we exploit regularization though other techniques such as structural could also potentially be used training requires repeated parsing so we use coarse to fine chart caching to greatly accelerate each iteration
97,A,N06-1040,N07-1051,0,Mohri and Roark (2006),"In Petrov et al. (2006), some simple smoothing is also shown to be effective. It is interesting to note that these grammars capture many of the “structural zeros” described by Mohri and Roark (2006) and pruning rules with probability belowe−10 reduces the grammar size drastically without influencing parsing performance. Some of our methods and conclusions are relevant to all state-split grammars, such as Klein and Manning (2003) or Dreyer and Eisner (2006), while others apply most directly to the hierarchical case.",some simple smoothing is also shown to be effective is interesting to note that these grammars capture many of the structural described by and pruning rules with probability reduces the grammar size drastically without influencing parsing performance Some of our methods and conclusions are relevant to all state split grammars such as or while others apply most directly to the hierarchical case
98,A,P01-1048,N03-2018,1,"(Ang et al., 2002; Litman et al., 2001; Batliner et al., 2000)","The success of computer-based tutoring systems could increase if they predicted and adapted to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Although (Ang et al., 2002; Litman et al., 2001; Batliner et al., 2000) have hand-labeled naturally-occurring utterances in a variety of corpora for various emotions, then extracted acoustic, prosodic and lexical features and used machine-learning techniques to develop predictive models, little work to date has addressed emotion detection in computer-based educational settings. In this paper we describe preliminary annotation of positive, negative, and neutral emotions in a human-human tutoring corpus and discuss the results of pilot machine learning experiments whose goal is to develop computational models of specific emotional states (Section 3) for use in a spoken dialogue system (Section 2).",The success of computer based tutoring systems could increase if they predicted and adapted to student emotional states reinforcing positive states while rectifying negative states Although have hand labeled naturally occurring utterances in variety of corpora for various emotions then extracted acoustic prosodic and lexical features and used machine learning techniques to develop predictive models little work to date has addressed emotion detection in computer based educational settings this paper we describe preliminary annotation of positive negative and neutral emotions in human human tutoring corpus and discuss the results of pilot machine learning experiments whose goal is to develop computational models of specific emotional states for use in spoken dialogue system
99,A,P01-1048,N04-1026,2,"(Litman et al., 2001; Lee et al., 2001; Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003)","In natural interactions, however, speakers can convey emotions using other types of features, and can also combine acoustic-prosodic and other feature types. As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (Litman et al., 2001; Lee et al., 2001; Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time. With noisier data and fewer features, it is not surprising that acoustic-prosodic features alone have been found to be of less predictive utility in these studies, leading spoken dialogue researchers to supplement such features with features based on other sources of information (e.g., lexical, syntactic, discourse).",natural interactions however speakers can convey emotions using other types of features and can also combine acoustic prosodic and other feature types result of this mismatch recent work motivated by spoken dialogue applications has started to use naturally occurring speech to train emotion predictors but often predicts emotions using only acoustic prosodic features that would be automatically available to dialogue system in real time With noisier data and fewer features it is not surprising that acoustic prosodic features alone have been found to be of less predictive utility in these studies leading spoken dialogue researchers to supplement such features with features based on other sources of information
100,A,P01-1048,W01-1610,1,"(Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001)","The current paper focuses on user corrections, and looks at places where people �rst become aware of a system problem (\aware sites""). In other papers (Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. One of our major �ndings is that prosody, which had already been shown to be a good predictor of misrecognitions (Litman et al., 2000; Hirschberg et al., 2000), is also useful to correctly classify corrections and aware sites.",The current paper focuses on user corrections and looks at places where people become aware of system problem other papers we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these utterance categories of our major is that prosody which had already been shown to be good predictor of misrecognitions is also useful to correctly classify corrections and aware sites
101,A,P01-1048,W03-0205,0,"(Litman et al., 2001)","Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its “back-end”.",Other research projects have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems Moreover speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states and user responses to system errors that are useful for triggering system adaptation are thus currently developing speech based dialogue system that uses text based system as its back end
102,A,P01-1048,W03-0409,0,"(Litman et al., 2000; Hirschberg et al., 2001; Litman et al., 2001)","Moreover, one will expect very little noise or no noise at all when manually annotating WERBIN and CABIN. For more information on our tasks and features, see (Litman et al., 2000; Hirschberg et al., 2001; Litman et al., 2001). There are a number of dimensions where our tasks differ from the tasks from the previous study.",Moreover will expect very little noise or no noise at all when manually annotating and For more information on our tasks and features see There are number of dimensions where our tasks differ from the tasks from the previous study
103,A,P02-1031,P03-1002,3,"(Gildea and Palmer, 2002)","Central to this new way of extracting information from texts are systems that label predicate-argument structures on the output of full parsers. One such augmented parser, trained on data available from the PropBank project has been recently presented in (Gildea and Palmer, 2002). In this paper we describe a domain-independent IE paradigm that is based on predicate-argument structures identified automatically by two different methods: (1) the statistical method reported in (Gildea and Palmer, 2002); and (2) a new method based on inductive learning which obtains 17% higher Fscore over the first method when tested on the same data.",Central to this new way of extracting information from texts are systems that label predicate argument structures on the output of full parsers such augmented parser trained on data available from the project has been recently presented in this paper we describe domain independent paradigm that is based on predicate argument structures identified automatically by different methods the statistical method reported in and new method based on inductive learning which obtains higher over the method when tested on the same data
104,A,P02-1046,W04-2405,1,"(Abney, 2002)","In the original definition of co-training, (Blum and Mitchell, 1998) state conditional independence of the views as a required criterion for co-training to work. In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. He is proposing a greedy algorithm to maximize agreement on unlabelled data, which produces good results in a co-training experiment for named entity classification.",the original definition of co training state conditional independence of the views as required criterion for co training to work recent work shows that the independence assumption can be relaxed and co training is still effective under weaker independence assumption is proposing greedy algorithm to maximize agreement on unlabelled data which produces good results in co training experiment for named entity classification
105,A,P02-1053,P06-2079,1,Turney (2002),"There have been attempts on tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al. (2004) for details). There is a large body of work on classifying the polarity of a document (e.g., Pang et al. (2002), Turney (2002)), a sentence (e.g., Liu et al. (2003), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Gamon et al. (2005)), a phrase (e.g., Wilson et al. (2005)), and a specific object (such as a product) mentioned in a document (e.g., Morinaga et al. (2002), Yi et al. (2003), Popescu and Etzioni (2005)). Below we will center our discussion of related work around the four types of features we will explore for polarity classification.",There have been attempts on tackling this so called document level subjectivity classification task with very encouraging results There is large body of work on classifying the polarity of document sentence phrase and specific object mentioned in document Below we will center our discussion of related work around the types of features we will explore for polarity classification
106,A,P02-1053,P07-1056,0,"(Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004)","This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004). While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006).",This measure could for instance be used to select small set of domains to annotate whose trained classifiers would transfer well to many other domains Sentiment detection and classification has received considerable attention recently While movie reviews have been the most studied domain sentiment analysis has extended to number of new domains ranging from stock message boards to congressional floor debates
107,A,P03-1002,P07-1075,0,Surdeanu et al. (2003),"They reported that dependency relations are not reliable for the hard cases, which, in our opinion, need the extraction of discourse relations to supplement dependency relation paths. Surdeanu et al. (2003) applied semantic parsing to capture the predicate-argument sentence structure. They suggested that semantic parsing is useful to capture verb arguments, which may be connected by long-distance dependency paths.",They reported that dependency relations are not reliable for the hard cases which in our opinion need the extraction of discourse relations to supplement dependency relation paths applied semantic parsing to capture the predicate argument sentence structure They suggested that semantic parsing is useful to capture verb arguments which may be connected by long distance dependency paths
108,A,P03-1002,N04-1030,3,Surdeanu et al. (2003),"In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers.",this paper we propose machine learning algorithm for shallow semantic parsing extending the work of and others Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers
109,A,P03-1054,C04-1097,1,Klein & Manning (2003),"Experiments by Daumé et al (2002) and the parsing work of Charniak (2000) and others indicate that further lexicalization may yield some additional improvements for ordering. However, the parsing results of Klein & Manning (2003) involving unlexicalized grammars suggest that gains may be limited. For comparison, we encourage implementers of other sentence realization systems to conduct order-only evaluations using PTB data.",Experiments by and the parsing work of and others indicate that further lexicalization may yield some additional improvements for ordering However the parsing results of involving unlexicalized grammars suggest that gains may be limited For comparison we encourage implementers of other sentence realization systems to conduct order only evaluations using data
110,A,P03-1054,C04-1097,0,Klein & Manning (2003),"Experiments by Daumé et al (2002) and the parsing work of Charniak (2000) and others indicate that further lexicalization may yield some additional improvements for ordering. However, the parsing results of Klein & Manning (2003) involving unlexicalized grammars suggest that gains may be limited. For comparison, we encourage implementers of other sentence realization systems to conduct order-only evaluations using PTB data.",Experiments by and the parsing work of and others indicate that further lexicalization may yield some additional improvements for ordering However the parsing results of involving unlexicalized grammars suggest that gains may be limited For comparison we encourage implementers of other sentence realization systems to conduct order only evaluations using data
111,A,P03-1069,C04-1108,0,"(Lapata, 2003)","Based on the experiments, they propose another algorithm that utilizes chronological ordering with topical segmentation to separate sentences referring to a topic from ones referring to another. Lapata (Lapata, 2003) proposes another approach to information ordering based on a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts. Lapata estimates transitional probability between sentences by some attributes such as verbs (precedence relationships of verbs in the corpus), nouns (entity-based coherence by keeping track of the nouns) and dependencies (structure of sentences).",Based on the experiments they propose another algorithm that utilizes chronological ordering with topical segmentation to separate sentences referring to topic from ones referring to another proposes another approach to information ordering based on probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from corpus of domain specific texts estimates transitional probability between sentences by some attributes such as verbs nouns and dependencies
112,A,P03-1069,C10-2105,0,"(Lapata, 2003; Althaus et al., 2004)","It is known that the readability of a collection of sentences, a summary, can be greatly improved by appropriately ordering them (Barzilay et al., 2002). Features proposed to create the appropriate order include publication date of document (Barzilay et al., 2002), content words (Lapata, 2003; Althaus et al., 2004), and syntactic role of words (Barzilay and Lapata, 2005). Some approaches use machine learning to integrate these features (Soricut and Marcu, 2006; Elsner et al., 2007).",is known that the readability of collection of sentences summary can be greatly improved by appropriately ordering them proposed to create the appropriate order include publication date of document content words and syntactic role of words Some approaches use machine learning to integrate these features
113,A,P03-1069,C10-2170,0,(Lapata 2003),"In MDS, information ordering is often realized on the sentence level and treated as a coherence enhancement task. A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003). A different way to capture local coherence in sentence ordering is the Centering Theory (CT, Grosz et al. 1995)-inspired entity-transition approach, advocated by Barzilay and Lapata (2005, 2008).",information ordering is often realized on the sentence level and treated as coherence enhancement task simple ordering criterion is the chronological order of the events represented in the sentences which is often augmented with other ordering criteria such as lexical overlap lexical cohesion or syntactic features different way to capture local coherence in sentence ordering is the Centering Theory inspired entity transition approach advocated by
114,A,P03-1069,D07-1009,2,"(Lapata, 2003; Karamanis et al., 2004; Okazaki et al., 2004; Barzilay and Lapata, 2005; Bollegala et al., 2006; Elsner and Charniak, 2007)","We conclude the paper by presenting and discussing our results. Text Structuring The insertion task is closely related to the extensively studied problem of sentence ordering.3 Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (Lapata, 2003; Karamanis et al., 2004; Okazaki et al., 2004; Barzilay and Lapata, 2005; Bollegala et al., 2006; Elsner and Charniak, 2007). These methods induce a total ordering based on pairwise relations between sentences.",conclude the paper by presenting and discussing our results Text Structuring The insertion task is closely related to the extensively studied problem of sentence Most of the existing algorithms represent text structure as linear sequence and are driven by local coherence constraints These methods induce total ordering based on pairwise relations between sentences
115,A,P03-1069,P04-1050,0,"(Barzilay et al., 2002; Lapata, 2003)","In this work, we explored this question by developing an approach to text structuring purely based on Centering, in which the role of other factors is deliberately ignored. In accordance with recent work in the emerging field of text-to-text generation (Barzilay et al., 2002; Lapata, 2003), we assume that the input to text structuring is a set of clauses. The output of text structuring is merely an ordering of these clauses, rather than the tree-like structure of database facts often used in traditional deep generation (Reiter and Dale, 2000).",this work we explored this question by developing an approach to text structuring purely based on in which the role of other factors is deliberately ignored accordance with recent work in the emerging field of text to text generation we assume that the input to text structuring is set of clauses The output of text structuring is merely an ordering of these clauses rather than the tree like structure of database facts often used in traditional deep generation
116,A,P03-1069,P13-2016,2,"(Lapata, 2003)","Evaluation results on a news corpus show the effectiveness of our proposed method. 1 Introduction Ordering texts is an important task in many natural language processing (NLP) applications. It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers.",Evaluation results on news corpus show the effectiveness of our proposed method Introduction Ordering texts is an important task in many natural language processing applications is typically applicable in the text generation field both for concept to text generation and text totext generation such as multiple document summarization question answering and so on However ordering set of sentences into coherent text is still hard and challenging problem for computers
117,A,P04-1015,C08-2012,0,"(Collins and Roark, 2004; McDonald et al., 2005)","For example, for ����������� (economic development and law construction in Shanghai), in traditional dependency analysis, � � (Shanghai) would depend on the conjunction word �(and), since conjunction words are usually regarded as heads in coordinate structures. Although the relatedness may go downward from the head, it would be difficult to derive the relatedness between � � (Shanghai) and � � (economic) or ��(law), since the two words are even not heads of the conjuncts ����  (ecoAs to analysis of NPs, there have been a lot of work on statistical techniques for lexical dependency parsing of sentences (Collins and Roark, 2004; McDonald et al., 2005), and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available. However, these techniques are all meant to building a dependency tree, while the conceptual relatedness in NPs may form a graph, with multidependency allowed.",For example for in traditional dependency analysis would depend on the conjunction word since conjunction words are usually regarded as heads in coordinate structures Although the relatedness may go downward from the head it would be difficult to derive the relatedness between and or since the words are even not heads of the conjuncts ecoAs to analysis of there have been lot of work on statistical techniques for lexical dependency parsing of sentences and these techniques potentially can be used for analysis of if appropriate resources for are available However these techniques are all meant to building dependency tree while the conceptual relatedness in may form graph with multidependency allowed
118,A,P04-1015,D07-1009,0,"(Collins and Roark, 2004)","Higher level parameters will thus naturally be shared by all paragraphs within a single section. In fact, when the perceptron update rule of (Dekel et al., 2004) – which modifies the weights of every divergent node along the predicted and true paths – is used in the ranking framework, it becomes virtually identical with the standard, flat, ranking perceptron of Collins (2002).5 In contrast, our approach shares the idea of (Cesa-Bianchi et al., 2006a) that “if a parent class has been predicted wrongly, then errors in the children should not be taken into account.” We also view this as one of the key ideas of the incremental perceptron algorithm of (Collins and Roark, 2004), which searches through a complex decision space step-by-step and is immediately updated at the first wrong move. Our work fuses this idea of selective hierarchical updates with the simplicity of the perceptron algorithm and the flexibility of arbitrary feature sharing inherent in the ranking framework.",Higher level parameters will thus naturally be shared by all paragraphs within single section fact when the update rule of which modifies the weights of every divergent node along the predicted and true paths is used in the ranking framework it becomes virtually identical with the standard flat ranking of contrast our approach shares the idea of that if parent class has been predicted wrongly then errors in the children should not be taken into also view this as of the key ideas of the incremental algorithm of which searches through complex decision space step by step and is immediately updated at the wrong move Our work fuses this idea of selective hierarchical updates with the simplicity of the algorithm and the flexibility of arbitrary feature sharing inherent in the ranking framework
119,A,P04-1015,P05-1023,2,"(Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004)","Work on kernel methods in natural language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning.",Work on kernel methods in natural language has focussed on the definition of appropriate kernels for natural language tasks particular most of the work on parsing with kernel methods has focussed on kernels over parse trees These kernels have all been hand crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones while at the same time maintaining the tractability of learning
120,A,P04-1015,P06-1096,0,"(Collins and Roark, 2004; Roark et al., 2004)","Given featuresΦ and a corresponding set of parametersw, a standard classification rulef is to return the highest scoring output sentencey, maximizing over correspondencesh: In the phrase-based model, computing the argmax exactly is intractable, so we approximate f with beam decoding. To tune the parametersw of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle,w could have been tuned by maximizing conditional probability or maximizing margin.",Given and corresponding set of parametersw standard classification rulef is to return the highest scoring output sentencey maximizing over correspondencesh the phrase based model computing the argmax exactly is intractable so we approximate with beam decoding tune the parametersw of the model we use the averaged algorithm because of its efficiency and past success on various tasks principle could have been tuned by maximizing conditional probability or maximizing margin
121,A,P04-1015,P11-1069,1,"(Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et al., 2008)","In addition, high accuracy can be maintained by using a model which utilises a rich set of features for making each local decision (Nivre et al., 2006). Following recent work applying global discriminative models to large-scale structured prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et al., 2008), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&C parser. Using standard development and test sets from CCGbank, our shift-reduce parser gives a labeled F-measure of 85.53%, which is competitive with the 85.45% F-measure of the C&C parser on recovery of predicate-argument dependencies from CCGbank.",addition high accuracy can be maintained by using model which utilises rich set of features for making each local decision Following recent work applying global discriminative models to large scale structured prediction problems we build our shift reduce parser using global linear model and compare it with the chartbased parser Using standard development and test sets from our shift reduce parser gives labeled measure of which is competitive with the measure of the parser on recovery of predicate argument dependencies from
122,A,P04-1015,J07-4004,0,Collins and Roark (2004),"Two possible extensions, which we have not investigated, include defining dependency features which account for all three elements of the triple in a PP-attachment (Collins and Brooks 1995), and defining a rule feature which includes the grandparent node (Johnson 1998). Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark (2004), which allows more “global” features. For estimating both the normal-form model and the dependency model, the following expectation of each feature fi, with respect to some model Λ, is required: where ρ(S) is the set of all parses for sentence S, and λλλ is the vector of weights for Λ.",possible extensions which we have not investigated include defining dependency features which account for all elements of the triple in attachment and defining rule feature which includes the grandparent node Another alternative for future work is to compare the dynamic programming approach taken here with the beam search approach of which allows more global features For estimating both the normal form model and the dependency model the following expectation of each feature fi with respect to some model is required where is the set of all parses for sentence and is the vector of weights for
123,A,P04-1054,C08-1088,1,"(Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005)","Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances.",Therefore researchers turn to kernel based methods which avoids the burden of feature engineering through computing the similarity of discrete objects directly From prior work to current research kernel methods have been showing more and more potential in relation extraction The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances
124,A,P04-1054,P06-1104,2,Culotta and Sorensen (2004),"For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP. In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). This paper presents a novel composite kernel to explore diverse knowledge for relation extraction.",For example the kernels for structured natural language data such as parse tree kernel string kernel and graph kernel are example instances of the wellknown convolution in relation extraction typical work on kernel methods includes and This paper presents novel composite kernel to explore diverse knowledge for relation extraction
125,A,P04-1054,P08-2023,0,Culotta and Sorensen (2004),"Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph.",proposed kernel over parse trees which recursively matched nodes from roots to leaves in top down manner extended this work to estimate similarity between augmented dependency trees The above work was further advanced by who argued that the information to extract relation between entities can be typically captured by the shortest path between them in the dependency graph
126,A,P06-1114,S12-1065,0,"(Harabagiu and Hickl, 2006)","CLTE presents a similar problem, but with T and H written in different languages. During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (Harabagiu and Hickl, 2006) or machine translation (MT) (Mirkin et al., 2009; Padó et al., 2009). Therefore, CLTE may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval.",presents similar problem but with and written in different languages During the last years many authors have focused on resolving detection as solutions to this problem have proved to be useful in many natural language processing tasks such as question answering or machine translation Therefore may also be useful for related tasks in which more than one language is involved such as cross lingual question answering or cross lingual information retrieval
127,A,P06-1114,C10-1087,0,"(Harabagiu and Hickl, 2006)","Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference.",Under the relationship between text and textual assertion is defined such that entails if humans reading would infer that is most likely true has been successfully applied to variety of natural language processing applications including information extraction and question answering Yet most entailment systems have thus far paid little attention to discourse aspects of inference
128,A,P06-1114,P08-1081,0,"(Berger et al., 2000; Jeon et al., 2005; Cui et al., 2005; Harabagiu and Hickl, 2006; Dang et al., 2007)","In our other work (Cong et al., 2008), we proposed a supervised approach for question detection and an unsupervised approach for answer detection without considering context detection. Extensive research has been done in questionanswering, e.g. (Berger et al., 2000; Jeon et al., 2005; Cui et al., 2005; Harabagiu and Hickl, 2006; Dang et al., 2007). They mainly focus on constructing answer for certain types of question from a large document collection, and usually apply sophisticated linguistic analysis to both questions and the documents in the collection.",our other work we proposed supervised approach for question detection and an unsupervised approach for answer detection without considering context detection Extensive research has been done in questionanswering They mainly focus on constructing answer for certain types of question from large document collection and usually apply sophisticated linguistic analysis to both questions and the documents in the collection
129,A,P07-1033,C08-1003,1,"(Daumé III, 2007)","While these models have been shown to perform very well when tested on the text collection related to the training data (what we call the source domain), the performance drops considerably when testing on text from other domains (called target domains). In order to build models that perform well in new (target) domains we usually find two settings (Daumé III, 2007): In the semi-supervised setting the goal is to improve the system trained on the source domain using unlabeled data from the target domain, and the baseline is that of the system c© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license ( Some rights reserved. trained on the source domain.",While these models have been shown to perform very well when tested on the text collection related to the training data the performance drops considerably when testing on text from other domains order to build models that perform well in new domains we usually find settings the semi supervised setting the goal is to improve the system trained on the source domain using unlabeled data from the target domain and the baseline is that of the system Licensed under the Creative Commons Attribution Noncommercial Share Alike Unported license Some rights reserved trained on the source domain
130,A,P07-1033,C10-1145,1,"(Daumé III, 2007)","The document collection for entity linking is commonly from other domains, but not Wikipedia. To benefit from Wikipedia data, we introduce a domain adaption approach (Daumé III, 2007) which is suitable for this work since we have enough “target” domain data. The approach is to augment the feature vectors of the instances.",The document collection for entity linking is commonly from other domains but not Wikipedia benefit from Wikipedia data we introduce domain adaption approach which is suitable for this work since we have enough target domain data The approach is to augment the feature vectors of the instances
131,A,P07-1033,P10-2056,3,"(Daume III, 2007; Finkel and Manning, 2009)","Global interpolation doesn’t take such variability into account and all predictions are weighted across models identically, regardless of the context. In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and Manning, 2009) for adapting a conditional maximum entropy (ME) LM (Rosenfeld, 1996) to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data. The main contribution of this paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class-based speedup technique (Goodman, 2001) to adapt ME LMs in large-vocabulary speech recognition when the amount of target data is small.",Global interpolation doesn take such variability into account and all predictions are weighted across models identically regardless of the context this paper we investigate recently proposed adaptation approach for adapting conditional maximum entropy to new domain given large corpus of out of domain training data and small corpus of in domain data The main contribution of this paper is that we show how the suggested hierarchical adaptation can be used with suitable priors and combined with the class based speedup technique to adapt in large vocabulary speech recognition when the amount of target data is small
132,A,P07-1056,C08-1135,0,Blitzer et al. (2007),Aue  and  Gamon  (2005)  attempt  to  solve  the  problem of  the  absence  of large  amounts  of  labeled  data  by  customizing sentiment classifiers to new domains using training data from other domains. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning. Read  (2005)  also  observed  significant  differences between the accuracy of classification of reviews in the same domain but published in different time periods.,attempt to solve the problem of the absence of large amounts of labeled data by customizing sentiment classifiers to new domains using training data from other domains investigate domain adaptation for sentiment classifiers using structural correspondence learning also observed significant differences between the accuracy of classification of reviews in the same domain but published in different time periods
133,A,P07-1056,P08-2059,2,"(Blitzer et al., 2007)","The algorithm updated the online classifier and evaluated it on held out test data to measure learning progress. We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters (Lewis et al., 2004) (used by Tong and Koller) and sentiment classification (Blitzer et al., 2007) and spam (Bickel, 2006). For each dataset we extracted binary unigram features and sentiment was prepared according to Blitzer et al. (2007).",The algorithm updated the online classifier and evaluated it on held out test data to measure learning progress selected binary datasets for evaluation and and sentiment classification and spam For each dataset we extracted binary unigram features and sentiment was prepared according to
134,A,P07-1056,P08-2065,2,Blitzer et al. (2007),Research into sentiment classification over multiple domains remains sparse. It is worth noting that Blitzer et al. (2007) deal with the domain adaptation problem for sentiment classification where labeled data from one domain is used to train a classifier for classifying data from a different domain. Our work focuses on the problem of how to make multiple domains ‘help each other’ when all contain some labeled samples.,Research into sentiment classification over multiple domains remains sparse is worth noting that deal with the domain adaptation problem for sentiment classification where labeled data from domain is used to train classifier for classifying data from different domain Our work focuses on the problem of how to make multiple domains help each other when all contain some labeled samples
135,A,P07-1056,P11-1014,3,"(Blitzer et al., 2007)","A classifier trained on one domain might not perform well on a different domain because it would fail to learn the sentiment of the unseen words. Work in cross-domain sentiment classification (Blitzer et al., 2007) focuses on the challenge of training a classifier from one or more domains (source domains) and applying the trained classifier in a different domain (target domain). A crossdomain sentiment classification system must overcome two main challenges.",trained on domain might not perform well on different domain because it would fail to learn the sentiment of the unseen words Work in cross domain sentiment classification focuses on the challenge of training from or more domains and applying the trained in different domain crossdomain sentiment classification system must overcome main challenges
136,A,P08-2026,P11-2049,0,"(Clegg and Shepherd, 2007; McClosky and Charniak, 2008)","However, manually creating a comprehensive set of such lexico-syntactic scope rules is a laborious and time-consuming process. In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008). Instead, we attempted to automatically extract lexico-syntactic scope rules from the BioScope corpus, relying only on consistent (but not necessarily accurate) parse tree representations.",However manually creating comprehensive set of such lexico syntactic scope rules is laborious and time consuming process addition such an approach relies heavily on the availability of accurately parsed sentences which could be problematic for domains such as biomedical texts Instead we attempted to automatically extract lexico syntactic scope rules from the BioScope corpus relying only on consistent parse tree representations
137,A,P09-1113,D10-1099,2,"(Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010)","It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations. A particularly attractive approach to relation extraction is based on distant supervision.1 Here in place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor.",also includes the prediction of entity types such as country citytown or person if we consider entity types as unary relations particularly attractive approach to relation extraction is based on distant Here in place of annotated text only an existing knowledge base is needed to train relation extractor The facts in the are heuristically aligned to an unlabelled training corpus and the resulting alignment is the basis for learning the extractor
138,A,P09-1113,N13-1008,2,"(Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010)","However, labeling textual relations is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema.",However labeling textual relations is time consuming and difficult leading to significant recent interest in distantly supervised learning Here aligns existing database records with the sentences in which these records have been rendered labeling the from this labeling we can train machine learning system as before However this method relies on the availability of large database that has the desired schema
139,A,P11-2071,D12-1003,1,"(Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daumé III and Jagarlamudi, 2011)","Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daumé III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1Although Vulić et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. different types of characters (Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011).",Therefore the context similarity based methods could not find accurate translation pairs if using small seed lexicon Some of the previous methods tried to alleviate the problem of the limited seed lexicon size while others did not require any seed lexicon However they suffer the problems of high computational cost sensitivity to parameters low accuracy and ineffectiveness for language pairs with regarded document aligned texts such as texts on Wikipedia as comparable corpora we do not limit comparable corpora to these kinds of texts different types of characters
140,A,P11-2071,D13-1109,2,Daumé III and Jagarlamudi (2011),"It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daumé III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain text in order to do domain adaptation. Although that work shows significant MT improvements, it is based primarily on distributional similarity, thus making it difficult to learn translations for low frequency source words with sparse word context counts.",is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings for example by crowdsourcing or pivoting through related languages mine translations for high frequency words in text in order to do domain adaptation Although that work shows significant improvements it is based primarily on distributional similarity thus making it difficult to learn translations for low frequency source words with sparse word context counts
141,A,P11-2071,P13-2072,0,"(Daumé III and Jagarlamudi, 2011; Clark et al., 2012)","However, a significant feature engineering effort is still required from practitioners. When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daumé III and Jagarlamudi, 2011; Clark et al., 2012). In the related field of web search ranking, automatically learned non-linear features have brought dramatic improvements in quality (Burges et al., 2005; Wu ∗This research was conducted during the author’s internship at Microsoft Research et al., 2010).",However significant feature engineering effort is still required from practitioners When linear model does not fit well researchers are careful to manually add important feature conjunctions as for example the related field of web search ranking automatically learned non linear features have brought dramatic improvements in quality
142,A,P94-1013,W95-0104,3,"[Yarowsky, 1994]","The main parameter  to tune for the method of context words is k, the half-width of the context window. Previous work [Yarowsky, 1994] shows that smaller values of k (3 or 4) work well for resolving local syntactic ambiguities, while larger values (20 to 50) are suitable for resolving semantic ambiguities. We tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that  k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes fl'om local syntax.",The main parameter to tune for the method of context words is the width of the context window Previous work shows that smaller values of work well for resolving local syntactic ambiguities while larger values are suitable for resolving semantic ambiguities tried the values and on some practice confusion sets and found that generally did best indicating that most of the action for our task and confusion sets comes local syntax
143,A,P95-1026,W99-0613,3,(Yarowsky 95),"We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).",present algorithms The method uses similar algorithm to that of with modifications motivated by The algorithm extends ideas from boosting algorithms designed for supervised learning tasks to the framework suggested by
144,A,P95-1026,P02-1046,3,"(Yarowsky, 1995)","The plenitude of unlabeled natural language data, and the paucity of labeled data, have made bootstrapping a topic of interest in computational linguistics. Current work has been spurred by two papers, (Yarowsky, 1995) and (Blum and Mitchell, 1998). Blum and Mitchell propose a conditional independence assumption to account for the efficacy of their algorithm, called co-training, and they give a proof based on that conditional independence assumption.",The plenitude of unlabeled natural language data and the paucity of labeled data have made bootstrapping topic of interest in computational linguistics Current work has been spurred by papers and and propose conditional independence assumption to account for the efficacy of their algorithm called co training and they give proof based on that conditional independence assumption
145,A,P95-1026,J04-3004,3,The Yarowsky (1995),"For many language-processing tasks, there are an abundance of unlabeled data, but labeled data are lacking and too expensive to create in large quantities, making bootstrapping techniques desirable. The Yarowsky (1995) algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics. In brief, it consists of two loops.",For many language processing tasks there are an abundance of unlabeled data but labeled data are lacking and too expensive to create in large quantities making bootstrapping techniques desirable algorithm was one of the bootstrapping algorithms to become widely known in computational linguistics brief it consists of loops
146,A,P95-1026,C08-1135,0,Yarowsky  (1995),"The sentiment of a document is calculated as the average semantic orientation of all such phrases. Yarowsky  (1995)  describes  a  'semi-unsupervised' approach to the problem of sense disambiguation  of  words,  also  using  a  set  of  initial seeds, in this case a few high quality sense annotations. These annotations are used to start an iterative process of learning information about the contexts  in  which  senses  of  words  appear,  in each iteration labeling senses of previously unlabeled  word  tokens  using  information  from the previous iteration.",The sentiment of document is calculated as the average semantic orientation of all such phrases describes unsupervised approach to the problem of sense disambiguation of words also using set of initial seeds in this case few high quality sense annotations These annotations are used to start an iterative process of learning information about the contexts in which senses of words appear in each iteration labeling senses of previously unlabeled word tokens using information from the previous iteration
147,A,P95-1026,P07-1125,0,Yarowsky (1995),"Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples for which the learner is most confident. Early work by Yarowsky (1995) falls within this framework. Banko and Brill (2001) use ‘bagging’ and agreement to measure confidence on unlabelled samples, and more recently McClosky et al. (2006) use selftraining for improving parse reranking.",Self training is an alternative single view algorithm in which labelled pool is incrementally enlarged with unlabelled samples for which the learner is most confident Early work by falls within this framework use bagging and agreement to measure confidence on unlabelled samples and more recently use selftraining for improving parse reranking
148,A,P96-1025,A00-2015,1,Collins (1996),"In the Japanese computat ional  linguistics community,  Shirai et al. (1995) employed Minami (1974)'s theory on scope embedding preference of Japanese subordinate clauses and applied it to rule-based Japanese dependency analysis. However, in their approach, since categories of subordinate clauses are obtained by manually analyzing a small number  of sentences, their coverage against a large corpus such as E D R  bracketed corpus (EDR, 1995) is quite low. 2 In order to realize a broad coverage and high performance dependency analysis of Japanese sentences which exploits scope embedding preference of subordinate  clauses, we propose a corpus-based and statistical alternative to the rule-based manual  approach (section 3). 3 clearly shows that dependency ambiguities of subordinate clauses are among the most problematic source of syntactic ambiguities in a Japanese sentence. 2In our implementation, the coverage of the categories of Shirai et al. (1995) is only 30% for all the subordinate clauses included in the whole EDR corpus. ~Previous works on statistical dependency analysis include Fujio and Matsumoto (1998) and Haruno et al. (1998) in Japanese analysis as well as Lafferty et al. (1992), Eisner (1996), and Collins (1996) in English analysis. In later sections, we discuss the advantages of our approach over several closely related previous works. particle (base) conjunctive-particle (volitional) Tenki-ga yoi-kara dekakeyou (Chunking) English Translation weather subject fine because let's go out First, we formalize the problem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other.",the computat ional linguistics community employed theory on scope embedding preference of subordinate clauses and applied it to rule based dependency analysis However in their approach since categories of subordinate clauses are obtained by manually analyzing small number of sentences their coverage against large corpus such as bracketed corpus is quite low order to realize broad coverage and high performance dependency analysis of sentences which exploits scope embedding preference of subordinate clauses we propose corpus based and statistical alternative to the rule based manual approach clearly shows that dependency ambiguities of subordinate clauses are among the most problematic source of syntactic ambiguities in sentence our implementation the coverage of the categories of is only for all the subordinate clauses included in the whole corpus works on statistical dependency analysis include and in analysis as well as and in analysis later sections we discuss the advantages of our approach over several closely related previous works particle conjunctive particle ga yoi kara dekakeyou Translation weather subject fine because let go out we formalize the problem of deciding scope embedding preference as classification problem in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which of given subordinate clauses has broader scope than the other
149,A,P96-1025,A97-1004,0,"(Brill, 1994; Collins, 1996)","The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)). Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992)).",The task of identifying sentence boundaries in text has not received as much attention as it deserves Many freely available natural language processing tools require their input to be divided into sentences but make no mention of how to accomplish this Others perform the division implicitly without discussing performance
150,A,P96-1025,C00-1017,0,"(Collins, 1996; Ratnaparkhi, 1997)","We do not argue for the psychological plausibility of SCFG parsers (or the parent-encoded variant) per se. Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models, since obtaining similar results for more sophisticated models (e.g. (Collins, 1996; Ratnaparkhi, 1997)) might have been attributed to special properties of these models. Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by (Jurafsky, 1996) and (Crocker and Brants, to appear).",do not argue for the psychological plausibility of parsers per se Our investigation of these models was motivated rather by our desire to obtain generalizable result for these simple and well understood models since obtaining similar results for more sophisticated models might have been attributed to special properties of these models Rather the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by and
151,A,P96-1025,C00-1029,0,Collins (1996),"We have also shown the similarity-class technique to be superior to using a �xed level of generalisation in WordNet. Further work will look at how to integrate probabilities such as p(cjv; r) into a model of dependency structure, similar to that of Collins (1996) and Collins (1997), which can be used for parse selection. However, knowledge of selectional preferences cannot by itself solve the problem of structural disambiguation, and this further work will also look at using additional knowledge, such as subcategorisation information.",have also shown the similarity class technique to be superior to using level of generalisation in WordNet Further work will look at how to integrate probabilities such as into model of dependency structure similar to that of and which can be used for parse selection However knowledge of selectional preferences can not by itself solve the problem of structural disambiguation and this further work will also look at using additional knowledge such as subcategorisation information
152,A,P96-1025,C00-1043,1,"(Collins, 1996)","World knowledge axioms can also be easily derived by processing the gloss de�nitions of WordNet (Fellbaum 1998). Semantic Transformations Instead of producing only a phrasal parse for the question and answer, we make use of one of the new statistical parsers of large real-world text coverage (Collins, 1996). The parse trees produced by such a parser can be easily translated into a semantic representation that (1) comprises all the phrase heads and (2) captures their inter-relationships by anonymous links.",World knowledge axioms can also be easily derived by processing the gloss of WordNet Semantic Transformations Instead of producing only phrasal parse for the question and answer we make use of of the new statistical parsers of large real world text coverage The parse trees produced by such parser can be easily translated into semantic representation that comprises all the phrase heads and captures their inter relationships by anonymous links
153,A,P96-1025,C00-1051,1,Collins (1996),"In this paper, we consider the task of deciding the dependency structure of a Japanese input sentence. Note that, while we restrict our discussion to analysis of Japanese sentences in this paper, what we present below should also be straightforwardly applicable to more wideranged tasks such as English dependency analysis just like the problem setting considered by Collins (1996). Given an input sentence s as a sequence of Bunsetsu-phrases (BPs)1, b1 b2 : : : bn, our task is to identify their inter-BP dependency structure R = fr(bi; bj)ji = 1; : : : ; ng, where r(bi; bj) denotes that bi depends on (or modi�es) bj .",this paper we consider the task of deciding the dependency structure of input sentence Note that while we restrict our discussion to analysis of sentences in this paper what we present below should also be straightforwardly applicable to more wideranged tasks such as dependency analysis just like the problem setting considered by Given an input sentence as sequence of phrases bn our task is to identify their inter dependency structure fr ji ng where denotes that bi depends on bj
154,A,P96-1025,C00-2141,0,Collins(1996),"An open test on 2780 Chinese real text sentences showed the satisfying results: 94%(92%) precision for the words with multiple (single) boundary tag output. in natural language processing for a long time. As the development of corpus linguistics, many statistics-based parsers were proposed, such as Magerman(1995)’s statistical decision tree parser, Collins(1996)’s bigram dependency model parser, Ratnaparkhi(1997)’s maximum entropy model parser. All of them tried to get the complete parse trees of the input sentences, based on the statistical data extracted from an annotated corpus.",open test on real text sentences showed the satisfying results precision for the words with multiple boundary tag output in natural language processing for long time the development of corpus linguistics many statistics based parsers were proposed such as statistical decision tree parser bigram dependency model parser maximum entropy model parser All of them tried to get the complete parse trees of the input sentences based on the statistical data extracted from an annotated corpus
155,A,P96-1025,C00-2146,0,"(Charniak, 1995; Collins, 1996)","There have been various researches to disambiguate the structural ambiguities in parsing. Lexical and contextual information has been shown to be most crucial for many parsing decisions, such as prepositional-phrase attachment (Hindle and Rooth, 1993). (Charniak, 1995; Collins, 1996) use the lexical information � This research was partially supported by KOSEF special basic research program (1997.9 � 2000.8). and (Magerman and Marcus, 1991; Magerman and Weir, 1992) use the contextual information for structural disambiguation. But, there have been few researches that used probability information for reducing the spurious ambiguities in choosing the most plausible parse tree of CCG formalism, especially for morpho-syntactic parsing of agglutinative language.",There have been various researches to disambiguate the structural ambiguities in parsing Lexical and contextual information has been shown to be most crucial for many parsing decisions such as prepositional phrase attachment use the lexical information This research was partially supported by special basic research program and use the contextual information for structural disambiguation But there have been few researches that used probability information for reducing the spurious ambiguities in choosing the most plausible parse tree of formalism especially for morpho syntactic parsing of agglutinative language
156,A,P96-1025,C02-1112,1,"(Collins, 1996)","They use a measure of semantic distance based on WordNet to find similar features. The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase. Unfortunately, they do not provide a comparison with a baseline system that would only use basic features.",They use measure of semantic distance based on WordNet to find similar features The features are extracted using statistical parser and consist of the head and modifiers of each phrase Unfortunately they do not provide comparison with baseline system that would only use basic features
157,A,P96-1025,E03-1051,0,"(Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)","Deciding about noun versus verb attachment of PPs is a known hard task in parsing, since it is understood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic ""world"" knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997).",Deciding about noun versus verb attachment of is known hard task in parsing since it is understood to involve knowing lexical preferences verb subcategorization fixed phrases but also semantic and pragmatic world knowledge typical current parser interleaves attachment with all its other disambiguation tasks However because of its interesting complexity line of work has concentrated on studying the task in isolation
158,A,P96-1025,E09-1097,2,Collins (1996),"Each edge has a pair of associated weights, one for each direction, defined by the functions : E×D → R, based on a probabilistic model of dependency relations. To calculate the edge weights, we adapt the definition of Collins (1996) to use direction rather than relation type (represented in the original as triples of non-terminals). Given a corpus, for some edgee = (u, v) ∈ E and direction d ∈ D, we calculate the edge weight as: We define the set of part-of-speech (PoS) tagsP and a functionpos: V → P, which maps vertices (representing words) to their PoS, to calculate the probability of a dependency relation, defined as: wherecnt((u, pos(u)), (v, pos(v)), d) is the number of times where(v, pos(v)) and (u, pos(u)) are seen in a sentence in the training data, and (v, pos(v)) modifies (u, pos(u)) in direction d.",Each edge has pair of associated weights for each direction defined by the functions based on probabilistic model of dependency relations calculate the edge weights we adapt the definition of to use direction rather than relation type Given corpus for some edgee and direction we calculate the edge weight as define the set of part of speech tagsP and which maps vertices to their to calculate the probability of dependency relation defined as wherecnt is the number of times where and are seen in sentence in the training data and modifies in direction
159,A,P96-1025,E99-1025,0,Collins (1996),"The need to impose structure leads to the need to have robust parsers. There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as Abney (1990), Grishman (1995), and Hobbs et al. (1997)) and Statistical Parsing (such as Charniak (1996), Magerman (1995), and Collins (1996)). Srinivas (1997a) has presented a different approach called supertagging that  integrates linguistically motivated lexical descriptions with the robustness of statistical techniques.",The need to impose structure leads to the need to have robust parsers There have been main robust parsing paradigms Finite State Grammar based approaches and Statistical Parsing has presented different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques
160,A,P96-1025,J06-2003,1,(Collins 1996),"There are many sentences that present two or more pieces of information. In such cases, the program splits a sentence into coordinated clauses (or coordinated verb phrases) by using a parser (Collins 1996) to distinguish when a coordinating conjunction (and, but, whereas) is conjoining two main clauses or two parts of a complex verb phrase. From 60 randomly selected sentences, 52 were correctly dealt with (41 needed no split, 11 were correctly split).",There are many sentences that present or more pieces of information such cases the program splits sentence into coordinated clauses by using parser to distinguish when coordinating conjunction is conjoining main clauses or parts of complex verb phrase From randomly selected sentences were correctly dealt with
161,A,P96-1025,J08-3003,2,(Collins 1996),"A parsing algorithm for building the dependency analyses (Eisner 1996; Sekine, Uchimoto, and Isahara 2000) 2. A conditional probability model to score the analyses (Collins 1996) Table 1 Unlabeled attachment scores and unlabeled word-to-word scores for the baseline parsers. Attach-to-next (first IG) 56.0 63.3 Attach-to-next (last IG) 54.1 63.3 Rule-based 70.5 79.3 3.",parsing algorithm for building the dependency analyses conditional probability model to score the analyses Table Unlabeled attachment scores and unlabeled word to word scores for the baseline parsers to next to next based
162,A,P96-1025,J11-3004,3,Collins (1996),"White rectangles in an item represent intervals of nodes that have been assigned a head by the parser, and dark squares represent nodes that have no head. One of the most straightforward projective dependency parsing strategies was introduced by Collins (1996), and is based on the CYK bottom–up parsing strategy (Kasami 1965; Younger 1967). Collins’s parser works with dependency trees which are linked to each other by creating links between their heads.",White rectangles in an item represent intervals of nodes that have been assigned head by the parser and dark squares represent nodes that have no head of the most straightforward projective dependency parsing strategies was introduced by and is based on the parsing strategy parser works with dependency trees which are linked to each other by creating links between their heads
163,A,P96-1025,M98-1009,1,"Collins (1996, 1997)","Note that nodes with semantic labels ending in “-r” are MUC reportable names and descriptors. Statistical Model In SIFT’s statistical model, augmented parse trees are generated according to a process similar to that described in Collins (1996, 1997). For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward.",Note that nodes with semantic labels ending in are reportable names and descriptors Statistical Model statistical model augmented parse trees are generated according to process similar to that described in For each constituent the head is generated followed by the modifiers which are generated from the head outward
164,A,P96-1025,P10-1002,2,"(Collins, 1996)","Previous graph-based dependency models usually use the index distance of wordi and wordj 1We exclude thein between features of McDonald et al. (2005a) since preliminary experiments show that these features bring no improvement to the word-pair classification model. to enrich the features with word distance information. However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of (Collins, 1996), namedCollins distance for convenience. A Collins distance comprises the answers of 6 questions: • Is there a comma immediately preceding the second of wordi and wordj? Besides the original features generated according to the templates in Table 1, the enhanced features with Collins distance as postfixes are also used in training and decoding of the word-pair classifier.",Previous graph based dependency models usually use the index distance of wordi and wordj exclude thein between features of since preliminary experiments show that these features bring no improvement to the word pair classification model to enrich the features with word distance information However in order to utilize some syntax information between the pair of words we adopt the syntactic distance representation of namedCollins distance for convenience distance comprises the answers of questions there comma immediately preceding the of wordi and wordj Besides the original features generated according to the templates in Table the enhanced features with distance as postfixes are also used in training and decoding of the word pair classifier
165,A,P97-1003,P98-1091,0,Collins (1997),"Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995), Charniak (1997), Collins (1997), and Goodman (1997). A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.",one might propagate lexical information upward through the productions Examples of formalisms using this approach include the work of and more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures
166,A,P97-1003,P98-1106,1,"(Collins, 1997)","Pseudo-Projectivity, A Polynomially Parsable Non-Projective Dependency Grammar Pseudo-Projectivity: A Polynomially Parsable Non-Projective Dependency Grammar S y l v a i n  K a h a n e *  and A l e x i s  N a s r  t and O w e n  R a m b o w t • TALANA Universit@ Paris 7 ( s k 0 c c r .  j u s s i e u . f r ) t LIA Universit@ d 'Avignon ( a l e x i s .  n a s r © l i a ,  u n i v - a v i g n o n ,  f r ) :~CoGenTex, Inc. (owenOcogentex.com) Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesni~re's work from the thirties3 Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not. One problem that has posed an impediment to more wide-spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency grammar which is not restricted to projective analyses.",Pseudo Projectivity Polynomially Parsable Non Projective Dependency Grammar Pseudo Projectivity Polynomially Parsable Non Projective Dependency Grammar and and Universit Paris Universit Inc Dependency grammar has long tradition in syntactic theory dating back to at least work from the Recently it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words which is what dependency grammars model explicitly do but context free phrasestructure grammars do not problem that has posed an impediment to more wide spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency grammar which is not restricted to projective analyses
167,A,P97-1003,P98-2148,0,"(Jelinek et al., 1994; Collins, 1997)","As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et ~1., 1989) is used for model description. Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that  every parent receives a headword from the head-child.",for there have been researches in which stochastic context free grammar is used for model description Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models these models every headword is propagated up through the derivation tree such that every parent receives headword from the head child
168,A,P97-1003,P98-2234,0,"(Collins, 1997)","One just needs those easy-to-find constructs to determine the category of an attachment problem. The 75.4% results may seen low compared to parsing results like the 88% precision and recall in (Collins, 1997), but those parsing results include many easier-to-parse constructs. (Manning and Carpenter, 1997) presents the VNPN example phrase ""saw the man with a telescope"", where attaching the preposition incorrectly can still result in 80% (4 of 5) recall, 100% precision and no crossing brackets. Of the 4 recalled constructs, 3 are easy-to-parse: 2 correspond to noun groups and 1 is the parse top level.",just needs those easy to find constructs to determine the category of an attachment problem The results may seen low compared to parsing results like the precision and recall in but those parsing results include many easier to parse constructs presents the example phrase saw the man with telescope where attaching the preposition incorrectly can still result in recall precision and no crossing brackets the recalled constructs are easy to parse correspond to noun groups and is the parse top level
169,A,P97-1003,P99-1059,1,"(Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997)","The acceptability of ""Nora convened the "" The authors were supported respectively under ARPA Grant N6600194-C-6043 ""Human Language Technology"" and Ministero dell'Universitk e della Ricerca Scientifica e Tecnologica project ""Methodologies and Tools of High Performance Systems for Multimedia Applications."" party"" then depends on the grammar writer's assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation, i The chart parsing algorithms used by most of the above authors run in time O(nS), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size O(n 2) in practice).",The acceptability of convened the The authors were supported respectively under Grant Human Language Technology and della Ricerca Scientifica Tecnologica project Methodologies and Tools of High Performance Systems for Multimedia Applications party then depends on the grammar writer assessment of whether parties can be convened Several recent real world parsers have improved state of the art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars The rationale is that soft selectional restrictions play crucial role in disambiguation The chart parsing algorithms used by most of the above authors run in time because bilexical grammars are enormous the part of the grammar relevant to length input has size in practice
170,A,P97-1003,P99-1065,3,(Collins 97),These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.,These differences are likely to pose new problems for techniques that have been developed on describe our experience in building on the parsing model of Our final results dependency accuracy represent good progress towards the accuracy of the parser on text
171,A,P97-1003,P00-1058,2,"(Magerman, 1995; Collins, 1997)","Of course, the price that the PTAG model pays is sparser data; the backo� model must therefore be chosen carefully. We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (Magerman, 1995; Collins, 1997). For each node �, these rules classify exactly one child of � as a head and the rest as either arguments or adjuncts.",course the price that the model pays is sparser data the model must therefore be chosen carefully want to extract from the Penn Treebank an whose derivations mirror the dependency analysis implicit in the head percolation rules of For each node these rules classify exactly child of as head and the rest as either arguments or adjuncts
172,A,P97-1003,P00-1060,0,"[Collins, 1997]","Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions. In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types’ effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types’ or feature types combination’s predictive power for syntactic structure.",Our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws series of interesting conclusions the field of statistical parsing various probabilistic evaluation models have been proposed where different models use different feature types How to evaluate the different feature types effects for syntactic parsing The paper proposes an information theory based feature types analysis model which uses the measures of predictive information quantity predictive information gain predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types or feature types combination predictive power for syntactic structure
173,A,P97-1003,P00-1065,1,"(Collins, 1997)","The system is a statistical one, based on training a classi�er on a labeled training set, and testing on an unlabeled test set. The system is trained by �rst using the Collins parser (Collins, 1997) to parse the 36,995 training sentences, matching annotated frame elements to parse constituents, and extracting various features from the string of words and the parse tree. During testing, the parser is run on the test sentences and the same features extracted.",The system is statistical based on training on labeled training set and testing on an unlabeled test set The system is trained by using the parser to parse the training sentences matching annotated frame elements to parse constituents and extracting various features from the string of words and the parse tree During testing the parser is run on the test sentences and the same features extracted
174,A,P97-1003,P02-1018,0,Collins (1997),"This research was supported by NSF awards DMS 0074276 and ITR IIS 0085940. useful tasks. Broad coverage syntactic parsers with good performance have recently become available (Charniak, 2000; Collins, 2000), but these typically produce as output a parse tree that only encodes local syntactic information, i.e., a tree that does not include any “empty nodes”. (Collins (1997) discusses the recovery of one kind of empty node, viz., WH-traces). This paper describes a simple patternmatching algorithm for post-processing the output of such parsers to add a wide variety of empty nodes to its parse trees.",This research was supported by awards and useful tasks coverage syntactic parsers with good performance have recently become available but these typically produce as output parse tree that only encodes local syntactic information tree that does not include any empty nodes This paper describes simple patternmatching algorithm for post processing the output of such parsers to add wide variety of empty nodes to its parse trees
175,A,P97-1003,P02-1031,1,Collins (1997),"Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the e�ect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction.",Even for single predicate semantic arguments often have multiple syntactic realizations as shown by the following paraphrases Correctly identifying the semantic roles of the sentence constituents is crucial part of interpreting text and in addition to forming an important part of the information extraction problem can serve as an intermediate step in machine translation or automatic summarization this paper we examine how the information provided by modern statistical parsers such as and contributes to solving this problem measure the of parser accuracy on semantic role prediction from parse trees and determine whether complete tree is indeed necessary for accurate role prediction
176,A,P97-1003,P02-1043,2,Collins (1997),"We too can extend the baseline model described in the previous section by including more features. Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly. In order to estimate the conditional probabilities of our model, we recursively smooth empirical estimatesêi of specific conditional distributions with (possible smoothed) estimates of less specific distributionsẽi�1, using linear interpolation: λ is a smoothing weight which depends on the particular distribution.2 When defining models, we will indicate a backoff level with a # sign between conditioning variables, eg.",too can extend the baseline model described in the previous section by including more features Like the models of the additional features in our model are generated probabilistically whereas in the parser of distance measures are assumed to be function of the already generated structure and are not generated explicitly order to estimate the conditional probabilities of our model we recursively smooth empirical of specific conditional distributions with estimates of less specific using interpolation is smoothing weight which depends on the particular When defining models we will indicate backoff level with sign between conditioning variables eg
177,A,P97-1003,P03-1002,1,"(Collins, 1997)","In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al., 1998). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). It consists of two tasks: (1) identifying the parse tree constituents corresponding to arguments of each predicate encoded in PropBank; and (2) recognizing the role corresponding to each argument.",previous work using the corpus proposed model predicting argument roles using the same statistical method as the one employed by for predicting semantic roles based on the corpus This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in consists of tasks identifying the parse tree constituents corresponding to arguments of each predicate encoded in and recognizing the role corresponding to each argument
178,A,P97-1003,P03-1013,3,"(e.g., Collins 1997; Charniak 2000)","This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra. Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., Collins 1997; Charniak 2000). However, most of the existing models have been developed for English and trained on the Penn Treebank (Marcus et al., 1993), which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup.",This indicates that sister head dependencies are more appropriate for treebanks with very flat structures such as Negra Treebank based probabilistic parsing has been the subject of intensive research over the past few years resulting in parsing models that achieve both broad coverage and high parsing accuracy However most of the existing models have been developed for and trained on the Penn Treebank which raises the question whether these models generalize to other languages and to annotation schemes that differ from the Penn Treebank markup
179,A,P97-1003,P03-1055,2,"(Collins, 1997)","Clearly, information about long-distance relationships is vital for semantic interpretation. However, such constructions prove to be difficult for stochastic parsers (Collins et al., 1999) and they either avoid tackling the problem (Charniak, 2000; Bod, 2003) or only deal with a subset of the problematic cases (Collins, 1997). Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a postprocessing step, after parsing.",Clearly information about long distance relationships is vital for semantic interpretation However such constructions prove to be difficult for stochastic parsers and they either avoid tackling the problem or only deal with subset of the problematic cases proposes an algorithm that is able to find long distance dependencies as postprocessing step after parsing
180,A,P97-1003,P04-1043,1,"(Collins, 1997)","Additionally, 30% of training was used as a validation-set. The sentences were processed using Collins’ parser (Collins, 1997) to generate parse-trees automatically. 4.2 Classification set-up The classifier evaluations were carried out using the SVM-light software (Joachims, 1999) available at svmlight.joachims.org with the default polynomial kernel for standard feature evaluations. To process PAF and SCF, we implemented our own kernels and we used them inside SVM-light. 7 for single arguments and the accuracy for the final multi-class classifier.",Additionally of training was used as validation set The sentences were processed using parser to generate parse trees automatically set up The classifier evaluations were carried out using the light software available at with the default polynomial kernel for standard feature evaluations process and we implemented our own kernels and we used them inside light for single arguments and the accuracy for the final multi class classifier
181,A,P97-1003,P04-1047,0,"(Collins, 1997)","As they do not include an evaluation, currently it is impossible to say how effective this technique is. (Xia et al., 2000) and (Chen and Vijay-Shanker, 2000) extract lexicalised TAGs from the Penn Treebank. Both techniques implement variations on the approaches of (Magerman, 1994) and (Collins, 1997) for the purpose of differentiating between complement and adjunct. In the case of (Xia et al., 2000), invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. (Hockenmaier et al., 2002) outline a method for the automatic extraction of a large syntactic CCG lexicon from Penn-II.",they do not include an evaluation currently it is impossible to say how effective this technique is and extract lexicalised from the Treebank Both techniques implement variations on the approaches of and for the purpose of differentiating between complement and adjunct the case of invalid elementary trees produced as result of annotation errors in the treebank are filtered out using linguistic heuristics outline method for the automatic extraction of large syntactic lexicon from
182,A,P97-1003,P04-1082,0,Both Collins (1997: 19),"The absence of rulebased approaches up until now is not motivated by the failure of such approaches in this domain; on the contrary, no one seems to have tried a rulebased approach to this problem. Instead it appears that there is an understandable predisposition against rule-based approaches, given the fact that data-driven, especially machine-learning, approaches have worked so much better in many other domains.1 Empty categories however seem different, in that, for the most part, their location and existence is determined, not by observable data, but by explicitly constructed linguistic principles, which 1Both Collins (1997: 19) and Higgins (2003: 100) are were consciously used in the annotation; i.e., unlike overt words and phrases, which correspond to actual strings in the data, empty categories are in the data only because linguists doing the annotation put them there. This paper therefore explores a rule-based approach to empty category recovery, with two purposes in mind:  first, to explore the limits of such an approach; and second, to establish a more realistic baseline for future (possibly data-driven or hybrid) approaches.",The absence of rulebased approaches up until now is not motivated by the failure of such approaches in this domain on the contrary no one seems to have tried rulebased approach to this problem Instead it appears that there is an understandable predisposition against rule based approaches given the fact that data driven especially machine learning approaches have worked so much better in many other Empty categories however seem different in that for the most part their location and existence is determined not by observable data but by explicitly constructed linguistic principles which and are were consciously used in the annotation unlike overt words and phrases which correspond to actual strings in the data empty categories are in the data only because linguists doing the annotation put them there This paper therefore explores rule based approach to empty category recovery with purposes in mind to explore the limits of such an approach and to establish more realistic baseline for future approaches
183,A,P97-1003,P05-1018,1,"(Collins, 1997)","Once we have identified entity classes, the next step is to fill out grid entries with relevant syntactic information. We employ a robust statistical parser (Collins, 1997) to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified. Passive verbs are recognized using a small set of patterns, and the underlying deep grammatical role for arguments involved in the passive construction is entered in the grid (see the grid cell o for Microsoft, Sentence 2, Table 2).",Once we have identified entity classes the next step is to fill out grid entries with relevant syntactic information employ robust statistical parser to determine the constituent structure for each sentence from which subjects objects and relations other than subject or object are identified Passive verbs are recognized using small set of patterns and the underlying deep grammatical role for arguments involved in the passive construction is entered in the grid
184,A,P97-1023,P02-1053,3,"(Hatzivassiloglou & McKeown, 1997)","The first step is to use a part-of-speech tagger to identify phrases in the input text that contain adjectives or adverbs (Brill, 1994). The second step is to estimate the semantic orientation of each extracted phrase (Hatzivassiloglou & McKeown, 1997). A phrase has a positive semantic orientation when it has good associations (e.g., “ romantic ambience”) and a negative semantic orientation when it has bad associations (e.g., “horrific events” ).",The step is to use part of speech tagger to identify phrases in the input text that contain adjectives or adverbs The step is to estimate the semantic orientation of each extracted phrase phrase has positive semantic orientation when it has good associations and negative semantic orientation when it has bad associations
185,A,W01-0521,P04-1014,0,"(Gildea, 2001)","The results show that each additional feature type increases performance. Hockenmaier also found the dependencies to be very beneficial — in contrast to recent results from the lexicalised PCFG parsing literature (Gildea, 2001) — but did not gain from the use of distance measures. One of the advantages of a log-linear model is that it is easy to include additional information, such as distance, as features.",The results show that each additional feature type increases performance also found the dependencies to be very beneficial in contrast to recent results from the lexicalised parsing literature but did not gain from the use of distance measures of the advantages of log linear model is that it is easy to include additional information such as distance as features
186,A,W01-0521,P06-1043,3,"(Ratnaparkhi, 1999; Hwa, 1999; Gildea, 2001; Bacchiani et al., 2006)","Lease and Charniak (2005) use the Charniak parser for biomedical data and find that the use of out-of-domain trees and in-domain vocabulary information can considerably improve performance. However, the work which is most directly comparable to ours is that of (Ratnaparkhi, 1999; Hwa, 1999; Gildea, 2001; Bacchiani et al., 2006). All of these papers look at what happens to modern WSJ-trained statistical parsers (Ratnaparkhi’s, Collins’, Gildea’s and Roark’s, respectively) as training data varies in size or usefulness (because we are testing on something other thanWSJ).",use the parser for biomedical data and find that the use of out of domain trees and in domain vocabulary information can considerably improve performance However the work which is most directly comparable to ours is that of All of these papers look at what happens to modern trained statistical parsers as training data varies in size or usefulness
187,A,W01-0521,W05-1516,0,"(Gildea, 2001)","However, it was discovered recently that bi-lexical statistics (parameters that involve two words) actually played much smaller role than previously believed. It was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output. Bikel (2004) observed that the bi-lexical statistics accounted for only 1.49% of the bigram statistics used by the parser.",However it was discovered recently that bi lexical statistics actually played much smaller role than previously believed was found in that the removal of bi lexical statistics from state of the art parser resulted very small change in the output observed that the bi lexical statistics accounted for only of the bigram statistics used by the parser
188,A,W01-0521,D09-1085,0,"(Gildea, 2001)","First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower (Collins, 1999). Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). Finally, some researchers have argued that the Parseval metrics (Black et al., 1991) are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies, for which scores are typically lower, is a better test of parser performance (Lin, 1995; Carroll et al., 1998).",the single score is an aggregate over highly skewed distribution of all constituent types evaluations which look at individual constituent or dependency types show that the accuracies on some semantically important constructions such as coordination and attachment are much lower it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains Finally some researchers have argued that the Parseval metrics are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies for which scores are typically lower is better test of parser performance
189,A,W02-1001,W03-0422,2,Collins (2002),"The learning approach presented here is closely related to –and inspired by– some recent works in the area of NLP and Machine Learning. Collins (2002) adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback. Crammer and Singer (2003) presented an online topic-ranking algorithm involving several perceptrons and ranking-based update rules for training them.",The learning approach presented here is closely related to inspired some recent works in the area of and Machine Learning adapted the learning algorithm to tagging tasks via sentence based global feedback presented an online topic ranking algorithm involving several perceptrons and ranking based update rules for training them
190,A,W02-1001,W04-0824,2,"(Collins, 2002; Crammer and Singer, 2003)","Then, we integrate the task-specific and the external training data with a multicomponent classifier that simplifies the system for hierarchical word sense disambiguation presented in (Ciaramita et al., 2003). The classifier consists of two components based on the averaged multiclass perceptron (Collins, 2002; Crammer and Singer, 2003). The first component is trained on the task-specific data while the second is trained on the former and on the external training data.",Then we integrate the task specific and the external training data with multicomponent classifier that simplifies the system for hierarchical word sense disambiguation presented in The classifier consists of components based on the averaged multiclass perceptron The component is trained on the task specific data while the is trained on the former and on the external training data
191,A,W02-1001,P12-1111,3,"(Collins, 2002)","The example in Table 3 shows raw and constrained input with respect to a typical input sentence. in a high-dimensional feature space. We adopt the basic feature set used in (Ratnaparkhi, 1996) and (Collins, 2002). Moreover, when deterministic constraints have applied to contextual words of w0, it is also possible to include some lookahead feature templates, such as: t0&t1, t0&t1&t2, and t−1&t0&t1 to the current word w0.",The example in Table shows raw and constrained input with respect to typical input sentence in high dimensional feature space adopt the basic feature set used in and Moreover when deterministic constraints have applied to contextual words of it is also possible to include some lookahead feature templates such as and to the current word
192,A,W02-1001,P13-4016,1,"(Collins, 2002)","This tuning can be performed for evaluation measures including BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a), with an easily extendable interface that makes it simple to support other measures. There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009). There are plans to implement more algorithms such as MIRA or AROW (Chiang, 2012) in the near future.",This tuning can be performed for evaluation measures including and with an easily extendable interface that makes it simple to support other measures There is also preliminary implementation of online learning methods such as the structured algorithm and regularized structured trained using There are plans to implement more algorithms such as or in the near future
193,A,W02-1001,N03-1033,2,Collins (2002),"Edmonton, May-June 2003 Main Papers , pp. 173-180 Proceedings of HLT-NAACL 2003 of features, but we show that by suitable use of a prior (i.e., regularization) in the conditional loglinear model – something not used by previous maximum entropy taggers – many such features can be added with an overall positive effect on the model. Indeed, as for the voted perceptron of Collins (2002), we can get performance gains by reducing the support threshold for features to be included in the model. Combining all these ideas, together with a few additional handcrafted unknown word features, gives us a part-of-speech tagger with a per-position tag accuracy of 97.24%, and a whole-sentence correct rate of 56.34% on Penn Treebank WSJ data.",May June Main Papers pp Proceedings of of features but we show that by suitable use of prior in the conditional loglinear model something not used by previous maximum entropy taggers many such features can be added with an overall positive effect on the model Indeed as for the voted perceptron of we can get performance gains by reducing the support threshold for features to be included in the model Combining all these ideas together with few additional handcrafted unknown word features gives us part of speech tagger with per position tag accuracy of and whole sentence correct rate of on Penn Treebank data
194,A,W02-1001,D13-1033,0,"(Collins, 2002; Hajič, 2004; Smith et al., 2005; Chrupała et al., 2008, and others)","In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajič, 2004; Smith et al., 2005; Chrupała et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships.",many languages words that participate in syntactic relation show covariance in some or all of their morphological features annotation of morphology assigns morphological descriptions to word forms is usually modeled as sequence model often in combination with part of speech tagging and lemmatization Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships
195,A,W03-0430,P04-1007,0,"(Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003)","Sayᾱti is the parameter vector after thei’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the averaged parametersᾱAVG are defined asᾱAVG = originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parametersᾱ to define a conditional distribution over the members of GEN(x) for a given inputx: normalization constant that depends onx andᾱ.",is the parameter vector after thei th example is processed on the th pass through the data in the algorithm in figure Then the averaged are defined originally proposed the averaged parameter method it was shown to give substantial improvements in accuracy for tagging tasks in Conditional Random Fields Conditional Random Fields have been applied to tasks such as parsing and tagging or segmentation tasks use the to define conditional distribution over the members of for given inputx normalization constant that depends onx
196,A,W03-0430,P05-1002,0,"(McCallum and Li, 2003)","Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks. CRFs are usually estimated using gradient-based methods such as limited memory variable metric (LMVM).",Efficient inference and training methods exist when the graphical structure of the model forms chain where each position in sequence is connected to its adjacent positions have been applied with impressive empirical results to the tasks of named entity recognition simplified part of speech tagging noun phrase chunking and extraction of tabular data among other tasks are usually estimated using gradient based methods such as limited memory variable metric
197,A,W03-0430,P05-1003,0,"(McCallum and Li, 2003)","LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search. In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation.",therefore provide viable alternative to regularisation without the need for hyperparameter search recent years conditional random fields have shown success on number of natural language processing tasks including shallow parsing named entity recognition and information extraction from research papers general this work has demonstrated the susceptibility of to overfit the training data during parameter estimation
198,A,W03-0430,P05-1044,0,"(McCallum and Li, 2003)","Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model.",Conditional random fields are quite effective at sequence labeling tasks like shallow parsing and namedentity extraction are log linear allowing the incorporation of arbitrary features into the model
199,A,W03-0430,P06-1028,0,"(McCallum and Li, 2003)","This allows CRFs to use flexible features such as complicated functions of multiple observations. The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003). Since the introduction of CRFs, intensive research has been undertaken to boost their effectiveness.",This allows to use flexible features such as complicated functions of multiple observations The modeling power of has been of great benefit in several applications such as shallow parsing and information extraction Since the introduction of intensive research has been undertaken to boost their effectiveness
200,A,W03-0430,P06-1078,0,"(McCallum and Li, 2003)","Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B́echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results.",Most previous studies of the of speech data used generative models such as hidden Markov models the other hand in text based better results are obtained using discriminative schemes such as maximum entropy models support vector machines and conditional random fields applied text level based to results
201,A,W03-0430,P08-1056,0,"(Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003)","NER systems have been developed for English and few other languages with high accuracy. These belong to two main categories based on machine learning (Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003) and language or domain specific rules (Grishman, 1995; Wakao et al., 1996). In English, the names are usually capitalized which is an important clue for identifying a name.",systems have been developed for and few other languages with high accuracy These belong to main categories based on machine learning and language or domain specific rules the names are usually capitalized which is an important clue for identifying name
202,A,W03-0430,P08-1081,0,"(McCallum and Li, 2003; Sha and Pereira, 2003)","A CRF is an undirected graphical model G of the conditional distribution P (Y|X). Y are the random variables over the labels of the nodes that are globally conditioned on X, which are the random variables of the observations. (See Section 3.4 for more about CRFs) Linear CRF model has been successfully applied in NLP and text mining tasks (McCallum and Li, 2003; Sha and Pereira, 2003). However, our problem cannot be modeled with Linear CRFs in the same way as other NLP tasks, where one node has a unique label.",is an undirected graphical model of the conditional distribution are the random variables over the labels of the nodes that are globally conditioned on which are the random variables of the observations model has been successfully applied in and text mining tasks However our problem can not be modeled with in the same way as other tasks where node has unique label
203,A,W03-0430,P10-1136,0,"(McCallum and Li, 2003)","In this work, we propose to use POS tagging and parsing outputs as features, in addition to other features, in extracting the semantic structure of web queries. Finally, there exist large bodies of work on information extraction using models based on Markov and semi-Markov CRFs (Lafferty et al., 2001; Sarawagi and Cohen, 2004), and in particular for the task of named entity recognition (McCallum and Li, 2003). The problem studied in this work is concerned with identifying more generic “semantic roles” of the constituents in noun phrase queries.",this work we propose to use tagging and parsing outputs as features in addition to other features in extracting the semantic structure of web queries Finally there exist large bodies of work on information extraction using models based on and semi and in particular for the task of named entity recognition The problem studied in this work is concerned with identifying more generic semantic roles of the constituents in noun phrase queries
204,A,W03-0430,P12-1055,0,"(Mccallum and Li, 2003; Etzioni et al., 2005)","Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings.",Owing to the availability of annotated corpora such as and data driven methods are now dominant Current studies of mainly focus on formal text such as news articles representative work is that of in which they systematically study the challenges of compare several solutions and show some interesting findings
205,A,W03-0430,W04-0705,0,(McCallum and Li 2003),"The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC6 Evaluation (Grishman and Sundheim, 1996). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.",The problem of name recognition and classification has been intensively studied since when it was introduced as part of the Evaluation wide variety of machine learning methods have been applied to this problem including Hidden Markov Models Maximum Entropy methods Decision Trees Conditional Random Fields Class based Language Model Agent based Approach and Support Vector Machines However the performance of even the best of these has been limited by the amount of labeled training data available to them and the range of features which they employ
206,A,W03-0430,W04-1221,0,"(McCallum and Li, 2003)","B-DNA labels the first word of a DNA mention, I-DNA labels all subsequent words (likewise for other entities), and O labels non-entities. For simplicity, this paper only refers to the entities, not all the IOB label variants. particular have been shown to be useful in partof-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), and named entity recognition for newswire data (McCallum and Li, 2003). They have also just recently been applied to the more limited task of finding gene and protein mentions (McDonald and Pereira, 2004), with promising early results.",labels the word of mention labels all subsequent words and labels non entities For simplicity this paper only refers to the entities not all the label variants particular have been shown to be useful in partof speech tagging shallow parsing and named entity recognition for newswire data They have also just recently been applied to the more limited task of finding gene and protein mentions with promising early results
207,A,W03-0430,W04-3230,0,"(McCallum and Li, 2003)","They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech).",They are considered to be the state of the art framework to date Empirical successes with have been reported recently in part of speech tagging shallow parsing named entity recognition word segmentation and Information Extraction Previous applications with assumed that observation sequence boundaries are fixed and the main focus was to predict label present Communication Science Laboratories Hikaridai Seika cho Soraku Kyoto taku sequence
208,A,W03-0430,W04-3234,0,"(McCallum and Li, 2003)","There are several ways in which this work might be extended and improved, both in its particular form and in general: � BWI models initial and terminal boundaries, but ignores characteristics of the extracted phrase other than its length. We are exploring mechanisms for modeling relevant phrasal structure. quential averaged perceptrons or CRFs (McCallum and Li, 2003), appear better suited to the NER problem than local symbolic learners, the two approaches search different hypothesis spaces. Based on the surmise that, by combining them, we can realize improvements over either in isolation, we are exploring mechanisms for integration. � The distributional clusters we find are independent of the problem to which we want to apply them and may sometimes be inappropriate or have the wrong granularity.",There are several ways in which this work might be extended and improved both in its particular form and in general models initial and terminal boundaries but ignores characteristics of the extracted phrase other than its length are exploring mechanisms for modeling relevant phrasal structure quential averaged perceptrons or appear better suited to the problem than local symbolic learners the approaches search different hypothesis spaces Based on the surmise that by combining them we can realize improvements over either in isolation we are exploring mechanisms for integration The distributional clusters we find are independent of the problem to which we want to apply them and may sometimes be inappropriate or have the wrong granularity
209,A,W04-2405,E12-1017,0,"(Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006)","Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data.",Yet the available unlabeled data is vast so we turn to semisupervised learning Here we adapt self training simple technique that leverages supervised learner to perform semisupervised learning our version model is trained on the labeled data then used to label the unlabeled target data
210,A,W04-3103,P07-1125,3,Light et al. (2004),"We analyze our learning model experimentally and report promising results for the task on a new publicly-available dataset.1 While there is a certain amount of literature within the linguistics community on the use of hedging in scientific text, eg. (Hyland, 1994), there is little of direct relevance to the task of classifying speculative language from an NLP/ML perspective. The most clearly relevant study is Light et al. (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. We will draw on this work throughout our presentation of the task.",analyze our learning model experimentally and report promising results for the task on new publicly available While there is certain amount of literature within the linguistics community on the use of hedging in scientific text eg there is little of direct relevance to the task of classifying speculative language from an perspective The most clearly relevant study is where the focus is on introducing the problem exploring annotation issues and outlining potential applications rather than on the specificities of the approach though they do present some results using manually crafted substring matching classifier and supervised on collection of abstracts will draw on this work throughout our presentation of the task
211,A,W04-3111,D07-1112,1,"(Kulick et al., 2004)","We were given around 15K sentences of labeled text from the Wall Street Journal (WSJ) (Marcus et al., 1993; Johansson and Nugues, 2007) as well as 200K unlabeled sentences. The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004). The two test domains were a collection of medline chemistry abstracts (pchem, the CYP portion of the Penn Biomedical Treebank) and the Child Language Data Exchange System corpus (CHILDES) (MacWhinney, 2000; Brown, 1973).",were given around sentences of labeled text from the Wall Street Journal as well as unlabeled sentences The development data was sentences of labeled biomedical oncology text as well as unlabeled sentences The test domains were collection of medline chemistry abstracts and the Child Language Data Exchange System corpus
212,A,W04-3111,D07-1128,0,"(Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973)","It has been applied to many tasks, such as parsing biomedical literature (Rinaldi et al., 2006; Rinaldi et al., 2007) and the whole British National Corpus, and has been evaluated in several ways. We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973). The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task.",has been applied to many tasks such as parsing biomedical literature and the whole British National Corpus and has been evaluated in several ways have achieved average results in the domain adaptation track open submission The performance of the parser is seriously affected by mapping problems to the particular dependency representation used in the shared task
213,A,W04-3111,D07-1119,1,"(Kulick et al, 2004)","Because of certain inconsistencies in the annotation guidelines, the organizers decided to make this task optional and hence we submitted just the parse produced by the parser trained for English. For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain (Kulick et al, 2004) as well as a test set of 5000 tokens (200 sentences) to parse (english_pchemtbtb_test.conll). There were three sets of unlabeled documents: we chose the smallest (unlab1) consisting of over 300,000 tokens (11663 sentences). unlab1 was tokenized, POS and lemmas were added using our version of TreeTagger (Schmid, 1994), and lemmas replaced with stems, which had turned out to be more effective than lemmas.",Because of certain inconsistencies in the annotation guidelines the organizers decided to make this task optional and hence we submitted just the parse produced by the parser trained for For the adaptation task we were given large collection of unlabeled data in the chemistry domain as well as test set of tokens to parse There were sets of unlabeled documents we chose the smallest consisting of over tokens was tokenized and lemmas were added using our version of and lemmas replaced with stems which had turned out to be more effective than lemmas
214,A,W04-3111,W08-0605,0,"(Kulick et al., 2004)","However, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004).",However the lack of annotated corpora which are indispensable for training machine learning models makes it difficult to broaden the scope of text mining applications the biomedical domain for example several annotated corpora such as PennBioIE and have been created and made publicly available but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address Active learning is framework which can be used for reducing the amount of human effort required to create training corpus
215,A,W04-3111,D07-1126,0,"(Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004)","This task is to adapt our model trained on PennBank data to the test data in the Biomedical domain. The pchemtb-closed shared task (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004) is used to illustrate our models. We do not use any additional unlabeled data in the Biomedical domain.",This task is to adapt our model trained on data to the test data in the domain The pchemtb closed shared task is used to illustrate our models do not use any additional unlabeled data in the domain
216,A,W04-3111,P10-1089,1,"(Kulick et al., 2004)","For out-of-domain data, we get 21K 6HMM-style taggers, like the fast TnT tagger used on our web corpus, do not use bilexical features, and so perform especially poorly on these cases. One motivation for our work was to develop a fast post-processor to fixVBN/VBD errors. examples from the Brown portion of the Treebank and 6296 examples from tagged Medline abstracts in the PennBioIE corpus (Kulick et al., 2004). There are two orthogonal sources of information for predicting VBN/VBD: 1) the noun-verb pair, and 2) the context around the pair.",For out of domain data we get style taggers like the fast tagger used on our web corpus do not use bilexical features and so perform especially poorly on these cases motivation for our work was to develop fast post processor to errors examples from the portion of the and examples from tagged abstracts in the PennBioIE corpus There are orthogonal sources of information for predicting the noun verb pair and the context around the pair
217,A,W04-3111,P07-1031,1,"(Kulick et al., 2004)","Smith, that we would like to distinguish from implicitly right-branching NPs in the next version of the corpus. Although our scheme is still developing, we believe that the current annotation is already useful for statistical modelling, and we demonstrate this empirically in Section 6. 4.1 Annotation Process Our annotation guidelines1 are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al., 2004). The annotation guidelines for this biomedical corpus (an addendum to the Penn Treebank guidelines) introduce the use of NML nodes to mark internal NP structure.",that we would like to distinguish from implicitly right branching in the next version of the corpus Although our scheme is still developing we believe that the current annotation is already useful for statistical modelling and we demonstrate this empirically in Section Annotation Process Our annotation are based on those developed for annotating full sub structure in the biomedical domain The annotation guidelines for this biomedical corpus introduce the use of nodes to mark internal structure
218,A,W04-3111,P09-1117,1,"(Kulick et al., 2004)","From the general-language newspaper domain, we took the training part of the MUC7 corpus (Linguistic Data Consortium, 2001) which incorporates seven different entity types,viz. per3The OUTSIDE class is assigned to each token that does not denote an entity in the underlying domain of discourse. sons, organizations, locations, times, dates, monetary expressions, and percentages. From the sublanguage biology domain, we used the oncology part of the PENNBIOIE corpus (Kulick et al., 2004) and removed all but three gene entity subtypes (generic, protein, and rna). Table 1 summarizes the quantitative characteristics of both corpora.4 The results reported below are averages of 20 independent runs.",From the general language newspaper domain we took the training part of the corpus which incorporates different entity types viz class is assigned to each token that does not denote an entity in the underlying domain of discourse sons organizations locations times dates monetary expressions and percentages From the sublanguage biology domain we used the oncology part of the corpus and removed all but gene entity subtypes Table summarizes the quantitative characteristics of both The results reported below are averages of independent runs
219,A,W04-3111,I05-2038,1,"(Kulick et al., 2004)","This problem of ellipsis can frequently occur in research abstracts, and it can be argued that the tokenization criteria must be changed for texts in biomedical domain (Yamamoto and Satou, 2004) so that such fragment as ‘IL-18’ and ‘mediated’ in ‘IL-18-ediated’ should be regarede as separate tokens. The Pennsylvania biology corpus (Kulick et al., 2004) partially solves this problem by separating a token where two or more subtokens are connected with hyphens, but in the cases where a shared part of the word is not separated by a hyphen (e.g. ‘metric’ of ‘stereo- and isometric alleles’) the word including the part is left uncut. The current GTB follows the GENIA corpus that it retains the tokenization criteria of the original Penn Treebank, but this must be reconsidered in future.",This problem of ellipsis can frequently occur in research abstracts and it can be argued that the tokenization criteria must be changed for texts in biomedical domain so that such fragment as and mediated in ediated should be regarede as separate tokens The biology corpus partially solves this problem by separating token where or more subtokens are connected with hyphens but in the cases where shared part of the word is not separated by hyphen the word including the part is left uncut The current follows the corpus that it retains the tokenization criteria of the original Penn Treebank but this must be reconsidered in future
220,A,W04-3252,C08-1040,2,"(Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Kurland and Lee, 2005; Kurland and Lee, 2006)","Eigenvector centrality is another powerful method that that has been applied to several types of networks. For example it has been used to measure centrality in hyperlinked web pages networks (Brin and Page, 1998; Kleinberg, 1998), lexical networks (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Kurland and Lee, 2005; Kurland and Lee, 2006), and semantic networks (Mihalcea et al., 2004). The interest of applying natural language processing techniques in the area of political science has been recently increasing. (Quinn et al., 2006) introduce a multinomial mixture model to cluster political speeches into topics or related categories.",Eigenvector centrality is another powerful method that that has been applied to several types of networks For example it has been used to measure centrality in hyperlinked web pages networks lexical networks and semantic networks The interest of applying natural language processing techniques in the area of political science has been recently increasing introduce multinomial mixture model to cluster political speeches into topics or related categories
221,A,W06-1615,P07-1056,3,"(Blitzer et al., 2006)","We propose solutions to these two questions and evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs, electronics, and kitchen appliances2. First, we show how to extend the recently proposed structural cor1For surveys of recent research on domain adaptation, see the ICML 2006 Workshop on Structural Knowledge Transfer for Machine Learning ( edu/) and the NIPS 2006 Workshop on Learning when test and training inputs have different distribution ( first.fraunhofer.de/projects/different06/) 2The dataset will be made available by the authors at publication time. respondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification. A key step in SCL is the selection of pivot features that are used to link the source and target domains.",propose solutions to these questions and evaluate them on corpus of reviews for different types of products from books electronics and kitchen we show how to extend the recently proposed structural surveys of recent research on domain adaptation see the Workshop on Structural Knowledge Transfer for Machine Learning and the Workshop on Learning when test and training inputs have different distribution dataset will be made available by the authors at publication time respondence learning domain adaptation algorithm for use in sentiment classification key step in is the selection of pivot features that are used to link the source and target domains
222,A,W07-1412,E12-1004,0,"(Zanzotto et al., 2007; Kouleykov and Magnini, 2005)","Entailment systems have been compared under this new perspective in various evaluation campaigns, the best known being the Recognizing Textual Entailment (RTE) initiative (Dagan et al., 2009). Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (Zanzotto et al., 2007; Kouleykov and Magnini, 2005). A few systems exploit deep FS analysis (Bos and Markert, 2006; Chambers et al., 2007).",Entailment systems have been compared under this new perspective in various evaluation campaigns the best known being the Recognizing Textual Entailment initiative Most systems are based on advanced components machine learning techniques syntactic transformations few systems exploit deep analysis
223,A,W07-1412,P08-1118,0,"(Hickl et al., 2006; MacCartney et al., 2006; Zanzotto et al., 2007)","Here, it is relatively easy to identify the lack of entailment: the first sentence involves no injuries, so the second is unlikely to be entailed. Most entailment systems function as weak proof theory (Hickl et al., 2006; MacCartney et al., 2006; Zanzotto et al., 2007), but contradictions require deeper inferences and model building. While mismatching information between sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences.",Here it is relatively easy to identify the lack of entailment the sentence involves no injuries so the is unlikely to be entailed Most entailment systems function as weak proof theory but contradictions require deeper inferences and model building While mismatching information between sentences is often good cue of non entailment it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences
224,A,W07-1412,P12-1047,2,"(Moschitti and Zanzotto, 2007)","Harmeling (2007) and Heilman and Smith (2010) classified sequence pairs based on transformation on syntactic trees. Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007). We formalize sentence re-writing learning as a kernel method.",and classified sequence pairs based on transformation on syntactic trees used kernel method on syntactic tree pairs formalize sentence re writing learning as kernel method
225,A,W07-1412,W09-2501,0,"(Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007)","In the absence of a universally accepted definition of MWEs, we define MWEs in the RTE setting as multi-word alignments, i.e., words that participate in more than one word alignment link between premise and hypothesis: The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps.",the absence of universally accepted definition of we define in the setting as multi word alignments words that participate in more than one word alignment link between premise and hypothesis The exclusion of that do not lead to multiword alignments is not significant loss since these cases are unlikely to cause significant problems for addition an alignment based approach has the advantage of generality Almost all existing models align the linguistic material of the premise and hypothesis and base at least part of their decision on properties of this alignment proceed in steps
226,A,W07-1412,W10-1609,0,"(Marneffe et al., 2006; Zanzotto et al., 2007; Castillo, 2009)","In this paper, we address the RTE task problem of determining the entailment value between Text and Hypothesis pairs in Spanish, applying machine learning techniques. In the past, RTEs Challenges machine learning algorithms were widely used for the task of recognizing textual entailment (Marneffe et al., 2006; Zanzotto et al., 2007; Castillo, 2009) and they have reported goods results for English language. Also, our system applies machine learning algorithms to the Spanish.",this paper we address the task problem of determining the entailment value between and pairs in applying machine learning techniques the past machine learning algorithms were widely used for the task of recognizing textual entailment and they have reported goods results for language Also our system applies machine learning algorithms to the
227,A,W07-1412,W11-2404,0,"(Zanzotto et al., 2007; Hickl and Bensley, 2007)","Third, looking at RTE-related literature and the outcomes of the six campaigns organised so far, the conclusions that can be drawn are often controversial. For instance, it is not clear whether the availability of larger amounts of training data correlates with better performance (Hickl et al., 2006) or not (Zanzotto et al., 2007; Hickl and Bensley, 2007), even within the same evaluation setting. In addition, ablation tests carried out in recent editions of the challenge do not allow for definite conclusions about the actual usefulness of tools and resources, even the most popular ones (Bentivogli et al., 2009).",looking at related literature and the outcomes of the campaigns organised so far the conclusions that can be drawn are often controversial For instance it is not clear whether the availability of larger amounts of training data correlates with better performance or not even within the same evaluation setting addition ablation tests carried out in recent editions of the challenge do not allow for definite conclusions about the actual usefulness of tools and resources even the most popular ones
228,A,W96-0213,A00-1031,1,"(Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996)","The debate about which paradigm solves the part-of-speech tagging problem best is not finished. Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996). They are only surpassed by combinations of different systems, forming a ""voting tagger"".",The debate about which paradigm solves the part of speech tagging problem best is not finished Recent comparisons of approaches that can be trained on corpora have shown that in most cases statistical aproaches yield better results than finite state rule based or memory based taggers They are only surpassed by combinations of different systems forming voting tagger
229,A,W96-0213,A00-2013,1,"(Ratnaparkhi, 1996)","In conclusion, we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances. English Part of Speech (POS) tagging has been widely described in the recent past, starting with the (Church, 1988) paper, followed by numerous others using various methods: neural networks (Julian Benello and Anderson, 1989), HMM tagging (Merialdo, 1992), decision trees (Schmid, 1994), transformation-based error-driven learning (Brill, 1995), and maximum entropy (Ratnaparkhi, 1996), to select just a few. However different the methods were, English dominated in these tests.",conclusion we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances Part of tagging has been widely described in the recent past starting with the paper followed by numerous others using various methods neural networks tagging decision trees transformation based error driven learning and maximum entropy to select just few However different the methods were dominated in these tests
230,A,W96-0213,A00-2020,0,Adwait Ratnaparkhi (1996),"Abney et al. (1999) applied boosting to part of speech tagging. Adwait Ratnaparkhi (1996) estimates a probability distribution for tagging using a maximum entropy approach. Regarding error detection in corpora, Ratnaparkhi (1996) discusses inconsistencies in the Penn Treebank and relates them to interannotator differences in tagging style.",applied boosting to part of speech tagging estimates probability distribution for tagging using maximum entropy approach Regarding error detection in corpora discusses inconsistencies in the Penn Treebank and relates them to interannotator differences in tagging style
231,A,W96-0213,A97-1004,2,"(Ratnaparkhi ,  1996)","As a. result, no hand-craf ted rules or lists are required by the highly portable system and it can be easily retrained for other languages or text genres. The  model  used here for sentence-boundary  detection is based on the m a x i m u m  entropy model used for POS tagging in (Ratnaparkhi ,  1996). For each potential  sentence boundary  token (., ?, and !), we est imate a joint  probabil i ty dis t r ibut ion p of the token and it.s surrounding context,  both  of which are denoted by c, occurr ing as an actual sentence I)oundary.",result no hand craf ted rules or lists are required by the highly portable system and it can be easily retrained for other languages or text genres The model used here for sentence boundary detection is based on the entropy model used for tagging in For each potential sentence boundary token we est imate joint probabil ty dis ibut ion of the token and surrounding context both of which are denoted by occurr ing as an actual sentence oundary
232,A,W96-0213,E03-1002,1,"(Ratnaparkhi, 1996)","We then tested the best non-lexicalized and the best lexicalized models on the testing set. 6 Standard measures of performance are shown in table 1. 7 The top panel of table 1 lists the results for the non-lexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser (Costa et al., 2001), an earlier statis5 In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus. We used a publicly available tagger (Ratnaparkhi, 1996) to tag the words and then used these in the input to the system. 6We found that 80 hidden units produced better performance than 60 or 100. Momentum was applied throughout training.",then tested the best non lexicalized and the best lexicalized models on the testing set measures of performance are shown in table The top panel of table lists the results for the non lexicalized model and the available results for other models which only use part of speech tags as inputs another neural network parser an earlier these experiments the tags are included in the input to the system but for compatibility with other parsers we did not use the hand corrected tags which come with the corpus used publicly available tagger to tag the words and then used these in the input to the system found that hidden units produced better performance than or was applied throughout training
233,A,W96-0213,J01-3003,1,(Ratnaparkhi 1996),"Our results were replicated in English without the need for a fully parsed corpus (Anoop Sarkar, p.c., citing a project report by Wootiporn Tripasai). Our method was applied to 23 million words of the WSJ that were automatically tagged with Ratnaparkhi's maximum entropy tagger (Ratnaparkhi 1996) and chunked with the partial parser CASS (Abney 1996). The results are very similar to ours (best accuracy 66.6%), suggesting that a more accurate tagger than the one used on our corpus might in fact be sufficient to overcome the fact that no full parse is available. subject and the object multisets; e.g., the overlap between (a, a, a, b} and {a} is {a,a,a}.",Our results were replicated in without the need for fully parsed corpus Our method was applied to million words of the that were automatically tagged with maximum entropy tagger and chunked with the partial parser The results are very similar to ours suggesting that more accurate tagger than the one used on our corpus might in fact be sufficient to overcome the fact that no full parse is available subject and the object multisets the overlap between and is
234,A,W96-0213,N04-1013,0,"(Ratnaparkhi, 1996)","As part of our assessment, we also measured the parsing speed of the two systems, taking into account all stages of processing that each system requires to produce its output. For example, since the Collins parser depends on a prior part-of-speech tagger (Ratnaparkhi, 1996), we included the time for POS tagging in our Collins measurements. XLE incorporates a sophisticated finite-state morphology and dictionary lookup component, and its time is part of the measure of XLE performance.",part of our assessment we also measured the parsing speed of the systems taking into account all stages of processing that each system requires to produce its output For example since the parser depends on prior part of speech tagger we included the time for tagging in our measurements incorporates sophisticated finite state morphology and dictionary lookup component and its time is part of the measure of performance
235,A,W96-0213,W02-0301,0,"(Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996)","Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with Association for Computational Linguistics. the Biomedical Domain, Philadelphia, July 2002, pp. 1-8.",Thus we need to employ powerful machine learning techniques which can incorporate various and complex features in consistent way Support Vector Machines and Maximum Entropy method are powerful learning methods that satisfy such requirements and are applied successfully to other tasks this paper we apply Support Vector Machines to biomedical named entity recognition and train them with Association for Computational Linguistics the Biomedical Domain Philadelphia pp
236,A,W96-0213,W03-0428,0,"(Ratnaparkhi, 1996)","In section 2, we discuss a character-level HMM, while in section 3 we discuss a sequence-free maximum-entropy (maxent) classifier which uses � -gram substring features. Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model (CMM), as used for tagging (Ratnaparkhi, 1996) or earlier NER work (Borthwick, 1999). Figure 1 shows a graphical model representation of our character-level HMM.",section we discuss character level while in section we discuss sequence free maximum entropy classifier which uses gram substring features Finally in section we add additional features to the maxent model and chain these models into conditional markov model as used for tagging or earlier work Figure shows graphical model representation of our character level
237,A,W96-0213,P99-1023,1,"(Ratnaparkhi, 1996)","Taggers are often preprocessors in NLP systems, making accurate performance especially important . Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) (Kupiec, 1992), (Charniak et al., 1993); rule-based systems (Brill, 1994), (Brill, 1995); memory-based systems (Daelemans et al., 1996); maximum-entropy systems (Ratnaparkhi, 1996); path voting constraint systems (Tiir and Oflazer, 1998); linear separator systems (Roth and Zelenko, 1998); and majority voting systems (van Halteren et al., 1998). This paper describes various modifications to an HMM tagger that  improve the performance to an accuracy comparable to or better than the best current single classifier taggers.",Taggers are often preprocessors in systems making accurate performance especially important Much research has been done to improve tagging accuracy using several different models and methods including hidden Markov models rule based systems memory based systems maximum entropy systems path voting constraint systems linear separator systems and majority voting systems This paper describes various modifications to an tagger that improve the performance to an accuracy comparable to or better than the best current single classifier taggers
238,A,W97-0703,W00-1438,3,Barzilay and Elhadad (1997),"They used WordNet, a lexical database which contains some semantic information ( Also using WordNet in their implenmntation. Barzilay and Elhadad (1997) dealt with some of tile limitations in Hirst and St-Onge's algorithm by examining every possible lexical chain which could be computed, not just those possible at a given point in the text. That is to say, while Hirst and St.",They used lexical database which contains some semantic information Also using in their implenmntation dealt with some of tile limitations in and Onge algorithm by examining every possible lexical chain which could be computed not just those possible at given point in the text That is to say while and
239,A,W97-0710,P04-3020,0,"(Teufel and Moens, 1997)","As a consequence, there is a large body of work on algorithms 5Notice that rows two and four in Table 1 are in fact redundant, since the “hub” (“weakness”) variations of the HITS (Positional) algorithms can be derived from their “authority” (“power”) counterparts by reversing the edge orientation in the graphs. 6Only seven edges are incident with vertex 15, less than e.g. eleven edges incident with vertex 14 – not selected as “important” by TextRank. for sentence extraction undertaken as part of the DUC evaluation exercises. Previous approaches include supervised learning (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities (Salton et al., 1997). It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. 6 Conclusions Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",consequence there is large body of work on algorithms that rows and in Table are in fact redundant since the hub variations of the algorithms can be derived from their authority counterparts by reversing the edge orientation in the graphs edges are incident with less than eleven edges incident with not selected as important by for sentence extraction undertaken as part of the evaluation exercises Previous approaches include supervised learning vectorial similarity computed between an initial abstract and sentences in the given document or intra document similarities is also notable the study reported in discussing the usefulness and limitations of automatic sentence extraction for summarization which emphasizes the need of accurate tools for sentence extraction as an integral part of automatic summarization systems Conclusions Intuitively works well because it does not only rely on the local context of text unit but rather it takes into account information recursively drawn from the entire text
240,A,W99-0613,P02-1046,1,"(Collins and Singer, 1999)","Hence we can compute the precision of every rule given the precision of any one. The empirical investigations described here and below use the data set of (Collins and Singer, 1999). The task is to classify names in text as person, location, or organization.",Hence we can compute the precision of every rule given the precision of any one The empirical investigations described here and below use the data set of The task is to classify names in text as person location or organization
241,A,W99-0613,W11-0319,0,"(Collins and Singer, 1999)","The study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which is a common cause of divergence in boostrapping approaches. Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for namedentity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. Some of our methods rely on the same intuition described above, i.e., using instances of other classes as negative training examples.",The study shows that using competing categories improves the accuracy of the system by avoiding sematic drift which is common cause of divergence in boostrapping approaches Similar approaches are used among others in for learning semantic lexicons in for namedentity recognition and in for hierarchical text categorization Some of our methods rely on the same intuition described above using instances of other classes as negative training examples
242,A,W99-0613,W09-1116,0,Collins and Singer (1999),Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1.,shows how to alternate between extracting instances of class and inducing new instance extracting patterns and apply bootstrapping to the related task of named entity recognition Our approach was directly influenced by the hypernym extractor of and we provided an analogous summary in Section
243,A,W99-0613,P07-1125,0,"(Collins and Singer, 1999)","This idea was formalised by Blum and Mitchell (1998) in their presentation of co-training. Co-training has also been used for named entity recognition (NER) (Collins and Singer, 1999), coreference resolution (Ng and Cardie, 2003), text categorization (Nigam and Ghani, 2000) and improving gene name data (Wellner, 2005). Conversely, single-view learning models operate without an explicit partition of the feature space.",This idea was formalised by in their presentation of co training training has also been used for named entity recognition coreference resolution text categorization and improving gene name data Conversely single view learning models operate without an explicit partition of the feature space
244,A,W06-2925,D07-1101,2,"(Carreras et al., 2006)","Otherwise we used the multi-root variant. The first-order features φ1(x, h, m) are the exact same implementation as in previous CoNLL system (Carreras et al., 2006). In turn, those features were inspired by successful previous work in firstorder dependency parsing (McDonald et al., 2005).",Otherwise we used the multi root variant The order features are the exact same implementation as in previous system turn those features were inspired by successful previous work in firstorder dependency parsing
245,A,W06-2925,D08-1059,0,"(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006)","Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of92.1% and86.2%, respectively. Graph-based (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006) and transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms offer two different approaches to data-driven dependency parsing. Given an input sentence, a graph-based algorithm finds the highest scoring parse tree from all possible outputs, scoring each complete tree, while a transition-based algorithm builds a parse by a sequence of actions, scoring each action individually.",Testing on the and Penn Treebank data the combined system gave state of the art accuracies respectively based and transition based parsing algorithms offer different approaches to data driven dependency parsing Given an input sentence graph based algorithm finds the highest scoring parse tree from all possible outputs scoring each complete tree while transition based algorithm builds parse by sequence of actions scoring each action individually
246,A,W06-2925,J11-1005,2,"(Carreras, Surdeanu, and Marquez 2006)","An alternative way to decide the number of training iterations is to set apart 10% from the training data Table 4 The accuracies of various word segmentors over the first SIGHAN bakeoff data. S01 93.8 90.1 95.1 93.0 95.5 S04 93.9 93.9 94.8 S05 94.2 89.4 91.8 95.9 S06 94.5 92.4 92.4 93.1 95.5 S08 90.4 93.6 92.0 94.8 S09 96.1 94.6 95.4 95.9 S10 94.7 94.7 94.8 S12 95.9 91.6 93.8 95.9 this article pure 97.0 94.6 94.6 95.4 95.5 this article combined 96.9 94.8 94.8 The best score in each column and the best average in each row is in boldface. *Zhang 2007 with the (Carreras, Surdeanu, and Marquez 2006) method applied (see text for details). as development test data, and use the rest for development training. For testing, all training data are used for training, with the number of training iterations set to be the number which gave the highest accuracy during the development experiments.",alternative way to decide the number of training iterations is to set apart from the training data Table The accuracies of various word segmentors over the bakeoff data this article pure this article combined The best score in each column and the best average in each row is in boldface with the method applied as development test data and use the rest for development training For testing all training data are used for training with the number of training iterations set to be the number which gave the highest accuracy during the development experiments
247,A,W06-2925,N12-1032,1,"(Nivre et al., 2006b; Carreras et al., 2006)","Projective trees are those in which every constituent (head plus all dependents) forms a complete subtree; non-projective parsing lacks this limitation. 2.3 Related work The organizers of the CoNLL 2007 shared task noted that languages with free word order and high morphological complexity are the most difficult for dependency parsing (Nivre et al., 2007). Most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based on each item in a token’s list of morphological attributes (Nivre et al., 2006b; Carreras et al., 2006); using the entire list as an atomic feature (Chang et al., 2006; Titov and Henderson, 2007); or generating features based on each pair of attributes in the cross-product of the lists of a potential head and dependent (McDonald et al., 2006; Nakagawa, 2007). Language-specific uses of morphological information have included using it to disambiguate function words (Bick, 2006) or to pick out finite verbs Unlabeled <attr>_agrees,head=<headPOS>,dep=<depPOS> <attr>_disagrees,head=<headPOS>,dep=<depPOS> head_<attr=value>,head=<headPOS>,dep=<depPOS> dep_<attr=value>,head=<headPOS>,dep=<depPOS> (Carreras et al., 2006).",Projective trees are those in which every constituent forms complete subtree non projective parsing lacks this limitation Related work The organizers of the shared task noted that languages with free word order and high morphological complexity are the most difficult for dependency parsing Most of the participants took language independent approaches toward leveraging this complexity into better performance generating machine learning features based on each item in token list of morphological attributes using the entire list as an atomic feature or generating features based on each pair of attributes in the cross product of the lists of potential head and dependent Language specific uses of morphological information have included using it to disambiguate function words or to pick out finite verbs Unlabeled attr headPOS attr headPOS headPOS headPOS
248,A,W06-2925,P10-1002,0,"(McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006)","The relative weightλ is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph.",The relative is adjusted to maximize the performance on the development set using an algorithm similar to minimum error rate training Both the graph based and the transition based parsing algorithms are related to our word pair classification model Similar to the graph based method our model is factored on dependency edges and its decoding procedure also aims to find maximum spanning tree in fully connected directed graph
249,A,W06-2925,W07-2217,0,Carreras et al. (2006),"We learn the parameters α from the training data with the perceptron (Rosemblatt, 1958), in the online multiclass formulation of the algorithm (Crammer & Singer, 2003) with uniform negative updates. The perceptron has been used in previous work on dependency parsing by Carreras et al. (2006), with a parser based on Eisner’s algorithm (Eisner, 2000), and also on incremental constituent parsing (Collins & Roark, 2006). Also the MST parser of McDonald uses a variant of the perceptron algorithm (McDonald, 2006).",learn the parameters from the training data with the in the online multiclass formulation of the algorithm with uniform negative updates The has been used in previous work on dependency parsing by with parser based on algorithm and also on incremental constituent parsing Also the parser of uses variant of the algorithm
250,A,W03-0428,D07-1016,0,"(Klein et al., 2003; Finkel et al., 2005)","As the English inclusion classifier does not rely on annotated data, it can be tested and evaluated once for the entire corpus. The ML classifier used for this experiment is a conditional Markov model tagger which is designed for, and proved successful in, named entity recognition in newspaper and biomedical text (Klein et al., 2003; Finkel et al., 2005). It can be trained to perform similar information extraction tasks such as English inclusion detection.",the inclusion classifier does not rely on annotated data it can be tested and evaluated once for the entire corpus The classifier used for this experiment is conditional model tagger which is designed for and proved successful in named entity recognition in newspaper and biomedical text can be trained to perform similar information extraction tasks such as inclusion detection
251,A,W03-0428,D08-1003,0,"(McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006)","An improved regex that addresses these problems is R2 = (\d{3}[-.\ ()]){1,2}[\dA-Z]{4}. While multiple machine learning approaches have been proposed for information extraction in recent years (McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998; Fukuda et al., 1998; Cunningham, 1999; Tanabe and Wilbur, 2002; Li et al., 2006; DeRose et al., 2007; Zhu et al., 2007). Yet, with a few notable exceptions, which we discuss later in Section 1.1, there has been very little work in reducing this human effort through the use of automatic learning techniques.",improved regex that addresses these problems is While multiple machine learning approaches have been proposed for information extraction in recent years manually created regexes remain widely adopted practical solution for information extraction Yet with few notable exceptions which we discuss later in there has been very little work in reducing this human effort through the use of automatic learning techniques
252,A,W03-0428,D11-1144,1,(Klein et al. 2003),"Indeed, in the classic HMM based NER, the simple feature used is the word identity itself, which might not be sufficiently discriminative in distinguishing between different classes. In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al. 2003). The principle of maximum entropy states that among all the distributions that satisfy feature constraints, we should pick the distribution with the highest entropy, since it makes the least assumption about the data and will have better generalization capability to unseen data.",Indeed in the classic based the simple feature used is the word identity itself which might not be sufficiently discriminative in distinguishing between different classes addition because of data sparsity problem due to the long tailed distribution of words in natural language sophisticated unknown word models are generally needed for good performance The principle of maximum entropy states that among all the distributions that satisfy feature constraints we should pick the distribution with the highest entropy since it makes the least assumption about the data and will have better generalization capability to unseen data
253,A,W03-0428,P05-2023,0,"(Klein et al., 2003)","We were therefore interested in determining the performance of a trained classifier for our task. We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al., 2003) and the identification of gene and protein names (Finkel et al., 2005). We performed several 10-fold cross-validation experiments with different feature sets.",were therefore interested in determining the performance of trained classifier for our task experimented with conditional model tagger that performed well on language independent and the identification of gene and protein names performed several fold cross validation experiments with different feature sets
254,A,W03-0428,P09-1116,1,"(Klein et al., 2003)","Table 4 CoNLL NER test set results System Test F1  Improv. Baseline CRF (Sec. 3.1) 83.78 W500 88.34 +4.56 P64 89.73 +5.94 P125 89.80 +6.02 W500 + P125 90.62 +6.84 W500 + P64 90.63 +6.85 W500 + P125 + P64 90.90 +7.12 W500 + P125 + P64+pos 90.62 +6.84 LDC64 87.24 +3.46 LDC125 88.33 +4.55 LDC64 +LDC125 88.44 +4.66 (Suzuki and Isozaki, 2008) 89.92 (Ando and Zhang, 2005) 89.31 (Florian et al., 2003) 88.76 (Chieu and Ng, 2003) 88.31 (Klein et al., 2003) 86.31 ford field Sports/American Football Information/Local & Regional Sports/Schedules & Tickets john deere gator Living/Landscaping & Gardening Living/Tools & Hardware Information/Companies & Industries Shopping/Stores & Products Shopping/Buying Guides & Researching justin timberlake lyrics Entertainment/Music Information/Arts & Humanities Entertainment/Celebrities Table 6 Labeler Consistency L1  L2 L3 Average F1 0.538 0.477 0.512 0.509 P 0.501 0.613 0.463 0.526 Given an input x, represented as a vector of m features: (x1, x2, ....., xm), a logistic regression classifier with parameter vector � L(w1, w2, ....., wm) computes the posterior probability of the output y, which is either 1 or -1, as We tag a query as belonging to a class if the probability of the class is among the highest 5 and is greater than 0.5. The baseline system uses only the words in the queries as features (the bag-of-words representation), treating the query classification problem as a typical text categorization problem.",Table test set results System Test Improv Baseline ford field Football Regional Tickets john deere gator Gardening Hardware Industries Products Guides Researching justin timberlake lyrics Humanities Table Labeler Consistency Average Given an input represented as vector of features logistic regression classifier with parameter vector computes the posterior probability of the output which is either or as tag query as belonging to class if the probability of the class is among the highest and is greater than The baseline system uses only the words in the queries as features treating the query classification problem as typical text categorization problem
255,A,W03-0428,P11-3019,0,"(Klein et al., 2003)","Here we represented morphemes as separate states, but including them as features together with the root state may produce better models. Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al., 2003). The author would like to thank William W.",Here we represented morphemes as separate states but including them as features together with the root state may produce better models Another approach we will also focus is dividing words into characters and applying character level models The author would like to thank
256,A,W03-0428,W07-1712,0,"(Klein et al., 2003)","Nevertheless, orthographic features can already be language-specific. For instance, capitalization is certainly very important for such languages as English or Dutch but it might be less useful for German. 2Sometimes, these types of features are referred to as wordexternal and word-internal (Klein et al., 2003) The feature set of some NER methods (Wu, 2002) also includes part-of-speech information and/or word prefixes and suffixes. Although this information (and especially lemmas) is very useful for the languages with rich morphology, it presupposes the existence of POS taggers for a given language.",Nevertheless orthographic features can already be language specific For instance capitalization is certainly very important for such languages as or but it might be less useful for these types of features are referred to as wordexternal and word internal The feature set of some methods also includes part of speech information word prefixes and suffixes Although this information is very useful for the languages with rich morphology it presupposes the existence of taggers for given language
257,A,P04-1084,C08-2026,0,"(Melamed et al., 2004)","A→ [BC] A(X1X2, Y1Y2)→ B(X1, Y1)C(X2, Y2) A→ 〈BC〉 A(X1X2, Y1Y2)→ B(X1, Y2)C(X2, Y1) A→ e | f A(e, f)→ ǫ A→ e | ǫ A(e, ǫ)→ ǫ A→ ǫ | f A(ǫ, f)→ ǫ Zens and Ney (2003) identify a class of alignment structures that cannot be induced by ITGs, but that can be induced by a number of similar synchronous grammar formalisms, e.g. synchronous tree substitution grammar (STSG) (Eisner, 2003). Inside-out alignments (Wu, 1997), such as the one in Example 1.3, cannot be induced byany of these theories; in fact, there seems to be no useful synchronous grammar formalisms available that handle inside-out alignments, with the possible exceptions of synchronous tree-adjoining grammars (Shieber and Schabes, 1990), Bertsch and Nederhof (2001) and generalized multitext grammars (Melamed et al., 2004), which are all way more complex than ITG, STSG and (2,2)-BRCG. Nevertheless, Wellington et al. (2006) report that 5% of the sentence pairs in an aligned parallel Chinese–English corpus contained inside-out alignments.",identify class of alignment structures that can not be induced by but that can be induced by number of similar synchronous grammar formalisms synchronous tree substitution grammar Inside out alignments such as the one in Example can not be induced byany of these theories in fact there seems to be no useful synchronous grammar formalisms available that handle inside out alignments with the possible exceptions of synchronous tree adjoining grammars and generalized multitext grammars which are all way more complex than and report that of the sentence pairs in an aligned parallel corpus contained inside out alignments
258,A,P04-1084,C10-2157,0,"(Wu, 1995; Melamed et al., 2004)","The Quasi-Synchronous Grammar formalism (Smith and Eisner, 2006) is a generative model that aims to produce the most likely target tree for a given source tree. It differs from the more strict synchronous grammar formalisms (Wu, 1995; Melamed et al., 2004) because it does not try to perform simultaneous parsing on parallel grammars; instead, the model learns an augmented target-language grammar whose rules make “soft alignments” with a given source tree. QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al., 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009).",The Quasi Synchronous Grammar formalism is generative model that aims to produce the most likely target tree for given source tree differs from the more strict synchronous grammar formalisms because it does not try to perform simultaneous parsing on parallel grammars instead the model learns an augmented target language grammar whose rules make soft alignments with given source tree has been applied to some tasks other than including answer selection for question answering paraphrase identification and parser adaptation and projection
259,A,P04-1084,D07-1078,0,"(Melamed et al., 2004; Zhang et al., 2006)","The last two sections are for experiments and conclusions. Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same language and assign the same probability to each string as the original grammar does.",The last sections are for experiments and conclusions Several researchers have already proposed methods for binarizing synchronous grammars in the context of machine translation binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same language and assign the same probability to each string as the original grammar does
260,A,P04-1084,P04-1083,1,"(Melamed et al., 2004)","The vehicle for the present guided tour shall be multitext grammar (MTG), which is a generalization of context-free grammar to the synchronous case (Melamed, 2003). We shall limit our attention to MTGs in Generalized Chomsky Normal Form (GCNF) (Melamed et al., 2004). This normal form allows simpler algorithm descriptions than the normal forms used by Wu (1997) and Melamed (2003).",The vehicle for the present guided tour shall be multitext grammar which is generalization of context free grammar to the synchronous case shall limit our attention to in Generalized Chomsky Normal Form This normal form allows simpler algorithm descriptions than the normal forms used by and
261,A,P04-1084,P06-1123,2,"(Melamed et al., 2004)","Each step of the derivation generates no more than two different nonterminals. Our measure of alignment complexity is analogous to what Melamed et al. (2004) call “fanout.”3 The least complex alignments on this measure — those that can be generated with zero gaps — are precisely those that can be generated by an 2If we imagine that each word is generated from a separate nonterminal as in GCNF (Melamed et al., 2004), then constraint 2 becomes a special case of constraint 1. 3For grammars that generate bitexts, fan-out is equal to the maximum number of allowed gaps plus two. Hindi/English 90 1 10 40 .04 Spanish/English 199 4 23 49 .03 French/English 447 2 15 29 .01 ITG.",Each step of the derivation generates no more than different nonterminals Our measure of alignment complexity is analogous to what call The least complex alignments on this measure those that can be generated with gaps are precisely those that can be generated by an we imagine that each word is generated from separate nonterminal as in then constraint becomes special case of constraint grammars that generate bitexts fan out is equal to the maximum number of allowed gaps plus
262,A,P04-1084,P10-1146,0,"(Melamed et al., 2004)","But both rules are arguably useful for translation. Wellington et al. therefore argue that in order to extract as many rules as possible, a more powerful formalism than synchronous CFG/TSG is required: for  example, generalized multitext grammar (Melamed et al., 2004), which is equivalent to synchronous set-local multicomponent CFG/TSG (Weir, 1988). But  the  problem illustrated  in  Figure 2 does not reflect a very deep fact about syntax or crosslingual divergences, but rather choices in annotation style that interact badly with the exact treeto-tree extraction heuristic.",But both rules are arguably useful for translation et al therefore argue that in order to extract as many rules as possible more powerful formalism than synchronous is required for example generalized multitext grammar which is equivalent to synchronous set local multicomponent But the problem illustrated in Figure does not reflect very deep fact about syntax or crosslingual divergences but rather choices in annotation style that interact badly with the exact treeto tree extraction heuristic
263,A,P05-1045,C10-1064,1,"(Finkel et al., 2005)","The annotation for English sentences is divided into two subtasks: entity mention recognition and relation detection. We utilized an offthe-shelf system, Stanford Named Entity Recognizer 4 (Finkel et al., 2005) for detecting entity mentions on the English sentences. The total number of English entities detected was 285,566.",The annotation for sentences is divided into subtasks entity mention recognition and relation detection utilized an offthe shelf system Stanford Named Entity Recognizer for detecting entity mentions on the sentences The total number of entities detected was
264,A,P05-1045,C10-1083,1,"(Finkel et al., 2005)","Here we employ a simple but representative technique to demonstrate the feasibility of discovering interesting new aspects for augmenting the ontology. We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer (Finkel et al., 2005). After that, we rank the phrases by pointwise Mutual Information (MI): where T is the given topic and ph refers to a candidate entity phrase.",Here we employ simple but representative technique to demonstrate the feasibility of discovering interesting new aspects for augmenting the ontology first extract named entities from scattered opinions using Stanford Named Entity Recognizer After that we rank the phrases by pointwise Mutual Information where is the given topic and ph refers to candidate entity phrase
265,A,P05-1045,C10-1087,1,"(Finkel et al., 2005)","This section describes this system’s architecture; the methods by which it was augmented to address discourse are presented in Section 5. Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution. For the latter purpose, we use OpenNLP and enable the substitution of coreferring terms.",This section describes this system architecture the methods by which it was augmented to address discourse are presented in Section Preprocessing First all documents are parsed and processed with standard tools for named entity recognition and coreference resolution For the latter purpose we use and enable the substitution of coreferring terms
266,A,P05-1045,C10-1091,1,(Finkel et al. 2005),"In the last example, to measure the sentiment of PP, we apply rule for the verb ‘end’ from the “Termination of activity” class, which reverses the non-neutral polarity of subject (in intransitive use of verb) or object (in transitive use of verb). For example, the polarity of both sentences ‘My whole enthusiasm and excitement disappear like a bubble touching a hot needle’ and ‘They discontinued helping children’ is negative. 5 Decision on Attitude Label The decision on the most appropriate final label for the clause, in case @AM annotates it using different attitude types according to the words with multiple annotations (e.g., see word ‘unfriendly’ in Table 1) or based on the availability of the words conveying different attitude types, is made based on the analysis of: 1) morphological tags of nominal heads and their premodifiers in the clause (e.g., first person pronoun, third person pronoun, demonstrative pronoun, nominative or genitive noun, etc.); 2) the sequence of hypernymic semantic relations of a particular noun in WordNet (Miller, 1990), which allows to determine its conceptual domain (e.g., “person, human being”, “artifact”, “event”, etc.); 3) the annotations from the Stanford Named Entity Recognizer (Finkel et al. 2005) that labels PERSON, ORGANIZATION, and LOCATION entities. For ex., ‘I feel highly unfriendly attitude towards me’ conveys emotion (‘NEG aff’: ‘sadness’), while ‘The shop assistant’s behavior was really unfriendly’ and ‘Plastic bags are environment unfriendly’ express judgment (‘NEG jud’) and appreciation (‘NEG app’), correspondingly. 6 Evaluation For the experiments, we used our own data set, as, to the best of our knowledge, there is no publicly available data set of sentences annotated by the fine-grained labels proposed in our work.",the last example to measure the sentiment of we apply rule for the verb end from the Termination of activity class which reverses the non neutral polarity of subject or object For example the polarity of both sentences whole enthusiasm and excitement disappear like bubble touching hot needle and They discontinued helping children is negative Decision on Attitude Label The decision on the most appropriate final label for the clause in case annotates it using different attitude types according to the words with multiple annotations or based on the availability of the words conveying different attitude types is made based on the analysis of morphological tags of nominal heads and their premodifiers in the clause the sequence of hypernymic semantic relations of particular noun in which allows to determine its conceptual domain the annotations from the Stanford Named Entity Recognizer that labels and entities For feel highly unfriendly attitude towards me conveys emotion while The shop assistant behavior was really unfriendly and Plastic bags are environment unfriendly express judgment and appreciation correspondingly Evaluation For the experiments we used our own data set as to the best of our knowledge there is no publicly available data set of sentences annotated by the fine grained labels proposed in our work
267,A,P05-1045,C10-1105,1,"(Finkel et al., 2005)","Semantic (5): We employ five semantic features. First, ifNPi is an NE, we create a feature whose value is the NE label ofNPi, as determined by the Stanford CRF-based NE recognizer (Finkel et al., 2005). However, ifNPi is a nominal, we create a feature that encodes the WordNet semantic class of which it is a hyponym, using the manually determined sense ofNPi. 4 Moreover, to improve generalization, we employ a feature whose value is the WordNet synset number of the head noun of a nominal.",Semantic employ semantic features ifNPi is an we create feature whose value is the label ofNPi as determined by the based recognizer However ifNPi is nominal we create feature that encodes the semantic class of which it is hyponym using the manually determined sense ofNPi Moreover to improve generalization we employ feature whose value is the synset number of the head noun of nominal
268,A,P05-1045,C10-1131,1,"(Finkel et al., 2005)","The parser was trained on the entire Penn TreeBank. The last step in the pipeline is named-entity tagging using Stanford NER Tagger (Finkel et al., 2005). Given an input text sentence and a hypothesis sentence, the task of RTE is to make predictions about whether or not the hypothesis can be entailed from the text sentence.",The parser was trained on the entire Penn TreeBank The last step in the pipeline is named entity tagging using Given an input text sentence and hypothesis sentence the task of is to make predictions about whether or not the hypothesis can be entailed from the text sentence
269,A,P05-1045,C10-2058,0,"(Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007)","For example, if two GPE entities are involved in a “conflict-attack” event, then they are unlikely to be connected by a “part-whole” relation; “Mahmoud Abbas” and “Abu Mazen” are likely to be coreferential if they get involved in the same “life-born” event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007)  can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization.",For example if entities are involved in conflict attack event then they are unlikely to be connected by part whole relation Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life born event Some prior work demonstrated the effectiveness of using semantic relations to improve entity coreference resolution while experimented with information fusion of relations across multiple documents The system can collapse and compress redundant facts extracted from multiple documents based on coreference resolution semantic similarity computation and normalization
270,A,P05-1045,C10-2078,1,"(Finkel et al., 2005)","While the instances were extracted from a news corpus, none of them are domain-specific and all expressions also occur in the BNC, which is a balanced, multi-domain corpus. To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger (Ratnaparkhi, 1996), parsed with the MaltParser2, and named entity tagged with the Stanford NE tagger (Finkel et al., 2005). The lemmatization was done by RASP (Briscoe and Carroll, 2006).",While the instances were extracted from news corpus none of them are domain specific and all expressions also occur in the which is balanced multi domain corpus compute the features which we extract in the next section all instances in our data sets were part of speech tagged by the tagger parsed with the and named entity tagged with the tagger The lemmatization was done by
271,A,P05-1045,C10-3011,1,"(Finkel et al., 2005)","This module processes the word tokenized file and creates an indexed entry for every word in the Antelogue repository. Named Entity Recognizer tagging (NER): We integrated Stanford’s NER tagger (Finkel et al., 2005). NER processor.",This module processes the word tokenized file and creates an indexed entry for every word in the Antelogue repository Named Entity Recognizer tagging integrated tagger processor
272,A,P05-1045,D07-1033,2,"(Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Krishnan and Manning, 2006)","Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not have different entity classes” were shown to be useful in named entity recognition (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Krishnan and Manning, 2006). We propose a new perceptron algorithm in this paper that can use non-local features along with local features.",Although this limitation makes training and inference tractable it also excludes the use of possibly useful non local features that are accessible after all labels are determined For example non local features such as same phrases in document do not have different entity classes were shown to be useful in named entity recognition propose new algorithm in this paper that can use non local features along with local features
273,A,P05-1045,D08-1035,0,"(e.g., Finkel 2005; Goldwater 2007)","The final segmentation is obtained by annealing the last 25,000 iterations to a temperature of zero. The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007). The total running time of our system is on the order of three minutes per document.",The final segmentation is obtained by annealing the last iterations to temperature of The use of annealing to obtain maximum posteriori configuration from sampling based inference is common The total running time of our system is on the order of three minutes per document
274,A,P05-1045,D09-1101,1,"(Finkel et al., 2005)","Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger (Toutanova et al., 2003). Semantic (1): The named entity (NE) tag ofwi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005). Gazetteers (8): Eight dictionaries containing pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms ofPERSON(6.3k).",Grammatical The part of speech tag of wi obtained using the log linear tagger Semantic The named entity tag ofwi obtained using the based recognizer Gazetteers dictionaries containing pronouns common words and words that are not names person names person titles and honorifics vehicle words location names company names and nouns extracted from that are hyponyms
275,A,P05-1045,D09-1119,0,"(Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009)","In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). Sequence labeling includes both a structural aspect (bracketing the chunks) and a tagging aspect (classifying the chunks).",this work we address the same question of determining the impact of lexical features on different family of tasks sequence labeling as illustrated by named entity recognition and chunking discussed above all state of the art published methods rely on lexical features for such tasks Sequence labeling includes both structural aspect and tagging aspect
276,A,P05-1045,D09-1120,1,"(Luo, 2005; Finkel and Manning, 2008)","At a high level, our system resembles a pairwise coreference model (Soon et al., 1999; Ng and Cardie, 2002; Bengston and Roth, 2008); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1, . . . ,mi−1, or the NULL mention to indicate the underlying entity has not yet been evoked. Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming first. 4The MUC measure is problematic when the system predicts many more clusters than actually exist (Luo, 2005; Finkel and Manning, 2008); also, singleton clusters do not contribute to evaluation. While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors.",high level our system resembles pairwise coreference model for each mention mi we select either single best antecedent amongst the previous mentions or the mention to indicate the underlying entity has not yet been evoked Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming measure is problematic when the system predicts many more clusters than actually exist also singleton clusters do not contribute to evaluation While much research has explored how to reconcile pairwise decisions to form coherent clusters we simply take the transitive closure of our pairwise decision which can and does cause system errors
277,A,P05-1045,D09-1158,1,"(Finkel et al., 2005)","In this paper, we apply it to the problem of named entity recognition (NER). In this section, we describe the NER classifier and the features used in our experiments. 4.1 NER features We used the features generated by the CRF package (Finkel et al., 2005). These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word.",this paper we apply it to the problem of named entity recognition this section we describe the classifier and the features used in our experiments features used the features generated by the package These features include the word string feature the case feature for the current word the context words for the current word and their cases the presence in dictionaries for the current word the position of the current word in the sentence prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word
278,A,P05-1045,D10-1048,1,"(Finkel et al., 2005)","The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X.",The syntactic information is used to identify the mention head words and to define the ordering of mentions in given sentence For fair comparison with previous work we do not use gold named entity labels or mention types but instead take the labels provided by the named entity recognizer Evaluation Metrics use evaluation metrics widely used in the literature pairwise computed over mention pairs in the same entity cluster which measures how many predicted clusters need to be merged to cover the gold clusters and which uses the intersection between predicted and gold clusters for given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall respectively refer the interested reader to
279,A,P05-1045,D10-1099,1,"(Finkel et al., 2005)","To summarize precisions across relations, we take their average, and their average weighted by the proportion of predicted instances for the given relation. We preprocess our textual data as follows: We first use the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in the corpus. The NER tagger segments each document into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LOCATION and NONE.",summarize precisions across relations we take their average and their average weighted by the proportion of predicted instances for the given relation preprocess our textual data as follows use the named entity recognizer to find entity mentions in the corpus The tagger segments each document into sentences and classifies each token into categories and
280,A,P05-1045,D11-1034,1,"(Finkel et al., 2005)","We focus on German-to-English. possible bias due to differences in punctuation conventions. Then, we use the Stanford Named Entity Recognizer (Finkel et al., 2005) to identify named entities, which we replace with a unique token (‘NE’). Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000).",focus on to possible bias due to differences in punctuation conventions Then we use the Named Entity Recognizer to identify named entities which we replace with unique token we replace all nouns with their tag we use the Tagger
281,A,P05-1045,D11-1072,1,(Finkel05),"We first identify noun phrases that potentially denote named entities. We use the Stanford NER Tagger (Finkel05) to discover these and segment the text accordingly. Entity Candidates: For possible entities (with unique canonical names) that a mention could denote, we harness existing knowledge bases like DBpedia or YAGO.",identify noun phrases that potentially denote named entities use the to discover these and segment the text accordingly Entity Candidates For possible entities that mention could denote we harness existing knowledge bases like DBpedia or
282,A,P05-1045,D11-1075,0,"(Finkel et al., 2005)","The first data set contains a set of seminar announcements (Freitag and McCallum, 1999), annotated with four slot labels, namely stime (start time), etime (end time), speaker and location. We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL (Finkel et al., 2005). There are 309 seminar announcements with 2262 candidates in this data set.",The data set contains set of seminar announcements annotated with slot labels namely stime etime speaker and location used as candidates all strings labeled in the annotated data as well as all named entities found by the tagger for There are seminar announcements with candidates in this data set
283,A,P05-1045,D11-1135,1,"(Finkel et al., 2005)","First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions.",we employ tools to tokenize sentence split and Part Speech tag document we recognize named entities by labelling tokens with and tags Consecutive tokens which share the same category are assembled into entity mentions
284,A,P05-1045,D11-1141,1,"(Finkel et al., 2005)","We report results at segmenting named entities in Table 6. Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer (Finkel et al., 2005), T-SEG obtains a 52% increase in F1 score. Because Twitter contains many distinctive, and infrequent entity types, gathering sufficient training data for named entity classification is a difficult task.",report results at segmenting named entities in Table Compared with the state of the art newstrained Stanford Named Entity Recognizer obtains increase in score Because contains many distinctive and infrequent entity types gathering sufficient training data for named entity classification is difficult task
285,A,P05-1045,D12-1009,1,"(Finkel et al., 2005)","The parent of block b is the random variable rb, and we alternate between sampling values of the latent classes z and the parents r. The sampling distributions are annealed, as a search technique to find the best configuration of assignments (Finkel et al., 2005). At temperature τ , we sample a block’s parent according to: For each conversation thread, any message is a candidate for the parent of block b (except b itself) including the dummy “start” block.",The parent of block is the random variable rb and we alternate between sampling values of the latent classes and the parents The sampling distributions are annealed as search technique to find the best configuration of assignments temperature we sample block parent according to For each conversation thread any message is candidate for the parent of block including the dummy start block
286,A,P05-1045,D12-1042,0,"(Finkel et al., 2005)","The first was developed by Riedel et al. (2010) by aligning Freebase3 relations with the New York Times (NYT) corpus. They used the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence. Riedel et al. (2010) observes that evaluating on this corpus underestimates true extraction accuracy because Freebase is incomplete.",The was developed by by aligning relations with the New York Times corpus They used the named entity recognizer to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence observes that evaluating on this corpus underestimates true extraction accuracy because is incomplete
287,A,P05-1045,D12-1072,0,"(Finkel et al., 2005)","Their aim was to automatically identify both quotes and speakers, and then to attribute each quote to a speaker, in a corpus of classic literature that they compiled themselves. To identify potential speakers they used the Stanford NER tagger (Finkel et al., 2005) and a method outlined in Davis et al. (2003) that allowed them to find nominal character references. They then grouped name variants and pronominal mentions into a coreference chain.",Their aim was to automatically identify both quotes and speakers and then to attribute each quote to speaker in corpus of classic literature that they compiled themselves identify potential speakers they used the tagger and method outlined in that allowed them to find nominal character references They then grouped name variants and pronominal mentions into coreference chain
288,A,P05-1045,D12-1076,1,"(Finkel et al., 2005)","We therefore extract two extra kinds of features, named entities that do not appear in the Wikipedia anchor text dictionary, and biographical information. We use Stanford Named Entity Recognizer(Finkel et al., 2005) to collect named entities which are not in the Wikipedia list. We use regular expressions to extract email address, phone numbers and birth years.",therefore extract extra kinds of features named entities that do not appear in the anchor text dictionary and biographical information use Stanford Named Entity Recognizer to collect named entities which are not in the list use regular expressions to extract email address phone numbers and birth years
289,A,P05-1045,D12-1080,1,"(Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011)","The search process assumes that if a fluent’s entity and slot value co-occur in a sentence,5 that sentence is typically a positive example of the fluent.6 This is sometimes known asdistant supervision(Craven and Kumlien, 1999; Mintz et al., 2009). We use the Stanford Core NLP suite (Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011) to annotate each document with POS and NER tags, parse trees, and coreference chains. On top of this, we apply a rule-based temporal expression extractor (Chang and Manning, 2012).",The search process assumes that if fluent entity and slot value co occur in that sentence is typically positive example of the This is sometimes known asdistant supervision use the Stanford Core suite to annotate each document with and tags parse trees and coreference chains top of this we apply rule based temporal expression extractor
290,A,P05-1045,D12-1131,0,Finkel et al. (2005),"Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models.",Skip chain and collective inference have been applied to problems in and to named entity recognition also integrated non local information into entity annotation algorithms using sampling Our model can be applied to variety of off theshelf structured prediction models
291,A,P05-1045,D13-1040,1,"(Finkel et al., 2005)","The latter were extracted from 428K documents. After post-processing (tokenization, sentence-splitting, and part-of-speech tagging), Must-link Tuple F(i, NEPAIR:PER-PER, TRIGGER:wife) ∧ P(j, fi) ∧ P(k, fi)⇒ ¬Z(j, t) ∨ Z(k, r) F(i, NEPAIR:PER-LOC, TRIGGER:die) ∧ P(j, fi) ∧ P(k, fi)⇒ ¬Z(j, t) ∨ Z(k, r) F(i, PATH:←nsubj←die→prep→in→pobj→) ∧ P(j, fi) ∧ P(k, fi)⇒ ¬Z(j, t) ∨ Z(k, r) F(i, SOURCE:Kobe, DEST:Lakers) ∧ P(j, fi) ∧ P(k, fi)⇒ ¬Z(j, t) ∨ Z(k, r) named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC (Finkel et al., 2005). Dependency paths for each pair of named entity mentions were extracted from the output of the MaltParser (Nivre et al., 2004).",The latter were extracted from documents After post processing Must link named entities were automatically recognized and labeled with and Dependency paths for each pair of named entity mentions were extracted from the output of the
292,A,P05-1045,D13-1042,1,"(Finkel et al., 2005)","But human editorial judgment being the bottleneck, we sampled 50% or 50,000 snippets, whichever was smaller. Starting with about 752,450 pages, we ran the Stanford NER (Finkel et al., 2005) to mark person spans. Pages with fewer than five non-person tokens per person were discarded; this effectively discarded long list pages without any informative text to disambiguate anyone, and left us with 574,135 pages.",But human editorial judgment being the bottleneck we sampled or snippets whichever was smaller Starting with about pages we ran the to mark person spans Pages with fewer than five non person tokens per person were discarded this effectively discarded long list pages without any informative text to disambiguate anyone and left us with pages
293,A,P05-1045,D13-1043,1,"(Finkel et al., 2005)","Extracting Named Entities. EXEMPLAR employs the Stanford NER (Finkel et al., 2005) to recognize named entities. We consider these types of entities: people, organization, location, miscellaneous and date.",Extracting Named Entities employs the to recognize named entities consider these types of entities people organization location miscellaneous and date
294,A,P05-1045,D13-1103,1,"(Finkel et al., 2005)","Extracted text is divided into sentences using Punkt unsupervised sentence splitter (Kiss and Strunk, 2006). The QA-SYS performs Part of Speech tagging using Stanford POS tagger (Toutanova et al., 2003), and Named Entity Recognition using Stanford NER (Finkel et al., 2005), and then builds a Lucene index over the set of input documents. In the second stage the QA-SYS performs POS tagging, NE recognition, and question type classification for an input question.",Extracted text is divided into sentences using unsupervised sentence splitter The performs Part of tagging using tagger and Named Entity Recognition using and then builds index over the set of input documents the stage the performs tagging recognition and question type classification for an input question
295,A,P05-1045,D13-1117,2,Finkel et al. (2005),"We predicted the label sequence Y = {LOC, MISC, ORG, PER, O}T without considering the BIO tags. For training the CRF model, we used a comprehensive set of features from Finkel et al. (2005) that gives state-of-the-art results on this task. A total number of 437906 features were generated on the CoNLL-2003 training dataset.",predicted the label sequence without considering the tags For training the model we used comprehensive set of features from that gives state of the art results on this task total number of features were generated on the training dataset
296,A,P05-1045,D13-1136,1,"(Finkel et al., 2005)","NYT+FB This dataset, developed by (Riedel et al., 2010), aligns Freebase relations with the New York Times corpus. Entities were found using the Stanford named entity tagger (Finkel et al., 2005), and were matched to their name in Freebase. For each mention, sentence level features are extracted which include part of speech, named entity and dependency tree path properties.",This dataset developed by aligns relations with the New York Times corpus Entities were found using the named entity tagger and were matched to their name in For each mention sentence level features are extracted which include part of speech named entity and dependency tree path properties
297,A,P05-1045,D13-1142,0,"(Finkel et al., 2005; Ratinov et al., 2011; Bunescu and Pasca, 2006; Kulkarni et al., 2009; Milne and Witten, 2008)","Detected NEs may also serve as anchor text for the link. NER is a fairly researched field (Finkel et al., 2005; Ratinov et al., 2011; Bunescu and Pasca, 2006; Kulkarni et al., 2009; Milne and Witten, 2008) and is also used in several commercial applications such as Zemanta, OpenCalais and AlchemyAPI4, which are able to automatically insert links for a NE pointing to a knowledge base such as Wikipedia or IMDB. However, at this point they are unable to link to arbitrary documents, but may be useful in conjunction with other methods.",Detected may also serve as anchor text for the link is fairly researched field and is also used in several commercial applications such as Zemanta OpenCalais and which are able to automatically insert links for pointing to knowledge base such as Wikipedia or However at this point they are unable to link to arbitrary documents but may be useful in conjunction with other methods
298,A,P05-1045,D13-1178,1,"(Finkel et al., 2005)","The set of types are: person, organization, location, time unit, number, amount, group, business, executive, leader, effect, activity, game, sport, device, equipment, structure, building, substance, nutrient, drug, illness, organ, animal, bird, fish, art, book, and publication. To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005)5, and also look up the argument in WordNet 2.1 and record 5We used the system downloaded from: stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it. the first three senses if they map to our target semantic types. We use regular expressions to recognize dates and numeric expressions, and map personal pronouns to <person>.",The set of types are person organization location time unit number amount group business executive leader effect activity game sport device equipment structure building substance nutrient drug illness organ animal bird fish art book and publication assign types to arguments we apply Stanford Named Entity Recognizer and also look up the argument in and record used the system downloaded from and used the class model distributed with it the senses if they map to our target semantic types use regular expressions to recognize dates and numeric expressions and map personal pronouns to person
299,A,P05-1045,D13-1191,1,"(Finkel et al., 2005)","Although our modeling approach ultimately treats texts as bags of terms (unigrams and bigrams), one important preprocessing step was taken to further improve the interpretability of the inferred representation: named entity mentions of persons. We identified these mentions of persons using Stanford NER (Finkel et al., 2005) and treated each person mention as a single token. In our qualitative analysis of the model (§4.2), we will show how this special treatment of person mentions enables the association of well-known individuals with debate topics.",Although our modeling approach ultimately treats texts as bags of terms important preprocessing step was taken to further improve the interpretability of the inferred representation named entity mentions of persons identified these mentions of persons using and treated each person mention as single token our qualitative analysis of the model we will show how this special treatment of person mentions enables the association of well known individuals with debate topics
300,A,P05-1045,E09-1007,0,"(Finkel et al., 2005)","The different materials that we obtained constitute the CBC system’s NE resource. Our aim now is to exploit this resource and to show that it allows to improve the performances of different classic NER systems. • CBC-NER system M (in short CBC M) based on the CBC system’s NE resource using the manual cluster annotation (line 1 in Table 1), • CBC-NER system A (in short CBC A) based on the CBC system’s NE resource using the automatic cluster annotation (line 1 in Table 1), • XIP NER or in short XIP (Brun and Hagège, 2004) (line 2 in Table 1), • Stanford NER (or in short Stanford) associated to the following model provided by the tool and which was trained on different news 1 CBC-NER system M 71.67 23.47 35.36 CBC-NER system A 70.66 32.86 44.86 Stanford + XIP + CBC M 72.94 77.70 75.24 Stanford + XIP + CBC A 73.55 78.93 76.15 GATE + XIP + CBC M 69.62 67.79 68.69 GATE + XIP + CBC A 69.87 69.10 69.48 corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005) (line 3 in Table 1), • GATE NER or in short GATE (Cunningham et al., 2002) (line 4 in Table 1), • and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned NER systems (lines 5 to 7 in Table 1). Notice that these baseline hybrid systems use the annotation combination process described in §2.6.1.",The different materials that we obtained constitute the system resource Our aim now is to exploit this resource and to show that it allows to improve the performances of different classic systems system based on the system resource using the manual cluster annotation system based on the system resource using the automatic cluster annotation or in short associated to the following model provided by the tool and which was trained on different news system system corpora ner eng or in short and several hybrid systems which are given by the combination of pairs taken among the set of the last mentioned systems Notice that these baseline hybrid systems use the annotation combination process described in
301,A,P05-1045,E09-1011,1,"(Finkel et al., 2005)","We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tagger (Ratnaparkhi, 1996). We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005). On the Arabic side, we normalize the data by changing final ’Y’ to ’y’, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources.",then proceed to split the data into smaller sentences and tag them using Ratnaparkhi Maximum Entropy Tagger parse the data using the Collins Parser and then tag person location and organization names using the Stanford Named Entity Recognizer the side we normalize the data by changing final to and changing the various forms of hamza to bare since these characters are written inconsistently in some sources
302,A,P05-1045,E09-1037,0,"(Finkel et al., 2005)","There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference.",There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non local features Some stem from work on graphical models including loopy belief propagation Gibbs sampling sequential Monte Carlo methods such as particle filtering and variational inference Also relevant are stacked learning interpretable as approximation of non local feature values and estimation which allows training without inference
303,A,P05-1045,E12-1029,1,"(Finkel et al., 2005)","The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger (Finkel et al., 2005) to identify Named Entities within the NP. The context features include four words to the left of the NP, four words to the right of the NP, and the lexico-syntactic patterns generated by AutoSlog to capture expressions around the NP (see (Riloff, 1993) for details).",The features include the head noun and its premodifiers also use the tagger to identify Named Entities within the The context features include words to the left of the words to the right of the and the lexico syntactic patterns generated by to capture expressions around the
304,A,P05-1045,E12-1033,1,"(Finkel et al., 2005)","ETHICS).8 For semantic role labeling we use SWIRL9, for chunk parsing CASS (Abney, 1991) and for constituency parsing Stanford Parser (Klein and Manning, 2003). Named-entity information is provided by Stanford Tagger (Finkel et al., 2005). Convolution kernels (CK) are special kernel functions.",For semantic role labeling we use for chunk parsing and for constituency parsing Parser Named entity information is provided by Tagger Convolution kernels are special kernel functions
305,A,P05-1045,E12-1054,1,"(Finkel et al., 2005)","Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER (Finkel et al., 2005). Normal Spelling Error We apply the Jazzy spelling detector4 and rule out all cases in which it is able to detect the error.",Named Entity The replaced token should not be part of named entity For this purpose we applied the Normal Spelling Error apply the spelling and rule out all cases in which it is able to detect the error
306,A,P05-1045,I08-5014,1,"(Finkel, Grenager, and Manning, 2005)","The data collected for English is used to populate the English named entity database which is significantly accurate. We have used the freely available Stanford Named Entity Recognizer (Finkel, Grenager, and Manning, 2005) in our engine. The data collected for Indian languages will be used to build a database of named entities for the given language. 2.2 Parser The crawler saves the content in an html form onto the system.",The data collected for is used to populate the named entity database which is significantly accurate have used the freely available Stanford Named Entity Recognizer in our engine The data collected for languages will be used to build database of named entities for the given language The crawler saves the content in an html form onto the system
307,A,P05-1045,J11-1002,0,"(Finkel, Grenager, and Manning 2005)","As propagation is not performed in KN06, we also implemented a nonpropagation version of our approach, in which opinion words are only extracted by the seed words and targets which are extracted by both the seeds and extracted opinion words. Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique (Lafferty, McCallum, and Pereira 2001) for extraction, which is a popular information extraction method and has been successfully used in labeling tasks such as POS tagging (Lafferty, McCallum, and Pereira 2001) and Named Entity Recognition (Finkel, Grenager, and Manning 2005). The well-known toolkit CRF++4 is employed.",propagation is not performed in we also implemented nonpropagation version of our approach in which opinion words are only extracted by the seed words and targets which are extracted by both the seeds and extracted opinion words Furthermore as our tasks can be regarded as sequential labeling problem we experimented with the conditional random fields technique for extraction which is popular information extraction method and has been successfully used in labeling tasks such as tagging and Named Entity Recognition The well known toolkit is employed
308,A,P05-1045,J12-4004,1,"(Finkel, Grenager, and Manning 2005)","We focus on French-to-English, but the results are robust and consistent (we repeated the same experiments for all language pairs, with very similar outcomes). First, we remove all punctuation to eliminate possible bias due to differences in punctuation conventions.3 Then, we use the Stanford Named Entity Recognizer (Finkel, Grenager, and Manning 2005) to identify named entities, which we replace with a unique token (‘NE’). Next, we replace all nouns with their part-of-speech (POS) tag; we use the Stanford POS Tagger (Toutanova and Manning 2000).",focus on to but the results are robust and consistent we remove all punctuation to eliminate possible bias due to differences in punctuation Then we use the Named Entity Recognizer to identify named entities which we replace with unique token we replace all nouns with their part of speech tag we use the Tagger
309,A,P05-1045,J13-2001,1,"(Finkel, Grenager, and Manning 2005)","To study whether the final performance of NE alignment is sensitive to the choice of initial NE recognizers, we investigate the final alignment performance across different Chinese and English NE recognizers. First, we test the NE alignment performance with the same Chinese NE recognizer (Wu’s system, adopted earlier) but with different English NE recognizers that include the Mallet toolkit (used before), the Stanford NE recognizer (Finkel, Grenager, and Manning 2005), and Minor Third (Cohen 2004). Table 4 shows the type-insensitive and type-sensitive (within parentheses) results.",study whether the final performance of alignment is sensitive to the choice of initial recognizers we investigate the final alignment performance across different and recognizers we test the alignment performance with the same recognizer but with different recognizers that include the toolkit the recognizer and Minor Third Table shows the type insensitive and type sensitive results
310,A,P05-1045,N06-1054,1,"(Finkel et al., 2005)","Nonetheless, such global properties can improve the accuracy of a model, so recent NLP papers have considered practical techniques for decoding with them. Such techniques include Gibbs sampling (Finkel et al., 2005), a general-purpose Monte Carlo method, and integer linear programming (ILP), (Roth and Yih, 2005), a general-purpose exact framework for NP-complete problems. Under generative models such as hidden Markov models, the probability of a labeled sequence depends only on its local properties.",Nonetheless such global properties can improve the accuracy of model so recent papers have considered practical techniques for decoding with them Such techniques include sampling general purpose Monte Carlo method and integer linear programming general purpose exact framework for complete problems Under generative models such as hidden models the probability of labeled sequence depends only on its local properties
311,A,P05-1045,N07-1009,0,Finkel et al. (2005),"Undirected graphical models such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) have shown great success for problems involving structured output variables (e.g. Wellner et al. (2004), Finkel et al. (2005)). For many real-world NLP applications, however, the required graph structure can be very complex, and computing the global normalization factor even approximately can be extremely hard.",Undirected graphical models such as Conditional Random Fields have shown great success for problems involving structured output variables For many real world applications however the required graph structure can be very complex and computing the global normalization factor even approximately can be extremely hard
312,A,P05-1045,N07-1011,0,"(Finkel et al., 2005)","However, they do not investigate rank-based loss functions. Others have attempted to train global scoring functions using Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004). The main distinctions of our approach are that it is simple to implement, not computationally intensive, and adaptable to arbitrary loss functions.",However they do not investigate rank based loss functions Others have attempted to train global scoring functions using sampling message propagation and linear programming The main distinctions of our approach are that it is simple to implement not computationally intensive and adaptable to arbitrary loss functions
313,A,P05-1045,N07-1042,0,Finkel et al. (2005),"In the context of information fusion of single relationships across multiple documents, Downey et al. (2005) propose a method that models the probabilities of positive and negative extracted classifications. More distantly related, Sutton and McCallum (2004) and Finkel et al. (2005) propose graphical models for combining information about a given entity from multiple mentions. In the field of question answering, Prager et al. (2004) answer a question about the list of compositions produced by a given subject by looking for related information about the subject’s birth and death.",the context of information fusion of single relationships across multiple documents propose method that models the probabilities of positive and negative extracted classifications More distantly related and propose graphical models for combining information about given entity from multiple mentions the field of question answering answer question about the list of compositions produced by given subject by looking for related information about the subject birth and death
314,A,P05-1045,N07-2046,0,"(Finkel et al, 2005; Ji & Grishman, 2005)","This might help alleviate the kind of error propagation with dualpass strategies that particularly afflicts long documents. Recent applications of statistical coreference models are beginning to show promise (Finkel et al, 2005; Ji & Grishman, 2005). Lastly, we can see this whole study as a particular challenge case for transfer learning, and indeed such work as Sutton and McCallum’s (2005) has looked at the name-tagging task from a transfer learning standpoint.",This might help alleviate the kind of error propagation with dualpass strategies that particularly afflicts long documents Recent applications of statistical coreference models are beginning to show promise Lastly we can see this whole study as particular challenge case for transfer learning and indeed such work as has looked at the name tagging task from transfer learning standpoint
315,A,P05-1045,N09-1037,1,"(Finkel et al., 2005)","The model we trained had 200 clusters, and we used it to assign each word in the training and test data to one of the clusters. For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005). For parse features, we used the exact same features as described in (Finkel and Manning, 2008).",The model we trained had clusters and we used it to assign each word in the training and test data to one of the clusters For the named entity features we used fairly standard feature set similar to those described in For parse features we used the exact same features as described in
316,A,P05-1045,N09-1068,1,"(Finkel et al., 2005)","The use of CRFs for sequence modeling has become standard so we will omit the model details; good explanations can be found in a number of places (Lafferty et al., 2001; Sutton and McCallum, 2007). Our features were based on those in (Finkel et al., 2005). We used three named entity datasets, from the CoNLL 2003, MUC-6 and MUC-7 shared tasks.",The use of for sequence modeling has become standard so we will omit the model details good explanations can be found in number of places Our features were based on those in used named entity datasets from the and shared tasks
317,A,P05-1045,N10-1068,0,"(Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009)","Bayesian learning is a wide-ranging field. We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009). Our overall plan is to give a generic algorithm for Bayesian training that is a “drop-in replacement” for EM training.",learning is wide ranging field focus on training using sampling because it has been popularly applied in the natural language literature Our overall plan is to give generic algorithm for training that is drop in replacement for training
318,A,P05-1045,N10-1117,0,"(Finkel et al., 2005)","In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008). In both cases we usually work in the framework of graphical models—in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials.",order to explore richer model structures the community has recently started to investigate the use of other well known machine learning techniques for marginal inference such technique is Markov chain Monte Carlo and in particular sampling another is sum product belief propagation both cases we usually work in the framework of graphical our case with factor graphs that describe our distributions through variables factors and factor potentials
319,A,P05-1045,N10-1121,1,"(Finkel et al., 2005)","The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008).",The documents were parsed using the Parser Namedentity information was obtained by the tagger roles were obtained by using the parser by
320,A,P05-1045,N12-1008,1,Finkel et al. (2005),"Constraints have been explored both at sentence and document level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency.",Constraints have been explored both at sentence and document level For example employ document level constraints to encourage global consistency of named entity assignments Likewise use constraints at multiple levels such as sentence level constraints to specify field boundaries and global constraints to ensure relation level consistency
321,A,P05-1045,N12-1013,2,"(Sutton and McCallum, 2005; Finkel et al., 2005)","By switching to ERMA training, we improve this result further to 85.1%. The second application, information extraction from seminar announcements, has been modeled previously with skip-chain CRFs (Sutton and McCallum, 2005; Finkel et al., 2005). The skip-chain CRF introduces loops and requires approximate inference, which motivates minimum risk training.",switching to training we improve this result further to The application information extraction from seminar announcements has been modeled previously with skip chain The skip chain introduces loops and requires approximate inference which motivates minimum risk training
322,A,P05-1045,N12-1065,1,"(Finkel et al., 2005)","Our experiments and their results are described in Section 4. Section 5 draws our conclusions and indicates avenues for future work. 2.1 Named entity recognition The Stanford named entity recognition (NER) software1 (Finkel et al., 2005) is an implementation of linear chain Conditional Random Field (CRF) sequence models, which includes a three class (person, organization, location and other) named entity recognizer for English. 2.2 Topic detection LDA (Blei et al., 2003) is a generative probabilistic model where documents are viewed as mixtures over underlying topics, and each topic is a distribution over words. Both the document-topic and the topicword distributions are assumed to have a Dirichlet prior.",Our experiments and their results are described in Section Section draws our conclusions and indicates avenues for future work Named entity recognition The named entity recognition is an implementation of linear chain Conditional Random Field sequence models which includes class named entity recognizer for Topic detection is generative probabilistic model where documents are viewed as mixtures over underlying topics and each topic is distribution over words Both the document topic and the topicword distributions are assumed to have prior
323,A,P05-1045,N12-1080,1,"(Finkel et al., 2005)","Our similarity measure is calculated using the number of shared named entities and nouns between sentences as seen in equation 9. For identification of named entities, we use Stanford NER (Finkel et al., 2005). It is straightforward to weight the resulting TextRank scores for each sentence using their cluster’s temporal importance. 4 Experimental Results We test on a set of 13 Wikipedia articles describing historical battles.",Our similarity measure is calculated using the number of shared named entities and nouns between sentences as seen in equation For identification of named entities we use is straightforward to weight the resulting scores for each sentence using their cluster temporal importance Experimental Results test on set of Wikipedia articles describing historical battles
324,A,P05-1045,N12-1084,0,"(Finkel, Grenager, and Manning, 2005; Ratinov and Roth, 2009; Ritter et al., 2011)","This sub-task labels each person-mention with a bullying role. It uses Named Entity Recognition (NER) (Finkel, Grenager, and Manning, 2005; Ratinov and Roth, 2009; Ritter et al., 2011) as a subroutine to identify named person entities, though we are also interested in unnamed persons such as “my teacher” and pronouns. It is related to Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002; Punyakanok, Roth, and Yih, 2008) but differs critically in that our roles are not tied to specific verb predicates.",This sub task labels each person mention with bullying role uses Named Entity Recognition as subroutine to identify named person entities though we are also interested in unnamed persons such as my teacher and pronouns is related to Semantic Role Labeling but differs critically in that our roles are not tied to specific verb predicates
325,A,P05-1045,N12-1085,0,"(Finkel et al., 2005)","While there are many approaches for inference in statistical models, we turn to MCMC methods (Neal, 1993) to discover the underlying structure of the model. More specifically, we seek a posterior distribution over latent variables that partition words in a sentence into flow and inert groups; we estimate this posterior using Gibbs sampling (Finkel et al., 2005). The sampler requires an initial state that respects the invariant.",While there are many approaches for inference in statistical models we turn to methods to discover the underlying structure of the model More specifically we seek posterior distribution over latent variables that partition words in sentence into flow and inert groups we estimate this posterior using sampling The sampler requires an initial state that respects the invariant
326,A,P05-1045,N13-1006,1,Finkel et al. (2005),"BerkeleyAligner also gives posterior probabilities Pa for each aligned word pair. We used the CRF-based Stanford NER tagger (using Viterbi decoding) as our baseline monolingual NER tool.6 English features were taken from Finkel et al. (2005). Table 1 lists the basic features of Chinese NER, where ◦ means string concatenation and yi is the named entity tag of the ith word wi.",also gives posterior probabilities for each aligned word pair used the based tagger as our baseline monolingual features were taken from Table lists the basic features of where means string concatenation and is the named entity tag of the ith word wi
327,A,P05-1045,N13-1007,0,"(Finkel et al., 2005)","Cohen’s kappa (Cohen, 1960) was 0.83 for English, 0.88 for Japanese, 5We filtered out phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore, Proposedlocal, SMT , and P&D, since most of them were not paraphrases. We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER. Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of Hashimoto et al. (2011)), and thus we did not apply the filter to them any further. and 0.85 for Chinese, all of which indicated reasonably good (Landis and Koch, 1977).",Cohen kappa was for for filtered out phrase pairs in which phrase contained named entity but the other did not contain the named entity from the output of ProposedScore Proposedlocal and since most of them were not paraphrases used for named entity recognition for and for Hashisup and did the named entity filtering of the same kind and thus we did not apply the filter to them any further and for all of which indicated reasonably good
328,A,P05-1045,N13-1037,0,"(Finkel et al., 2005)","In part-of-speech tagging, the accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to 85% accuracy on Twitter (Gimpel et al., 2011). In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test set (Finkel et al., 2005). In parsing, Foster et al. (2011) report double-digit decreases in accuracy for four different state-of-the-art parsers when applied to social media text.",part of speech tagging the accuracy of the tagger falls from on Wall Street Journal text to accuracy on named entity recognition the trained recognizer achieves measure down from on the test set parsing report double digit decreases in accuracy for different state of the art parsers when applied to social media text
329,A,P05-1045,P06-1059,2,"(Finkel et al., 2005)","In conventional CRFs and semi-CRFs, one can only use the information on the adjacent previous label when defining the features on a certain state or entity. In NER tasks, however, information about a distant entity is often more useful than information about the previous state (Finkel et al., 2005). For example, consider the sentence “... including Sp1 and CP1.” where the correct labels of “Sp1” and “CP1” are both “protein”.",conventional and semi one can only use the information on the adjacent previous label when defining the features on certain state or entity tasks however information about distant entity is often more useful than information about the previous state For example consider the sentence including and where the correct labels of and are both protein
330,A,P05-1045,P06-1141,3,Finkel et al. (2005),"Both these approaches use loopy belief propagation (Pearl, 1988; Yedidia et al., 2000) for approximate inference. Finkel et al. (2005) hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data. They then employ Gibbs sampling (Geman and Geman, 1984) for dealing with their local feature weights and their non-local penalties to do approximate inference.",Both these approaches use loopy belief propagation for approximate inference hand set penalties for inconsistency in entity labeling at different occurrences in the text based on some statistics from training data They then employ sampling for dealing with their local feature weights and their non local penalties to do approximate inference
331,A,P05-1045,P06-2054,0,"(Sutton and McCallum, 2004; Finkel et al., 2005)","One is to add edges to structure to allow higher-order dependencies and another is to add features (or observable variables) to encode the non-locality. An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (Sutton and McCallum, 2004; Finkel et al., 2005). However, this approach requires additional time complexity in inference/learning time and it is only suitable for representing constraints by enforcing label consistency.",is to add edges to structure to allow higher order dependencies and another is to add features to encode the non locality additional consistent edge of linear chain conditional random field explicitly models the dependencies between distant occurrences of similar words However this approach requires additional time complexity in time and it is only suitable for representing constraints by enforcing label consistency
332,A,P05-1045,P08-1031,0,Finkel et al. (2005),"Doing so analytically is intractable due to the complexity of the model, but sampling-based techniques can be used to estimate the posterior. We employ Gibbs sampling, previously used in NLP by Finkel et al. (2005) and Goldwater et al. (2006), among others. This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model (Gelman et al., 2004).",Doing so analytically is intractable due to the complexity of the model but sampling based techniques can be used to estimate the posterior employ Gibbs sampling previously used in by and among others This technique repeatedly samples from the conditional distributions of each hidden variable eventually converging on Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model
333,A,P05-1045,P08-2012,1,Finkel et al. (2005),"We used the MUC-6 formal training and test data, as well as theNWIRE andBNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER, but we found that several documents where too long forlp solve to find a solution.4 We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005). The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger.",used the formal training and test data as well as portions of the corpus This corpus had portion but we found that several documents where too long forlp solve to find added named entity tags to the data using the tagger of The data is already annotated with tags so when they conflicted they overrode the tags output by the tagger
334,A,P05-1045,P08-4003,1,"(Finkel et al., 2005)","Using a generic format for standoff annotation allows the use of the coreference resolution as part of a larger system, but also performing qualitative error analysis using integrated MMAX2 functionality (annotation chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution.",Using generic format for standoff annotation allows the use of the coreference resolution as part of larger system but also performing qualitative error analysis using integrated functionality annotation chunks and named entities as well as additional information such as part of speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper Starting out with chunking pipeline which uses classical combination of tagger and chunker with the tagger the chunker and the Named Entity Recognizer the desire to use richer syntactic representations led to the development of parsing pipeline which uses and reranking parser to assign tags and uses base as chunk equivalents while also providing syntactic trees that can be used by feature extractors also supports using the parser yielding an easy to use only solution
335,A,P05-1045,P09-1113,1,"(Finkel et al., 2005)","Thus each syntactic row in Table 3 represents a single syntactic feature. 5.3 Named entity tag features Every feature contains, in addition to the content described above, named entity tags for the two entities. We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005). The tagger provides each word with a label from {person, location, organization, miscellaneous, none}. 5.4 Feature conjunction Rather than use each of the above features in the classifier independently, we use only conjunctive features.",Thus each syntactic row in Table represents single syntactic feature Named entity tag features Every feature contains in addition to the content described above named entity tags for the entities perform named entity tagging using the class named entity tagger The tagger provides each word with label from person location organization miscellaneous none Feature conjunction Rather than use each of the above features in the classifier independently we use only conjunctive features
336,A,P05-1045,P09-2041,1,"(Finkel et al., 2005)","Our expectation is that true news stories will have been reported in various forums, and hence the number of web documents which include the same combination of entities will be higher than with satire documents. To implement this method, we first use the Stanford Named Entity Recognizer4 (Finkel et al., 2005) to identify the set of person and organisation entities, E, from each article in the corpus. From this, we estimate the validity of the combination of entities in the article as: where g is the set of matching documents returned by Google using a conjunctive query.",Our expectation is that true news stories will have been reported in various forums and hence the number of web documents which include the same combination of entities will be higher than with satire documents implement this method we first use the Stanford Named Entity to identify the set of person and organisation entities from each article in the corpus From this we estimate the validity of the combination of entities in the article as where is the set of matching documents returned by using conjunctive query
337,A,P05-1045,P10-1015,1,"(Finkel et al., 2005)","Holmes) from the text. We processed each novel with the Stanford NER tagger (Finkel et al., 2005) and extracted noun phrases that were categorized as persons or organizations. We then clustered the noun phrases into coreferents for the same entity (person or organization).",Holmes from the text processed each novel with the tagger and extracted noun phrases that were categorized as persons or organizations then clustered the noun phrases into coreferents for the same entity
338,A,P05-1045,P10-1056,1,"(Finkel et al., 2005)","In addition, first mentions of entities in text introduce the entity into the discourse and so must be informative and properly descriptive (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, andLOCATIONs. First mentions to peopleFeature exploration on our development set found that under-specified references to people are much more disruptive to a summary than short references to organizations or locations.",addition first mentions of entities in text introduce the entity into the discourse and so must be informative and properly descriptive run the Stanford Named Entity Recognizer and record the number of mentions to peopleFeature exploration on our development set found that under specified references to people are much more disruptive to summary than short references to organizations or locations
339,A,P05-1045,P10-1148,1,"(Finkel et al., 2005)","That is, a non-animate noun can hardly constitute an experience. In order to make a distinction, we use the dependency parser and a named-entity recognizer (Finkel et al., 2005) that can recognize person pronouns and person names. 3.2 Classification To train our classifier, we first crawled weblogs from Wordpress3, one of the most popular blog sites in use today. Worpress provides an interface to search blog posts with queries.",That is non animate noun can hardly constitute an experience order to make distinction we use the dependency parser and named entity recognizer that can recognize person pronouns and person names Classification train our classifier we first crawled weblogs from one of the most popular blog sites in use provides an interface to search blog posts with queries
340,A,P05-1045,P10-2049,1,"(Finkel et al., 2005)","We identified a few typical sources of errors in a preliminary error analysis. We therefore suggest three extensions to the algorithm which are on the one hand possible in the OM setting and on the other hand represent special features of the target discourse type: [1.] We observed that the Stanford Named Entity Recognizer (Finkel et al., 2005) is superior to the Person detection of the (MUC6 trained) CogNIAC implementation. We therefore filter out Person antecedent candidates which the Stanford NER detects for the impersonal and demonstrative pronouns and Location & Organization candidates for the personal pronouns.",identified few typical sources of errors in preliminary error analysis therefore suggest extensions to the algorithm which are on the one hand possible in the setting and on the other hand represent special features of the target discourse type observed that the Named Entity Recognizer is superior to the Person detection of the implementation therefore filter out Person antecedent candidates which the detects for the impersonal and demonstrative pronouns and Location Organization candidates for the personal pronouns
341,A,P05-1045,P11-1037,1,"(Finkel et al., 2005)","Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new genre of text, which are short, informal, ungrammatical and noise prone. For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary, which requires a lot of annotated tweets or rules.",Because of the domain mismatch current systems trained on non tweets perform poorly on tweets new genre of text which are short informal ungrammatical and noise prone For example the average of the which is trained on the shared task data set and achieves state of the art performance on that task drops from to on tweets Thus building domain specific for tweets is necessary which requires lot of annotated tweets or rules
342,A,P05-1045,P11-1082,0,"(Finkel et al., 2005)","On the other hand, if the CR model is used, the YAGO feature for an instance involvingNPk and preceding clusterc will have the value 1 if and only if NPk has a TYPE or MEANS relation with any of the NPs inc. Since knowledge extraction from webbased encyclopedia is typically noisy (Ponzetto and Poesio, 2009), we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer (Finkel et al., 2005) and the other NP is a common noun. FrameNet is a lexico-semantic resource focused on semantic frames (Baker et al., 1998).",the other hand if the model is used the feature for an instance involvingNPk and preceding will have the value if and only if has or relation with any of the inc Since knowledge extraction from webbased is typically noisy we use to determine whether have relation only if is named entity of type person organization or location according to the recognizer and the other is common noun is lexico semantic resource focused on semantic frames
343,A,P05-1045,P11-1113,0,Finkel et al. (2005),"They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.",They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction used sampling simple Monte Carlo method used to perform approximate inference in factored probabilistic models using simulated annealing in place of decoding in sequence models such as and it is possible to incorporate non local structure while preserving tractable inference
344,A,P05-1045,P11-1114,1,"(Finkel et al., 2005)","These patterns are similar to dependency relations in that they typically represent the syntactic role of the NP with respect to other constituents (e.g., subject-of, object-of, and noun arguments). Semantic features:we use the Stanford NER tagger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign semantic class labels to each NP’s head noun. One of our goals was to explore the use ofdocument genre to permit more aggressive strategies for extracting role fillers.",These patterns are similar to dependency relations in that they typically represent the syntactic role of the with respect to other constituents features we use the tagger to determine if the targeted is named entity and we use the parser to assign semantic class labels to each head noun of our goals was to explore the use ofdocument genre to permit more aggressive strategies for extracting role fillers
345,A,P05-1045,P11-3004,1,"(Finkel et al., 2005)","We represent both contexts as vectors of URIs. To create d we extract all NEs from the text using the Stanford NE Recognizer (Finkel et al., 2005) and represent each NE by its Wikipedia URI. If a surface form is ambiguous, we choose the most popular NE with the popularity metric described below.",represent both contexts as vectors of create we extract all from the text using the Stanford Recognizer and represent each by its Wikipedia surface form is ambiguous we choose the most popular with the popularity metric described below
346,A,P05-1045,P12-1042,1,"(Finkel et al., 2005)","In addition to this shallow parsing method, we also use named entity recognition (NER) to identify more entities. We use the Stanford Named Entity Recognizer (Finkel et al., 2005) for this purpose. It recognizes three types of entities: person, location, and organization.",addition to this shallow parsing method we also use named entity recognition to identify more entities use the Stanford Named Entity Recognizer for this purpose recognizes types of entities person location and organization
347,A,P05-1045,P12-1072,1,"(Finkel et al., 2005)","Association, National Football League, and Major League Baseball) in 2009. Due to the large size of the corpora, we uniformly sampled a subset of documents for each corpus and ran the Stanford NER tagger (Finkel et al., 2005), which tagged named entities mentions as person, location, and organization. We used named entity of type person from the political blogs corpus, while we are interested in person and organization entities for the sports news corpus.",Association National Football League and Major League Baseball in Due to the large size of the corpora we uniformly sampled subset of documents for each corpus and ran the tagger which tagged named entities mentions as person location and organization used named entity of type person from the political blogs corpus while we are interested in person and organization entities for the sports news corpus
348,A,P05-1045,P12-1073,0,"(Finkel et al., 2005)","In case of multiple possible labels, the most frequent one is denoted by * in the Figure. The Figure also shows the results of the Stanford NER tagger for English (Finkel et al., 2005) (we used the MUC-7 classifier). Table 2 reports the performance of the local (L Wiki-tagger), local+global (LG Wiki tagger) and the Stanford tagger.",case of multiple possible labels the most frequent one is denoted by in the Figure The Figure also shows the results of the tagger for Table reports the performance of the local and the tagger
349,A,P05-1045,P12-1075,1,"(Finkel et al., 2005)","We carry out experiments on New York Times articles from years 2000 to 2007 (Sandhaus, 2008). Following (Yao et al., 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al., 2005) and dependency parsing (Nivre et al., 2004). We extract dependency paths for each pair of named entities in one sentence.",carry out experiments on New York Times articles from to Following we filter out noisy documents and use natural language packages to annotate the documents including tagging and dependency parsing extract dependency paths for each pair of named entities in sentence
350,A,P05-1045,P12-1087,1,"(Finkel et al., 2005)","Our feature extraction process consists of three steps: 1. Run Stanford CoreNLP with POS tagging and named entity recognition (Finkel et al., 2005); 2. Run dependency parsing on TAC with the Ensemble parser (Surdeanu and Manning, 2010) and on ClueWeb with MaltParser (Nivre et al., 2007)8; and 3.",Our feature extraction process consists of steps Stanford with tagging and named entity recognition dependency parsing on with the parser and on with and
351,A,P05-1045,P12-1089,2,"(Sutton and McCallum, 2004; Finkel et al., 2005)","Interestingly, several researchers have attempted to model label consistency and high-level relational constraints using state-of-the-art sequential models of named entity recognition (NER). Mainly, predetermined word-level dependencies were represented as links in the underlying graphical model (Sutton and McCallum, 2004; Finkel et al., 2005). Finkel et al. (2005) further modelled high-level semantic constraints; for example, using the CMU seminar announcements dataset, spans labeled as start timeor end timewere required to be semantically consistent.",Interestingly several researchers have attempted to model label consistency and high level relational constraints using state of the art sequential models of named entity recognition Mainly predetermined word level dependencies were represented as links in the underlying graphical model further modelled high level semantic constraints for example using the seminar announcements dataset spans labeled as start timeor end timewere required to be semantically consistent
352,A,P05-1045,P12-2013,1,"(Finkel et al., 2005)","Sentiment toward an entity: We again adopt a simplifying view by modeling all the named entities in a sentence without heeding the roles these entities play, i.e. whether they are targets or not. Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005). We only focus on Person and Organization named entities.",Sentiment toward an entity again adopt simplifying view by modeling all the named entities in sentence without heeding the roles these entities play whether they are targets or not Accordingly we extract all the named entities in sentence using Stanford Name Entity Recognizer only focus on Person and Organization named entities
353,A,P05-1045,P12-2024,1,"(Faruqui and Padó, 2010; Finkel et al., 2005)","After tokenization, Giza++ and Moses were respectively used to align the corpora and extract a lexical phrase table (PT). Similarly, the semantic phrase table (SPT) has been ex2Recently, a new dataset including “Unknown” pairs has been used in the “Cross-Lingual Textual Entailment for Content Synchronization” task at SemEval-2012 (Negri et al., 2012). tracted from the same corpora annotated with the Stanford NE tagger (Faruqui and Padó, 2010; Finkel et al., 2005). Dependency relations (DR) have been extracted running the Stanford parser (Rafferty and Manning, 2008; De Marneffe et al., 2006).",After tokenization and Moses were respectively used to align the corpora and extract lexical phrase table Similarly the semantic phrase table has been new dataset including Unknown pairs has been used in the Cross Lingual Textual Entailment for Content Synchronization task at SemEval tracted from the same corpora annotated with the tagger Dependency relations have been extracted running the parser
354,A,P05-1045,P12-2064,0,"(Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005)","We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract the features. evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances.",adopted the feature set investigated in for article error correction use the to extract the features evaluated in terms of accuracy precision recall and score is the number of correct predictions divided by the total number of instances
355,A,P05-1045,P12-2069,1,"(Finkel et al., 2005)","Before tackling these, we perform some preprocessing on the cluster of documents. It includes: cleaning up and normalization of the input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE (Cunningham et al., 2002), syntactical parsing and dependency parsing (collapsed) using the Stanford Parser (de Marneffe et al., 2006), and Named Entity Recognition using Stanford NER (Finkel et al., 2005). We have also developed a date resolution engine that focuses on days of the week and relative terms.",Before tackling these we perform some preprocessing on the cluster of documents includes cleaning up and normalization of the input using regular expressions sentence segmentation tokenization and lemmatization using syntactical parsing and dependency parsing using the and Named Entity Recognition using have also developed date resolution engine that focuses on days of the week and relative terms
356,A,P05-1045,P12-3003,0,"(Finkel et al., 2005)","The implementation of QuickView requires adapting existing NLP components trained on formal texts, which often performs poorly on tweets. For example, the average F1 of the Stanford NER (Finkel et al., 2005) drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets, while Liu et al. (2010) report that the F1 score of a state-ofthe-art SRL system (Meza-Ruiz and Riedel, 2009) falls to 42.5% on tweets as apposed to 75.5% on news. However, the adaptation of those components is challenging, owing to the lack of annotated tweets and the inadequate signals provided by a noisy and short tweet.",The implementation of requires adapting existing components trained on formal texts which often performs poorly on tweets For example the average of the drops from to on tweets while report that the score of state ofthe art system falls to on tweets as apposed to on news However the adaptation of those components is challenging owing to the lack of annotated tweets and the inadequate signals provided by noisy and short tweet
357,A,P05-1045,P13-1106,1,"(Finkel et al., 2005)","We only use the dev set for model development. The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al., 2005). It also serves as a stateof-the-art monolingual baseline for both English and Chinese.",only use the dev set for model development The based tagger was used as the monolingual component in our models also serves as stateof the art monolingual baseline for both and
358,A,P05-1045,P13-1129,0,"(Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006)","Other preprocessing steps that are required for processing a new novel include standarizing the typographical conventions, and performing POS tagging, NER tagging, and dependency parsing. We utilize the Stanford tools (Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006). In this section, we describe experiments conducted to evaluate our speaker identification approach.",Other preprocessing steps that are required for processing new novel include standarizing the typographical conventions and performing tagging tagging and dependency parsing utilize the tools this section we describe experiments conducted to evaluate our speaker identification approach
359,A,P05-1045,P13-1146,0,(Finkel 2005),"Tagging mentions of named entities with lexical types has been pursued in previous work. Most well-known is the Stanford named entity recognition (NER) tagger (Finkel 2005) which assigns coarse-grained types like person, organization, location, and other to noun phrases that are likely to denote entities. There is fairly little work on finegrained typing, notable results being (Fleischmann 2002; Rahman 2010; Ling 2012; Yosef 2012).",Tagging mentions of named entities with lexical types has been pursued in previous work Most well known is the named entity recognition tagger which assigns coarse grained types like person organization location and other to noun phrases that are likely to denote entities There is fairly little work on finegrained typing notable results being
360,A,P05-1045,P13-1166,0,"(Finkel et al., 2005)","The complex features based on VIAF,7 GeoNames8 and WordNet do contribute to the classification in the Mallet setup as removing them and only using the focus token, window and generic features causes a slight drop in overall F-score from 49.45 to 47.25. When training the Stanford NER system (Finkel et al., 2005) on just the tokens from the Freire data set and the parameters from english.all.3class.distsim.prop (included in the Stanford NER release, see also Van Erp and Van der Meij (2013)), our F-scores come very close to those reported by Freire et al. (2012), but mostly with a higher recall and lower precision. It is puzzling that the Stanford system obtains such high results with only very simple features, whereas for Mallet the complex features show improvement over simpler features.",The complex features based on and do contribute to the classification in the setup as removing them and only using the focus token window and generic features causes slight drop in overall score from to When training the system on just the tokens from the data set and the parameters from our scores come very close to those reported by but mostly with higher recall and lower precision is puzzling that the system obtains such high results with only very simple features whereas for the complex features show improvement over simpler features
361,A,P05-1045,P13-2036,1,"(Finkel et al., 2005)","We consider entities occurring more than η times as nodes and entity pairs co-occurring more than σ times as edges. To identify entities, we use a CRF-based named entity tagger (Finkel et al., 2005) and a Chinese word breaker (Gao et al., 2003) for English and Chinese corpora, respectively. Given two graphs Ge = (Ve, Ee) and Gc = (Vc, Ec), we initialize |Ve|-by-|Vc| initial similarity matrix R0 using PH and CX for every pair (e, c) where e ∈ Ve and c ∈ Vc.",consider entities occurring more than times as nodes and entity pairs co occurring more than times as edges identify entities we use based named entity tagger and word breaker for and corpora respectively Given graphs and we initialize by initial similarity matrix using and for every pair where and
362,A,P05-1045,P13-2116,0,"(Finkel et al., 2005)","With p = 0.01, Joint has statistically significant improvement of F1 score over all three other approaches with each probability threshold. We validate the hypothesis that using linguistic features, e.g., part-of-speech tags (Toutanova and Manning, 2000), named-entity tags (Finkel et al., 2005), and dependency trees (de Marneffe et al., 2006), helps improve the quality of our approach, called Joint. There are different ways to use shallow and linguistic features; we select Type	   Features Shallow	   Regular	  Expressions	  (Dalvi	  et	  al.,	  2012) Term	  proximity	  (Matsuo	  et	  al.,	  2003) DicConary	  and	  Freebase	  (Mintz	  et	  al.,	  2009) We created the following variants of Joint.",With Joint has statistically significant improvement of score over all other approaches with each probability threshold validate the hypothesis that using linguistic features part of speech tags named entity tags and dependency trees helps improve the quality of our approach called Joint There are different ways to use shallow and linguistic features we select Type Features Shallow Regular Expressions Term proximity and created the following variants of Joint
363,A,P05-1045,P13-2128,0,"(Finkel et al., 2005)","In lexical semantics, smoothing is often achieved by backing off from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves.",lexical semantics smoothing is often achieved by backing off from words to semantic classes either adopted from resource such as or induced from data Similarly distributional features support generalization in Named Entity Recognition Although distributional information is often used for smoothing to our knowledge there is little work on smoothing distributional models themselves
364,A,P05-1045,P13-2141,1,"(Finkel et al., 2005)","We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component. We used Open NLP POS tagger, Standford NER (Finkel et al., 2005) and MaltParser (Nivre et al., 2006) to label/tag sentences. We employed liblinear (Fan et al., 2008) as classifiers for NED and relation extraction and the solver is L2LR. 4.2 Performance of Relation Extraction Held-out Evaluation.",used Wikipedia articles for generating training data for the component used Open tagger and to sentences employed liblinear as classifiers for and relation extraction and the solver is Performance of Relation Extraction Held out Evaluation
365,A,P05-1045,P13-4023,0,"(Finkel et al., 2005)","In order to perform “live” entity type classification based on ad-hoc text inputs, several performance optimizations have been undertaken to operate under real-time conditions. State-of-the-art tools for named entity recognition such as the Stanford NER Tagger (Finkel et al., 2005) compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). However, we are not aware of any online tool that performs fine-grained typing of entity mentions.",order to perform live entity type classification based on ad hoc text inputs several performance optimizations have been undertaken to operate under real time conditions State of the art tools for named entity recognition such as the compute semantic tags only for small set of coarse grained types Person Location and Organization However we are not aware of any online tool that performs fine grained typing of entity mentions
366,A,P05-1045,Q13-1028,1,"(Finkel et al., 2005)","We approximate this facet by computing the number of explicit references to people, relying on three sources of information about animacy of words. The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005). We also created a list of personal pronouns such as ‘he’, ‘myself’ etc. which standardly indicate animate entities (animate pronouns).",approximate this facet by computing the number of explicit references to people relying on sources of information about animacy of words The is named entity tags returned by the recognition tool also created list of personal pronouns such as he myself etc which standardly indicate animate entities
367,A,P05-1045,S10-1020,1,"(Finkel et al., 2005)","Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Manning, 2003) and NE-tagging (Finkel et al., 2005), Wordnet for semantic classes and the U. S. census data for assigning gender values to person names.",Corry relies on rich linguistically motivated feature set which has however been manually reduced to features for efficiency reasons Corry has only participated in the open setting as it has already number of preprocessing modules integrated into the system the toolkit for parsing and tagging Wordnet for semantic classes and the census data for assigning gender values to person names
368,A,P05-1045,S12-1035,1,"(Finkel et al., 2005)","We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charsrlconll-1.1.tgz 7The original focus annotation targeted the 3,993 negations Train Devel Test 1 role 2,210 515 672 2 roles 89 15 38 3 roles 3 0 2 All 2,302 530 712 • Semantic roles using the labeler described by (Punyakanok et al., 2008); and • Verbal negation, indicates with ‘N’ if that token correspond to a verbal negation for which focus must be predicted. Figure 2 provides a sample of PB-FOC.",gathered all negations from sections and and discarded negations for which the focus or PropBank annotations were not sound leaving For each verbal negation provides the current sentence and the previous and next sentences as context For each sentence along with the gold focus annotations contains the following additional annotations number tags using the Brill tagger Named Entities using the named entity recognizer recognizer Chunks using the chunker by tree using the parser original focus annotation targeted the negations Train Devel Test role roles roles All Semantic roles using the labeler described by and negation indicates with if that token correspond to verbal negation for which focus must be predicted Figure provides sample of
369,A,P05-1045,S12-1082,1,"(Finkel et al., 2005; Toutanova et al., 2003)","In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 =∞. Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. The exact type of pre-processing used depends on the metric used.",this work we use models with Initially all sentences are pre processed by the suite of tools process that includes named entity recognition normalization part of speech tagging lemmatization and stemming The exact type of pre processing used depends on the metric used
370,A,P05-1045,S12-1085,1,"(Finkel et al., 2005)","This tokenization strategy gives us the best results among all our three runs. For capturing and normalizing the above mentioned expressions, we make use of the Stanford NER Toolkit (Finkel et al., 2005). Some normalized samples are mentioned in figure 2.",This tokenization strategy gives us the best results among all our runs For capturing and normalizing the above mentioned expressions we make use of the Toolkit Some normalized samples are mentioned in figure
371,A,P05-1045,S13-1010,1,"(Finkel et al., 2005)","Wikipedia pages for nouns with senses (according to the disambiguation page) in a set of predefined categories were identified to form the list Wikipedia. Named entity recognition The Stanford Named Entity Recogniser (Finkel et al., 2005) was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are extracted to form the list Stanford. WordNet The final heuristic makes use of WordNet (Fellbaum, 1998) which lists nouns that are often used as proper nouns with capitalisation.",Wikipedia pages for nouns with senses in set of predefined categories were identified to form the list Wikipedia Named entity recognition The Stanford Named Entity Recogniser was run over the and any nouns that occur in the corpus with both named entity and non named entity tags are extracted to form the list Stanford The final heuristic makes use of which lists nouns that are often used as proper nouns with capitalisation
372,A,P05-1045,S13-1018,0,"(Finkel et al., 2005; Toutanova et al., 2003)","They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.",They can not be redistributed so we used the urls and scripts provided by the organizers to extract the corresponding metadata analysed the text in the metadata performing lemmatization tagging named entity recognition and classification and date detection using preliminary score for each similarity type was then calculated as follows General cosine similarity of
373,A,P05-1045,S13-1023,1,"(Finkel et al., 2005)","In our experiments, k was set to 0.1, the default value in the original model. In addition to the above text similarity measures, we used also the following common measures: (wq1 , . . . , wqn) the vectors of tf.idf weights associated to sentences p and q, the cosine distance is calculated as: where Lev(p, q) is the Levenshtein distance between the two sentences, taking into account the characters. 2.7.3 Named Entity Overlap We used the Stanford Named Entity Recognizer by (Finkel et al., 2005), with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date. Then we calculated a per-class overlap measure (in this way, “France” as an Organization does not match “France” as a Location): where Np and Nq are the sets of NEs found, respectively, in sentences p and q.",our experiments was set to the default value in the original model addition to the above text similarity measures we used also the following common measures the vectors of weights associated to sentences and the cosine distance is calculated as where is the distance between the sentences taking into account the characters Named Entity Overlap used the Stanford Named Entity Recognizer by with the class model trained for Time Location Organization Person Money Percent Date Then we calculated per class overlap measure where and are the sets of found respectively in sentences and
374,A,P05-1045,W06-1643,0,"(Finkel et al., 2005)","In the case of BNs, we write: We can reduce a particular skip-chain CRF to represent only the set of cliques along (yt−1, yt) adjacency edges and (yst , yt) skip edges, resulting in only two potential functions: Our CRF and BN models were designed using MALLET (McCallum, 2002), which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training dataD = (x(i),y(i)) i=1, and provides probabilistic inference algorithms for linear-chain BNs and CRFs. Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al., 2005). Approximation is needed when the junction tree of a graphical model is associated with prohibitively large cliques.",the case of we write can reduce particular skip chain to represent only the set of cliques along adjacency edges and skip edges resulting in only two potential functions Our and models were designed using which provides tools for training log linear models with optimization techniques and maximize the loglikelihood of our training and provides probabilistic inference algorithms for linear chain and Most previous work with containing nonlocal dependencies used approximate probabilistic inference techniques including and Gibbs sampling Approximation is needed when the junction tree of graphical model is associated with prohibitively large cliques
375,A,P05-1045,W06-1655,0,"(Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003)","We have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds. There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003). The current work indicates that these systems might be improved by moving to a semi-CRF model.",have presented the intuitive argument that the log odds may be advantageous because it does not exhibit the asymmetry of the log probability but it would be satisfying to justify the choice on more theoretical grounds There is significant volume of work exploring the use of for variety of chunking tasks including named entity recognition gene prediction shallow parsing and others The current work indicates that these systems might be improved by moving to semi model
376,A,P05-1045,W09-0422,1,"(Finkel et al., 2005)","Unlike in PDT 2.0, the information about the original syntactic form is stored with each tnode (values such asv:inf for an infinitive verb form, v:since+fin for the head of a subordinate clause of a certain type,adj:attr for an adjective in attribute position,n:for+X for a given prepositional group are distinguished). 6We used the full development set of 2k sentences for “Moses T” and a subset of 1k sentences for the other two setups due to time constraints. One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005). The nodes in the English t-layer are grouped according to the detected named entities and they are assigned the type of entity (location, person, or organization).",Unlike in the information about the original syntactic form is stored with each tnode used the full development set of sentences for Moses and subset of sentences for the other setups due to time constraints of the steps in the analysis of is named entity recognition using Stanford Named Entity Recognizer The nodes in the layer are grouped according to the detected named entities and they are assigned the type of entity
377,A,P05-1045,W09-1119,1,"(Finkel et al., 2005)","We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features. The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005). Our goal was to compare the performance of the taggers across several datasets.",have downloaded the tagger and used the strongest provided model trained on the data with distributional similarity features The results we obtained on the test set were consistent with what was reported in Our goal was to compare the performance of the taggers across several datasets
378,A,P05-1045,W09-1218,0,"(Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007)","We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information.",explore different approach to deal with such information using global features Use of global features for structured prediction problem has been explored by several applications such as sequential labeling and dependency parsing with great deal of success attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information
379,A,P05-1045,W10-0725,1,"(Finkel et al., 2005)","We give Turkers one example as a guide along with the instructions. The texts we use in our experiments are the development set of the RTE-5 challenge (Bentivogli et al., 2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al., 2005). In all, it contains 600 T-H pairs, and we use the texts to generate facts and counter-facts and hypotheses as references.",give Turkers example as guide along with the instructions The texts we use in our experiments are the development set of the challenge and we preprocess the data using the named entity recognizer all it contains pairs and we use the texts to generate facts and counter facts and hypotheses as references
380,A,P05-1045,W10-1902,0,Finkel et al. (2005),"Huang et al. (2007) combines a linear-chain CRF and two SVM models to enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction. However, the CRF models used in these systems were all linear-chain CRFs.",combines linear chain and models to enhance the recall used Gibbs Sampling to add non local dependencies into linear chain model for information extraction However the models used in these systems were all linear chain
381,A,P05-1045,W10-2906,1,"(Finkel et al., 2005)","Neither of these data sets were annotated with named entities, so we manually annotated 200 sentences from each of them. We used the Stanford NER tagger (Finkel et al., 2005) with its default configuration as our full monolingual model for each language. We weakened both the English and German models by removing several non-lexical and word-shape features.",Neither of these data sets were annotated with named entities so we manually annotated sentences from each of them used the tagger with its default configuration as our full monolingual model for each language weakened both the and models by removing several non lexical and word shape features
382,A,P05-1045,W10-3102,1,"(Finkel et al., 2005)","In the cases where the two student annotators agreed on the annotation, that annotation was chosen for the final corpus. In the cases where they did not agree, an annotation made by the chief annotator was chosen. 2.3 The Stanford NER based on CRF The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Conditional Random Fields (Finkel et al., 2005) and has been used extensively for identifying named entities in news text. For example in the CoNLL-2003, where the topic was language-independent named entity recognition, Stanford NER CRF was used both on English and German news text for training and evaluation.",the cases where the student annotators agreed on the annotation that annotation was chosen for the final corpus the cases where they did not agree an annotation made by the chief annotator was chosen The based on The Named Entity Recognizer is based on the machine learning algorithm Conditional Random Fields and has been used extensively for identifying named entities in news text For example in the where the topic was language independent named entity recognition was used both on and news text for training and evaluation
383,A,P05-1045,W11-0219,1,"(Finkel et al., 2005)","To support  more robust processing as well as domain configurability, the core system is informed by a variety of statistical and symbolic preprocessors. These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger (Toutanova and Manning, 2000), the Stanford named-entity recognizer (NER) (Finkel et al., 2005) and the Stanford Parser (Klein and Manning, 2003). The output of these and other specialized preprocessors (such as a street address recognizer) are sent  to the parser as advice.",support more robust processing as well as domain configurability the core system is informed by variety of statistical and symbolic preprocessors These include several off the shelf statisical tools such as the tagger the named entity recognizer and the Parser The output of these and other specialized preprocessors are sent to the parser as advice
384,A,P05-1045,W11-0810,2,"(Finkel et al., 2005)","We compared three state-of-the-art NER taggers: one from Stanford University (henceforth, Stanford tagger), one from the University of Illinois (henceforth, the LBJ tagger) and BBN IdentiFinder (henceforth, IdentiFinder). The Stanford Tagger is based on Conditional Random Fields (Finkel et al., 2005). It was trained on 100 million words from the English Gigawords corpus.",compared state of the art taggers from University from the University of Illinois and IdentiFinder The is based on Conditional Random Fields was trained on words from the English Gigawords corpus
385,A,P05-1045,W11-0902,1,"(Finkel et al., 2005)","Note however that the domain customization discussion in Section 5 is independent of the system architecture or classifiers used for EMD and RMD, and we expect the proposed ideas to apply to other IE approaches as well. We performed all pre-processing (tokenization, part-of-speech (POS) tagging) with the Stanford CoreNLP toolkit.2 For EMD we used the Stanford named entity recognizer (Finkel et al., 2005). In all our experiments we used a generic set of features (“macro”) and the IO notation3 for entity mention labels (e.g., the labels for the tokens “over the Seattle Seahawks on Sunday” (from Figure 1) are encoded as “O O NFLTEAM NFLTEAM O DATE”). 3The IO notation facilitates faster inference than the IOB or IOB2 notations with minimal impact on performance, when there are fewer adjacent mentions with the same type. – Head words of the two arguments and their combination – Entity mention labels of the two arguments and their combination – Sequence of dependency labels in the dependency path linking the heads of the two arguments – Lemmas of all words in the dependency path – Syntactic path in the constituent parse tree between the largest constituents headed by the same words as the two arguments (similar to Gildea and Jurafsky (2002)) The RMD model was built from scratch as a multi-class classifier that extracts binary relations between entity mentions in the same sentence.",Note however that the domain customization discussion in is independent of the system architecture or classifiers used for and and we expect the proposed ideas to apply to other approaches as well performed all pre processing tokenization part of speech tagging with the For we used the named entity recognizer all our experiments we used generic set of features and the for entity mention labels the labels for the tokens over the Seattle Seahawks on are encoded as notation facilitates faster inference than the or notations with minimal impact on performance when there are fewer adjacent mentions with the same type Head words of the arguments and their combination Entity mention labels of the arguments and their combination Sequence of dependency labels in the dependency path linking the heads of the arguments of all words in the dependency path path in the constituent parse tree between the largest constituents headed by the same words as the arguments The model was built from scratch as multi class classifier that extracts binary relations between entity mentions in the same sentence
386,A,P05-1045,W11-1907,1,"(Finkel et al., 2005)","These annotations are created by a pipeline of preprocessing components. We use theStanford MaxentTagger (Toutanova et al., 2003) for partof-speech tagging, and theStanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities. In order to derive syntactic information, we use theCharniak/Johnson reranking parser (Charniak and Johnson, 2005) combined with a constituent-to-dependency conversion Tool ( treebank_converter ).",These annotations are created by pipeline of preprocessing components use theStanford MaxentTagger for partof speech tagging and theStanford Named Entity Recognizer for annotating named entities order to derive syntactic information we use reranking parser combined with constituent to dependency conversion Tool
387,A,P05-1045,W11-1908,1,"(Finkel et al., 2005)","First, we create a list ofcandidate mentionsby merging basic NP chunks with named entities. NP chunks are computed from the parse trees provided in the CoNLL distribution, Named entities are extracted with the Stanford NER tool (Finkel et al., 2005). For each candidate mention, we store it minimal and maximal span.",we create list ofcandidate mentionsby merging basic chunks with named entities chunks are computed from the parse trees provided in the distribution Named entities are extracted with the tool For each candidate mention we store it minimal and maximal span
388,A,P05-1045,W11-2202,1,"(Finkel et al., 2005)","Data Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people. We then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens (these were usually errors).",Data Evaluation was performed on corpus of blogs describing United States politics in ran the Stanford Named Entity Recognition system to obtain set of candidate mentions which the system judged to be names of people then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens
389,A,P05-1045,W11-2705,1,"(Finkel et al., 2005)","For question classification, we used Li and Roth (2002) taxonomy and a machine learning-based classifier fed with features derived from a rule-based classifier (Silva et al., 2011). For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford’s Conditional Random-Field-based named entity recognizer (Finkel et al., 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION. For the generation of questions we used the top 16 documents retrieved by the Google for 9 personalities from several domains, like literature (e.g., Jane Austen) and politics (e.g., Adolf Hitler).",For question classification we used taxonomy and machine learning based classifier fed with features derived from rule based classifier For the learning of patterns we used the top documents retrieved by and to recognize the named entities in the pattern we apply several strategies namely the Stanford Conditional Random Field based named entity recognizer to detect entities of type regular expressions to detect and type entities gazetteers to detect entities of type For the generation of questions we used the top documents retrieved by the for personalities from several domains like literature and politics
390,A,P05-1045,W12-0102,1,"(Finkel et al., 2005)","If more named entities cooccur in two documents, they are very likely to talk about the same event or subject and thus should be more comparable. We use Stanford named entity recognizer7 to extract named entities from the texts (Finkel et al., 2005). Again, cosine is then applied to measure the similarity of named entities (denoted by WN ) between a document pair.",more named entities cooccur in documents they are very likely to talk about the same event or subject and thus should be more comparable use named entity to extract named entities from the texts Again cosine is then applied to measure the similarity of named entities between document pair
391,A,P05-1045,W12-0103,1,"(Finkel et al., 2005)","In order to obtain the Level2 representation of these corpora, the documents and the test sets must be annotated. For the annotation pipeline we use the TnT POS tagger (Brants, 2000), WordNet (Fellbaum, 1998), the YamCha chunker (Kudo and Matsumoto, 2003), the Stanford NERC (Finkel et al., 2005), and an in-house temporal expressions recogniser. Table 2 shows some statistics for the parallel corpus and the two different levels of annotation.",order to obtain the representation of these corpora the documents and the test sets must be annotated For the annotation pipeline we use the tagger WordNet the chunker the and an in house temporal expressions recogniser Table shows some statistics for the parallel corpus and the different levels of annotation
392,A,P05-1045,W12-0604,1,"(Finkel et al., 2005)","For the multi-class experiment, we use the 350 blogs corresponding to the 7 categories. Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recognizer (Finkel et al., 2005), which labels the entities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous. After several tests, we found that Location, Organization, Person and Miscellaneous were the most useful for topic classification, and we thus ignored the rest for the results presented here.",For the multi class experiment we use the blogs corresponding to the categories Both the blogs and the articles were tagged using the Stanford Named Entity Recognizer which labels the entities according to these types Time Location Organization Person Money Percent Date and Miscellaneous After several tests we found that Location Organization Person and Miscellaneous were the most useful for topic classification and we thus ignored the rest for the results presented here
393,A,P05-1045,W12-2007,1,"(Finkel et al., 2005)","We use the pattern of predicate matches against the TextRunner database to assess the degree and the equivocality of the connection between NE and NP. We use the Stanford Named Entity Recognizer (Finkel et al., 2005) that tags named entities as people, locations, organizations, and miscellaneous. We annotated a sample of 90 essays for named entities; the sample yielded 442 tokens, which we classified as shown in Table 2.",use the pattern of predicate matches against the database to assess the degree and the equivocality of the connection between and use the Stanford Named Entity Recognizer that tags named entities as people locations organizations and miscellaneous annotated sample of essays for named entities the sample yielded tokens which we classified as shown in Table
394,A,P05-1045,W12-2408,1,"(Finkel et al., 2005)","NER has long been studied by the research community and many different approaches have been developed (Tjong Kim Sang and De Meulder, 2003; Doddington et al., 2004). One successful and freely available named entity recognizer is the Stanford NER system (Finkel et al., 2005), which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents. As already mentioned, we first selected and ran several existing de-identification and NER systems detecting person names in our clinical documents.",has long been studied by the research community and many different approaches have been developed successful and freely available named entity recognizer is the system which provides an implementation of linear chain sequence models coupled with well engineered feature extractors for and trained with newswire documents already mentioned we selected and ran several existing de identification and systems detecting person names in our clinical documents
395,A,P05-1045,W12-2903,1,"(Finkel et al., 2005)","Each dialog was processed using the Stanford Core NLP tools. The Stanford tools perform part of speech tagging (Toutanova et al., 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al., 2005), and coreference resolution (Lee et al., 2011). From the output of the Stanford tools, the following features were extracted for each utterance: word bigrams (pairs of adjacent words); dependency-head relations, along with the type of dependency relation (basically, governors — e.g., verbs — and their dependents — e.g., nouns); named entities (persons, organizations, etc.); and the whole utterance.",Each dialog was processed using the Core tools The tools perform part of speech tagging constituent and dependency parsing named entity recognition and coreference resolution From the output of the tools the following features were extracted for each utterance word bigrams dependency head relations along with the type of dependency relation named entities and the whole utterance
396,A,P05-1045,W12-3109,1,"(Finkel et al., 2005)","Another aspect that might pose a potential problem to MT is the occurrence of words that were only observed a few times or in very particular contexts, as it is often the case for Named Entities. We used the Stanford NER Tagger (Finkel et al., 2005) to detect words that belong to one of four groups: Person, Location, Organization and Misc. Each group is represented by a binary feature.",Another aspect that might pose potential problem to is the occurrence of words that were only observed few times or in very particular contexts as it is often the case for Named used the to detect words that belong to of groups Person Location Organization and Misc Each group is represented by binary feature
397,A,P05-1045,W13-0905,1,"(Finkel et al., 2005)","The selectional restrictions, however, are not linked to any lexicons so a mapping was constructed in order to allow for automated detection of preference violations. Our first experiment utilizes WN, VN, and the Stanford Parser (de Marneffe et al., 2006) and Named Entity Recognizer (Finkel et al., 2005). The Stanford Parser identifies the verbs, as well as their corresponding subjects and direct objects.",The selectional restrictions however are not linked to any lexicons so mapping was constructed in order to allow for automated detection of preference violations Our experiment utilizes and the and Named Entity Recognizer The identifies the verbs as well as their corresponding subjects and direct objects
398,A,P05-1045,W13-1101,0,"(e.g., Finkel et al., 2005)","In general, language models could be used for more context-sensitive spelling correction. Given the preponderance of terms on the web, using a named entity recognizer (e.g., Finkel et al., 2005) for preprocessing may also provide benefits. We would like to thank Giuseppe Attardi for his help in using DeSR; Can Liu, Shoshana Berleant, and the IU CL discussion group for discussion; and the three anonymous reviewers for their helpful comments.",general language models could be used for more context sensitive spelling correction Given the preponderance of terms on the web using named entity recognizer for preprocessing may also provide benefits would like to thank Attardi for his help in using Can Liu Shoshana Berleant and the discussion group for discussion and the anonymous reviewers for their helpful comments
399,A,P05-1045,W13-2234,1,"(Finkel et al., 2005)","Semantic. We extract further information indicating whether a named entity, as identified by the Stanford NE Recognizer (Finkel et al., 2005) begins at wi. These features are relevant as there 5Realization of the classes D and N as lexical items is straightforward.",Semantic extract further information indicating whether named entity as identified by the begins at wi These features are relevant as there of the classes and as lexical items is straightforward
400,A,P05-1045,W13-2414,0,"(Finkel et al., 2005)","We have evaluated two schemas with a limited number of the NE categories. In the first more common (Finkel et al., 2005) schema, all PNs are divided into four MUC categories, i.e. person, organization, location and other. In the other schema, assuming a separate phases for PN recognition and classification (Al-Rfou’ and Skiena, 2012), we mapped all the PN categories to a single category, namely NAM.",have evaluated schemas with limited number of the categories the more common schema all are divided into categories person organization location and other the other schema assuming separate phases for recognition and classification we mapped all the categories to single category namely
401,A,P05-1045,W13-3516,1,"(Finkel et al., 2005)","In addition to automatically predicting the arguments, we also trained the IMS system to tag PropBank frameset IDs. English BC 1671 80.17 77.20 78.66 BN 2180 88.95 85.69 87.29 MZ 1161 82.74 82.17 82.45 NW 4679 86.79 84.25 85.50 TC 362 74.09 61.60 67.27 WB 1133 77.72 68.05 72.56 Overall 11186 84.04 80.86 82.42 Chinese BC 667 72.49 58.47 64.73 BN 3158 82.17 71.50 76.46 NW 1453 86.11 76.39 80.96 MZ 1043 65.16 56.66 60.62 TC 200 48.00 60.00 53.33 WB 886 80.60 51.13 62.57 Overall 7407 78.20 66.45 71.85 4.4 Named Entities We retrained the Stanford named entity recognizer20 (Finkel et al., 2005) on the OntoNotes data. Table 6 shows the performance details for all the languages across all 18 name types broken down by genre.",addition to automatically predicting the arguments we also trained the system to tag PropBank frameset Overall Chinese Overall Named Entities retrained the named entity on the data Table shows the performance details for all the languages across all name types broken down by genre
402,A,P05-1045,W13-4008,1,"(Finkel et al., 2005)","For example, ‘Joker’,‘Clarke Kent’ are related to ‘Batman’ and ‘Darth Vader’, ‘Yoda’ to ‘Star Wars’. To extract the extended targets, we capture named entities (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer (Finkel et al., 2005) and sort them based on their page occurrence count. Out of top-k (k = 20) NEs, some can belong to both of the debate topics.",For example Joker Clarke Kent are related to and Darth Vader Yoda to Star Wars extract the extended targets we capture named entities from the Wikipedia page of the debate topic using the Stanford Named Entity Recognizer and sort them based on their page occurrence count Out of top some can belong to both of the debate topics
403,B,P05-1045,C10-1064,1,"(Finkel et al., 2005)","The annotation for English sentences is divided into two subtasks: entity mention recognition and relation detection. We utilized an offthe-shelf system, Stanford Named Entity Recognizer 4 (Finkel et al., 2005) for detecting entity mentions on the English sentences. The total number of English entities detected was 285,566.",The annotation for sentences is divided into subtasks entity mention recognition and relation detection utilized an offthe shelf system Stanford Named Entity Recognizer for detecting entity mentions on the sentences The total number of entities detected was
404,B,P05-1045,C10-1083,1,"(Finkel et al., 2005)","Here we employ a simple but representative technique to demonstrate the feasibility of discovering interesting new aspects for augmenting the ontology. We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer (Finkel et al., 2005). After that, we rank the phrases by pointwise Mutual Information (MI): where T is the given topic and ph refers to a candidate entity phrase.",Here we employ simple but representative technique to demonstrate the feasibility of discovering interesting new aspects for augmenting the ontology first extract named entities from scattered opinions using Stanford Named Entity Recognizer After that we rank the phrases by pointwise Mutual Information where is the given topic and ph refers to candidate entity phrase
405,B,P05-1045,C10-1087,1,"(Finkel et al., 2005)","This section describes this system’s architecture; the methods by which it was augmented to address discourse are presented in Section 5. Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution. For the latter purpose, we use OpenNLP and enable the substitution of coreferring terms.",This section describes this system architecture the methods by which it was augmented to address discourse are presented in Section Preprocessing First all documents are parsed and processed with standard tools for named entity recognition and coreference resolution For the latter purpose we use and enable the substitution of coreferring terms
406,B,P05-1045,C10-1091,1,(Finkel et al. 2005),"In the last example, to measure the sentiment of PP, we apply rule for the verb ‘end’ from the “Termination of activity” class, which reverses the non-neutral polarity of subject (in intransitive use of verb) or object (in transitive use of verb). For example, the polarity of both sentences ‘My whole enthusiasm and excitement disappear like a bubble touching a hot needle’ and ‘They discontinued helping children’ is negative. 5 Decision on Attitude Label The decision on the most appropriate final label for the clause, in case @AM annotates it using different attitude types according to the words with multiple annotations (e.g., see word ‘unfriendly’ in Table 1) or based on the availability of the words conveying different attitude types, is made based on the analysis of: 1) morphological tags of nominal heads and their premodifiers in the clause (e.g., first person pronoun, third person pronoun, demonstrative pronoun, nominative or genitive noun, etc.); 2) the sequence of hypernymic semantic relations of a particular noun in WordNet (Miller, 1990), which allows to determine its conceptual domain (e.g., “person, human being”, “artifact”, “event”, etc.); 3) the annotations from the Stanford Named Entity Recognizer (Finkel et al. 2005) that labels PERSON, ORGANIZATION, and LOCATION entities. For ex., ‘I feel highly unfriendly attitude towards me’ conveys emotion (‘NEG aff’: ‘sadness’), while ‘The shop assistant’s behavior was really unfriendly’ and ‘Plastic bags are environment unfriendly’ express judgment (‘NEG jud’) and appreciation (‘NEG app’), correspondingly. 6 Evaluation For the experiments, we used our own data set, as, to the best of our knowledge, there is no publicly available data set of sentences annotated by the fine-grained labels proposed in our work.",the last example to measure the sentiment of we apply rule for the verb end from the Termination of activity class which reverses the non neutral polarity of subject or object For example the polarity of both sentences whole enthusiasm and excitement disappear like bubble touching hot needle and They discontinued helping children is negative Decision on Attitude Label The decision on the most appropriate final label for the clause in case annotates it using different attitude types according to the words with multiple annotations or based on the availability of the words conveying different attitude types is made based on the analysis of morphological tags of nominal heads and their premodifiers in the clause the sequence of hypernymic semantic relations of particular noun in which allows to determine its conceptual domain the annotations from the Stanford Named Entity Recognizer that labels and entities For feel highly unfriendly attitude towards me conveys emotion while The shop assistant behavior was really unfriendly and Plastic bags are environment unfriendly express judgment and appreciation correspondingly Evaluation For the experiments we used our own data set as to the best of our knowledge there is no publicly available data set of sentences annotated by the fine grained labels proposed in our work
407,B,P05-1045,C10-1105,1,"(Finkel et al., 2005)","Semantic (5): We employ five semantic features. First, ifNPi is an NE, we create a feature whose value is the NE label ofNPi, as determined by the Stanford CRF-based NE recognizer (Finkel et al., 2005). However, ifNPi is a nominal, we create a feature that encodes the WordNet semantic class of which it is a hyponym, using the manually determined sense ofNPi. 4 Moreover, to improve generalization, we employ a feature whose value is the WordNet synset number of the head noun of a nominal.",Semantic employ semantic features ifNPi is an we create feature whose value is the label ofNPi as determined by the based recognizer However ifNPi is nominal we create feature that encodes the semantic class of which it is hyponym using the manually determined sense ofNPi Moreover to improve generalization we employ feature whose value is the synset number of the head noun of nominal
408,B,P05-1045,C10-1131,1,"(Finkel et al., 2005)","The parser was trained on the entire Penn TreeBank. The last step in the pipeline is named-entity tagging using Stanford NER Tagger (Finkel et al., 2005). Given an input text sentence and a hypothesis sentence, the task of RTE is to make predictions about whether or not the hypothesis can be entailed from the text sentence.",The parser was trained on the entire Penn TreeBank The last step in the pipeline is named entity tagging using Given an input text sentence and hypothesis sentence the task of is to make predictions about whether or not the hypothesis can be entailed from the text sentence
409,B,P05-1045,C10-2058,0,"(Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007)","For example, if two GPE entities are involved in a “conflict-attack” event, then they are unlikely to be connected by a “part-whole” relation; “Mahmoud Abbas” and “Abu Mazen” are likely to be coreferential if they get involved in the same “life-born” event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007)  can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization.",For example if entities are involved in conflict attack event then they are unlikely to be connected by part whole relation Mahmoud Abbas and Abu Mazen are likely to be coreferential if they get involved in the same life born event Some prior work demonstrated the effectiveness of using semantic relations to improve entity coreference resolution while experimented with information fusion of relations across multiple documents The system can collapse and compress redundant facts extracted from multiple documents based on coreference resolution semantic similarity computation and normalization
410,B,P05-1045,C10-2078,1,"(Finkel et al., 2005)","While the instances were extracted from a news corpus, none of them are domain-specific and all expressions also occur in the BNC, which is a balanced, multi-domain corpus. To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger (Ratnaparkhi, 1996), parsed with the MaltParser2, and named entity tagged with the Stanford NE tagger (Finkel et al., 2005). The lemmatization was done by RASP (Briscoe and Carroll, 2006).",While the instances were extracted from news corpus none of them are domain specific and all expressions also occur in the which is balanced multi domain corpus compute the features which we extract in the next section all instances in our data sets were part of speech tagged by the tagger parsed with the and named entity tagged with the tagger The lemmatization was done by
411,B,P05-1045,C10-3011,1,"(Finkel et al., 2005)","This module processes the word tokenized file and creates an indexed entry for every word in the Antelogue repository. Named Entity Recognizer tagging (NER): We integrated Stanford’s NER tagger (Finkel et al., 2005). NER processor.",This module processes the word tokenized file and creates an indexed entry for every word in the Antelogue repository Named Entity Recognizer tagging integrated tagger processor
412,B,P05-1045,D07-1033,3,"(Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Krishnan and Manning, 2006)","Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not have different entity classes” were shown to be useful in named entity recognition (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Krishnan and Manning, 2006). We propose a new perceptron algorithm in this paper that can use non-local features along with local features.",Although this limitation makes training and inference tractable it also excludes the use of possibly useful non local features that are accessible after all labels are determined For example non local features such as same phrases in document do not have different entity classes were shown to be useful in named entity recognition propose new algorithm in this paper that can use non local features along with local features
413,B,P05-1045,D08-1035,0,"(e.g., Finkel 2005; Goldwater 2007)","The final segmentation is obtained by annealing the last 25,000 iterations to a temperature of zero. The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007). The total running time of our system is on the order of three minutes per document.",The final segmentation is obtained by annealing the last iterations to temperature of The use of annealing to obtain maximum posteriori configuration from sampling based inference is common The total running time of our system is on the order of three minutes per document
414,B,P05-1045,D09-1101,1,"(Finkel et al., 2005)","Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger (Toutanova et al., 2003). Semantic (1): The named entity (NE) tag ofwi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005). Gazetteers (8): Eight dictionaries containing pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms ofPERSON(6.3k).",Grammatical The part of speech tag of wi obtained using the log linear tagger Semantic The named entity tag ofwi obtained using the based recognizer Gazetteers dictionaries containing pronouns common words and words that are not names person names person titles and honorifics vehicle words location names company names and nouns extracted from that are hyponyms
415,B,P05-1045,D09-1119,0,"(Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009)","In this work, we address the same question of determining the impact of lexical features on a different family of tasks: sequence labeling, as illustrated by named entity recognition and chunking. As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). Sequence labeling includes both a structural aspect (bracketing the chunks) and a tagging aspect (classifying the chunks).",this work we address the same question of determining the impact of lexical features on different family of tasks sequence labeling as illustrated by named entity recognition and chunking discussed above all state of the art published methods rely on lexical features for such tasks Sequence labeling includes both structural aspect and tagging aspect
416,B,P05-1045,D09-1120,1,"(Luo, 2005; Finkel and Manning, 2008)","At a high level, our system resembles a pairwise coreference model (Soon et al., 1999; Ng and Cardie, 2002; Bengston and Roth, 2008); for each mention mi, we select either a single-best antecedent amongst the previous mentions m1, . . . ,mi−1, or the NULL mention to indicate the underlying entity has not yet been evoked. Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming first. 4The MUC measure is problematic when the system predicts many more clusters than actually exist (Luo, 2005; Finkel and Manning, 2008); also, singleton clusters do not contribute to evaluation. While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors.",high level our system resembles pairwise coreference model for each mention mi we select either single best antecedent amongst the previous mentions or the mention to indicate the underlying entity has not yet been evoked Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming measure is problematic when the system predicts many more clusters than actually exist also singleton clusters do not contribute to evaluation While much research has explored how to reconcile pairwise decisions to form coherent clusters we simply take the transitive closure of our pairwise decision which can and does cause system errors
417,B,P05-1045,D09-1158,1,"(Finkel et al., 2005)","In this paper, we apply it to the problem of named entity recognition (NER). In this section, we describe the NER classifier and the features used in our experiments. 4.1 NER features We used the features generated by the CRF package (Finkel et al., 2005). These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word.",this paper we apply it to the problem of named entity recognition this section we describe the classifier and the features used in our experiments features used the features generated by the package These features include the word string feature the case feature for the current word the context words for the current word and their cases the presence in dictionaries for the current word the position of the current word in the sentence prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word
418,B,P05-1045,D10-1048,1,"(Finkel et al., 2005)","The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X.",The syntactic information is used to identify the mention head words and to define the ordering of mentions in given sentence For fair comparison with previous work we do not use gold named entity labels or mention types but instead take the labels provided by the named entity recognizer Evaluation Metrics use evaluation metrics widely used in the literature pairwise computed over mention pairs in the same entity cluster which measures how many predicted clusters need to be merged to cover the gold clusters and which uses the intersection between predicted and gold clusters for given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall respectively refer the interested reader to
419,B,P05-1045,D10-1099,1,"(Finkel et al., 2005)","To summarize precisions across relations, we take their average, and their average weighted by the proportion of predicted instances for the given relation. We preprocess our textual data as follows: We first use the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in the corpus. The NER tagger segments each document into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LOCATION and NONE.",summarize precisions across relations we take their average and their average weighted by the proportion of predicted instances for the given relation preprocess our textual data as follows use the named entity recognizer to find entity mentions in the corpus The tagger segments each document into sentences and classifies each token into categories and
420,B,P05-1045,D11-1034,1,"(Finkel et al., 2005)","We focus on German-to-English. possible bias due to differences in punctuation conventions. Then, we use the Stanford Named Entity Recognizer (Finkel et al., 2005) to identify named entities, which we replace with a unique token (‘NE’). Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000).",focus on to possible bias due to differences in punctuation conventions Then we use the Named Entity Recognizer to identify named entities which we replace with unique token we replace all nouns with their tag we use the Tagger
421,B,P05-1045,D11-1072,1,(Finkel05),"We first identify noun phrases that potentially denote named entities. We use the Stanford NER Tagger (Finkel05) to discover these and segment the text accordingly. Entity Candidates: For possible entities (with unique canonical names) that a mention could denote, we harness existing knowledge bases like DBpedia or YAGO.",identify noun phrases that potentially denote named entities use the to discover these and segment the text accordingly Entity Candidates For possible entities that mention could denote we harness existing knowledge bases like DBpedia or
422,B,P05-1045,D11-1075,1,"(Finkel et al., 2005)","The first data set contains a set of seminar announcements (Freitag and McCallum, 1999), annotated with four slot labels, namely stime (start time), etime (end time), speaker and location. We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL (Finkel et al., 2005). There are 309 seminar announcements with 2262 candidates in this data set.",The data set contains set of seminar announcements annotated with slot labels namely stime etime speaker and location used as candidates all strings labeled in the annotated data as well as all named entities found by the tagger for There are seminar announcements with candidates in this data set
423,B,P05-1045,D11-1135,1,"(Finkel et al., 2005)","First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions.",we employ tools to tokenize sentence split and Part Speech tag document we recognize named entities by labelling tokens with and tags Consecutive tokens which share the same category are assembled into entity mentions
424,B,P05-1045,D11-1141,2,"(Finkel et al., 2005)","We report results at segmenting named entities in Table 6. Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer (Finkel et al., 2005), T-SEG obtains a 52% increase in F1 score. Because Twitter contains many distinctive, and infrequent entity types, gathering sufficient training data for named entity classification is a difficult task.",report results at segmenting named entities in Table Compared with the state of the art newstrained Stanford Named Entity Recognizer obtains increase in score Because contains many distinctive and infrequent entity types gathering sufficient training data for named entity classification is difficult task
425,B,P05-1045,D12-1009,0,"(Finkel et al., 2005)","The parent of block b is the random variable rb, and we alternate between sampling values of the latent classes z and the parents r. The sampling distributions are annealed, as a search technique to find the best configuration of assignments (Finkel et al., 2005). At temperature τ , we sample a block’s parent according to: For each conversation thread, any message is a candidate for the parent of block b (except b itself) including the dummy “start” block.",The parent of block is the random variable rb and we alternate between sampling values of the latent classes and the parents The sampling distributions are annealed as search technique to find the best configuration of assignments temperature we sample block parent according to For each conversation thread any message is candidate for the parent of block including the dummy start block
426,B,P05-1045,D12-1042,1,"(Finkel et al., 2005)","The first was developed by Riedel et al. (2010) by aligning Freebase3 relations with the New York Times (NYT) corpus. They used the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence. Riedel et al. (2010) observes that evaluating on this corpus underestimates true extraction accuracy because Freebase is incomplete.",The was developed by by aligning relations with the New York Times corpus They used the named entity recognizer to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence observes that evaluating on this corpus underestimates true extraction accuracy because is incomplete
427,B,P05-1045,D12-1072,0,"(Finkel et al., 2005)","Their aim was to automatically identify both quotes and speakers, and then to attribute each quote to a speaker, in a corpus of classic literature that they compiled themselves. To identify potential speakers they used the Stanford NER tagger (Finkel et al., 2005) and a method outlined in Davis et al. (2003) that allowed them to find nominal character references. They then grouped name variants and pronominal mentions into a coreference chain.",Their aim was to automatically identify both quotes and speakers and then to attribute each quote to speaker in corpus of classic literature that they compiled themselves identify potential speakers they used the tagger and method outlined in that allowed them to find nominal character references They then grouped name variants and pronominal mentions into coreference chain
428,B,P05-1045,D12-1076,1,"(Finkel et al., 2005)","We therefore extract two extra kinds of features, named entities that do not appear in the Wikipedia anchor text dictionary, and biographical information. We use Stanford Named Entity Recognizer(Finkel et al., 2005) to collect named entities which are not in the Wikipedia list. We use regular expressions to extract email address, phone numbers and birth years.",therefore extract extra kinds of features named entities that do not appear in the anchor text dictionary and biographical information use Stanford Named Entity Recognizer to collect named entities which are not in the list use regular expressions to extract email address phone numbers and birth years
429,B,P05-1045,D12-1080,0,"(Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011)","The search process assumes that if a fluent’s entity and slot value co-occur in a sentence,5 that sentence is typically a positive example of the fluent.6 This is sometimes known asdistant supervision(Craven and Kumlien, 1999; Mintz et al., 2009). We use the Stanford Core NLP suite (Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011) to annotate each document with POS and NER tags, parse trees, and coreference chains. On top of this, we apply a rule-based temporal expression extractor (Chang and Manning, 2012).",The search process assumes that if fluent entity and slot value co occur in that sentence is typically positive example of the This is sometimes known asdistant supervision use the Stanford Core suite to annotate each document with and tags parse trees and coreference chains top of this we apply rule based temporal expression extractor
430,B,P05-1045,D12-1131,0,Finkel et al. (2005),"Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models.",Skip chain and collective inference have been applied to problems in and to named entity recognition also integrated non local information into entity annotation algorithms using sampling Our model can be applied to variety of off theshelf structured prediction models
431,B,P05-1045,D13-1040,1,"(Finkel et al., 2005)","The latter were extracted from 428K documents. After post-processing (tokenization, sentence-splitting, and part-of-speech tagging), Must-link Tuple F(i, NEPAIR:PER-PER, TRIGGER:wife) ∧ P(j, fi) ∧ P(k, fi)⇒ ¬Z(j, t) ∨ Z(k, r) F(i, NEPAIR:PER-LOC, TRIGGER:die) ∧ P(j, fi) ∧ P(k, fi)⇒ ¬Z(j, t) ∨ Z(k, r) F(i, PATH:←nsubj←die→prep→in→pobj→) ∧ P(j, fi) ∧ P(k, fi)⇒ ¬Z(j, t) ∨ Z(k, r) F(i, SOURCE:Kobe, DEST:Lakers) ∧ P(j, fi) ∧ P(k, fi)⇒ ¬Z(j, t) ∨ Z(k, r) named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC (Finkel et al., 2005). Dependency paths for each pair of named entity mentions were extracted from the output of the MaltParser (Nivre et al., 2004).",The latter were extracted from documents After post processing Must link named entities were automatically recognized and labeled with and Dependency paths for each pair of named entity mentions were extracted from the output of the
432,B,P05-1045,D13-1042,1,"(Finkel et al., 2005)","But human editorial judgment being the bottleneck, we sampled 50% or 50,000 snippets, whichever was smaller. Starting with about 752,450 pages, we ran the Stanford NER (Finkel et al., 2005) to mark person spans. Pages with fewer than five non-person tokens per person were discarded; this effectively discarded long list pages without any informative text to disambiguate anyone, and left us with 574,135 pages.",But human editorial judgment being the bottleneck we sampled or snippets whichever was smaller Starting with about pages we ran the to mark person spans Pages with fewer than five non person tokens per person were discarded this effectively discarded long list pages without any informative text to disambiguate anyone and left us with pages
433,B,P05-1045,D13-1043,1,"(Finkel et al., 2005)","Extracting Named Entities. EXEMPLAR employs the Stanford NER (Finkel et al., 2005) to recognize named entities. We consider these types of entities: people, organization, location, miscellaneous and date.",Extracting Named Entities employs the to recognize named entities consider these types of entities people organization location miscellaneous and date
434,B,P05-1045,D13-1103,1,"(Finkel et al., 2005)","Extracted text is divided into sentences using Punkt unsupervised sentence splitter (Kiss and Strunk, 2006). The QA-SYS performs Part of Speech tagging using Stanford POS tagger (Toutanova et al., 2003), and Named Entity Recognition using Stanford NER (Finkel et al., 2005), and then builds a Lucene index over the set of input documents. In the second stage the QA-SYS performs POS tagging, NE recognition, and question type classification for an input question.",Extracted text is divided into sentences using unsupervised sentence splitter The performs Part of tagging using tagger and Named Entity Recognition using and then builds index over the set of input documents the stage the performs tagging recognition and question type classification for an input question
435,B,P05-1045,D13-1117,1,Finkel et al. (2005),"We predicted the label sequence Y = {LOC, MISC, ORG, PER, O}T without considering the BIO tags. For training the CRF model, we used a comprehensive set of features from Finkel et al. (2005) that gives state-of-the-art results on this task. A total number of 437906 features were generated on the CoNLL-2003 training dataset.",predicted the label sequence without considering the tags For training the model we used comprehensive set of features from that gives state of the art results on this task total number of features were generated on the training dataset
436,B,P05-1045,D13-1136,1,"(Finkel et al., 2005)","NYT+FB This dataset, developed by (Riedel et al., 2010), aligns Freebase relations with the New York Times corpus. Entities were found using the Stanford named entity tagger (Finkel et al., 2005), and were matched to their name in Freebase. For each mention, sentence level features are extracted which include part of speech, named entity and dependency tree path properties.",This dataset developed by aligns relations with the New York Times corpus Entities were found using the named entity tagger and were matched to their name in For each mention sentence level features are extracted which include part of speech named entity and dependency tree path properties
437,B,P05-1045,D13-1142,0,"(Finkel et al., 2005; Ratinov et al., 2011; Bunescu and Pasca, 2006; Kulkarni et al., 2009; Milne and Witten, 2008)","Detected NEs may also serve as anchor text for the link. NER is a fairly researched field (Finkel et al., 2005; Ratinov et al., 2011; Bunescu and Pasca, 2006; Kulkarni et al., 2009; Milne and Witten, 2008) and is also used in several commercial applications such as Zemanta, OpenCalais and AlchemyAPI4, which are able to automatically insert links for a NE pointing to a knowledge base such as Wikipedia or IMDB. However, at this point they are unable to link to arbitrary documents, but may be useful in conjunction with other methods.",Detected may also serve as anchor text for the link is fairly researched field and is also used in several commercial applications such as Zemanta OpenCalais and which are able to automatically insert links for pointing to knowledge base such as Wikipedia or However at this point they are unable to link to arbitrary documents but may be useful in conjunction with other methods
438,B,P05-1045,D13-1178,1,"(Finkel et al., 2005)","The set of types are: person, organization, location, time unit, number, amount, group, business, executive, leader, effect, activity, game, sport, device, equipment, structure, building, substance, nutrient, drug, illness, organ, animal, bird, fish, art, book, and publication. To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005)5, and also look up the argument in WordNet 2.1 and record 5We used the system downloaded from: stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it. the first three senses if they map to our target semantic types. We use regular expressions to recognize dates and numeric expressions, and map personal pronouns to <person>.",The set of types are person organization location time unit number amount group business executive leader effect activity game sport device equipment structure building substance nutrient drug illness organ animal bird fish art book and publication assign types to arguments we apply Stanford Named Entity Recognizer and also look up the argument in and record used the system downloaded from and used the class model distributed with it the senses if they map to our target semantic types use regular expressions to recognize dates and numeric expressions and map personal pronouns to person
439,B,P05-1045,D13-1191,1,"(Finkel et al., 2005)","Although our modeling approach ultimately treats texts as bags of terms (unigrams and bigrams), one important preprocessing step was taken to further improve the interpretability of the inferred representation: named entity mentions of persons. We identified these mentions of persons using Stanford NER (Finkel et al., 2005) and treated each person mention as a single token. In our qualitative analysis of the model (§4.2), we will show how this special treatment of person mentions enables the association of well-known individuals with debate topics.",Although our modeling approach ultimately treats texts as bags of terms important preprocessing step was taken to further improve the interpretability of the inferred representation named entity mentions of persons identified these mentions of persons using and treated each person mention as single token our qualitative analysis of the model we will show how this special treatment of person mentions enables the association of well known individuals with debate topics
440,B,P05-1045,E09-1007,2,"(Finkel et al., 2005)","The different materials that we obtained constitute the CBC system’s NE resource. Our aim now is to exploit this resource and to show that it allows to improve the performances of different classic NER systems. • CBC-NER system M (in short CBC M) based on the CBC system’s NE resource using the manual cluster annotation (line 1 in Table 1), • CBC-NER system A (in short CBC A) based on the CBC system’s NE resource using the automatic cluster annotation (line 1 in Table 1), • XIP NER or in short XIP (Brun and Hagège, 2004) (line 2 in Table 1), • Stanford NER (or in short Stanford) associated to the following model provided by the tool and which was trained on different news 1 CBC-NER system M 71.67 23.47 35.36 CBC-NER system A 70.66 32.86 44.86 Stanford + XIP + CBC M 72.94 77.70 75.24 Stanford + XIP + CBC A 73.55 78.93 76.15 GATE + XIP + CBC M 69.62 67.79 68.69 GATE + XIP + CBC A 69.87 69.10 69.48 corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005) (line 3 in Table 1), • GATE NER or in short GATE (Cunningham et al., 2002) (line 4 in Table 1), • and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned NER systems (lines 5 to 7 in Table 1). Notice that these baseline hybrid systems use the annotation combination process described in §2.6.1.",The different materials that we obtained constitute the system resource Our aim now is to exploit this resource and to show that it allows to improve the performances of different classic systems system based on the system resource using the manual cluster annotation system based on the system resource using the automatic cluster annotation or in short associated to the following model provided by the tool and which was trained on different news system system corpora ner eng or in short and several hybrid systems which are given by the combination of pairs taken among the set of the last mentioned systems Notice that these baseline hybrid systems use the annotation combination process described in
441,B,P05-1045,E09-1011,1,"(Finkel et al., 2005)","We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tagger (Ratnaparkhi, 1996). We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005). On the Arabic side, we normalize the data by changing final ’Y’ to ’y’, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources.",then proceed to split the data into smaller sentences and tag them using Ratnaparkhi Maximum Entropy Tagger parse the data using the Collins Parser and then tag person location and organization names using the Stanford Named Entity Recognizer the side we normalize the data by changing final to and changing the various forms of hamza to bare since these characters are written inconsistently in some sources
442,B,P05-1045,E09-1037,0,"(Finkel et al., 2005)","There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference.",There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non local features Some stem from work on graphical models including loopy belief propagation Gibbs sampling sequential Monte Carlo methods such as particle filtering and variational inference Also relevant are stacked learning interpretable as approximation of non local feature values and estimation which allows training without inference
443,B,P05-1045,E12-1029,1,"(Finkel et al., 2005)","The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger (Finkel et al., 2005) to identify Named Entities within the NP. The context features include four words to the left of the NP, four words to the right of the NP, and the lexico-syntactic patterns generated by AutoSlog to capture expressions around the NP (see (Riloff, 1993) for details).",The features include the head noun and its premodifiers also use the tagger to identify Named Entities within the The context features include words to the left of the words to the right of the and the lexico syntactic patterns generated by to capture expressions around the
444,B,P05-1045,E12-1033,1,"(Finkel et al., 2005)","ETHICS).8 For semantic role labeling we use SWIRL9, for chunk parsing CASS (Abney, 1991) and for constituency parsing Stanford Parser (Klein and Manning, 2003). Named-entity information is provided by Stanford Tagger (Finkel et al., 2005). Convolution kernels (CK) are special kernel functions.",For semantic role labeling we use for chunk parsing and for constituency parsing Parser Named entity information is provided by Tagger Convolution kernels are special kernel functions
445,B,P05-1045,E12-1054,1,"(Finkel et al., 2005)","Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER (Finkel et al., 2005). Normal Spelling Error We apply the Jazzy spelling detector4 and rule out all cases in which it is able to detect the error.",Named Entity The replaced token should not be part of named entity For this purpose we applied the Normal Spelling Error apply the spelling and rule out all cases in which it is able to detect the error
446,B,P05-1045,I08-5014,1,"(Finkel, Grenager, and Manning, 2005)","The data collected for English is used to populate the English named entity database which is significantly accurate. We have used the freely available Stanford Named Entity Recognizer (Finkel, Grenager, and Manning, 2005) in our engine. The data collected for Indian languages will be used to build a database of named entities for the given language. 2.2 Parser The crawler saves the content in an html form onto the system.",The data collected for is used to populate the named entity database which is significantly accurate have used the freely available Stanford Named Entity Recognizer in our engine The data collected for languages will be used to build database of named entities for the given language The crawler saves the content in an html form onto the system
447,B,P05-1045,J11-1002,0,"(Finkel, Grenager, and Manning 2005)","As propagation is not performed in KN06, we also implemented a nonpropagation version of our approach, in which opinion words are only extracted by the seed words and targets which are extracted by both the seeds and extracted opinion words. Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique (Lafferty, McCallum, and Pereira 2001) for extraction, which is a popular information extraction method and has been successfully used in labeling tasks such as POS tagging (Lafferty, McCallum, and Pereira 2001) and Named Entity Recognition (Finkel, Grenager, and Manning 2005). The well-known toolkit CRF++4 is employed.",propagation is not performed in we also implemented nonpropagation version of our approach in which opinion words are only extracted by the seed words and targets which are extracted by both the seeds and extracted opinion words Furthermore as our tasks can be regarded as sequential labeling problem we experimented with the conditional random fields technique for extraction which is popular information extraction method and has been successfully used in labeling tasks such as tagging and Named Entity Recognition The well known toolkit is employed
448,B,P05-1045,J12-4004,1,"(Finkel, Grenager, and Manning 2005)","We focus on French-to-English, but the results are robust and consistent (we repeated the same experiments for all language pairs, with very similar outcomes). First, we remove all punctuation to eliminate possible bias due to differences in punctuation conventions.3 Then, we use the Stanford Named Entity Recognizer (Finkel, Grenager, and Manning 2005) to identify named entities, which we replace with a unique token (‘NE’). Next, we replace all nouns with their part-of-speech (POS) tag; we use the Stanford POS Tagger (Toutanova and Manning 2000).",focus on to but the results are robust and consistent we remove all punctuation to eliminate possible bias due to differences in punctuation Then we use the Named Entity Recognizer to identify named entities which we replace with unique token we replace all nouns with their part of speech tag we use the Tagger
449,B,P05-1045,J13-2001,2,"(Finkel, Grenager, and Manning 2005)","To study whether the final performance of NE alignment is sensitive to the choice of initial NE recognizers, we investigate the final alignment performance across different Chinese and English NE recognizers. First, we test the NE alignment performance with the same Chinese NE recognizer (Wu’s system, adopted earlier) but with different English NE recognizers that include the Mallet toolkit (used before), the Stanford NE recognizer (Finkel, Grenager, and Manning 2005), and Minor Third (Cohen 2004). Table 4 shows the type-insensitive and type-sensitive (within parentheses) results.",study whether the final performance of alignment is sensitive to the choice of initial recognizers we investigate the final alignment performance across different and recognizers we test the alignment performance with the same recognizer but with different recognizers that include the toolkit the recognizer and Minor Third Table shows the type insensitive and type sensitive results
450,B,P05-1045,N06-1054,2,"(Finkel et al., 2005)","Nonetheless, such global properties can improve the accuracy of a model, so recent NLP papers have considered practical techniques for decoding with them. Such techniques include Gibbs sampling (Finkel et al., 2005), a general-purpose Monte Carlo method, and integer linear programming (ILP), (Roth and Yih, 2005), a general-purpose exact framework for NP-complete problems. Under generative models such as hidden Markov models, the probability of a labeled sequence depends only on its local properties.",Nonetheless such global properties can improve the accuracy of model so recent papers have considered practical techniques for decoding with them Such techniques include sampling general purpose Monte Carlo method and integer linear programming general purpose exact framework for complete problems Under generative models such as hidden models the probability of labeled sequence depends only on its local properties
451,B,P05-1045,N07-1009,0,Finkel et al. (2005),"Undirected graphical models such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) have shown great success for problems involving structured output variables (e.g. Wellner et al. (2004), Finkel et al. (2005)). For many real-world NLP applications, however, the required graph structure can be very complex, and computing the global normalization factor even approximately can be extremely hard.",Undirected graphical models such as Conditional Random Fields have shown great success for problems involving structured output variables For many real world applications however the required graph structure can be very complex and computing the global normalization factor even approximately can be extremely hard
452,B,P05-1045,N07-1011,0,"(Finkel et al., 2005)","However, they do not investigate rank-based loss functions. Others have attempted to train global scoring functions using Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004). The main distinctions of our approach are that it is simple to implement, not computationally intensive, and adaptable to arbitrary loss functions.",However they do not investigate rank based loss functions Others have attempted to train global scoring functions using sampling message propagation and linear programming The main distinctions of our approach are that it is simple to implement not computationally intensive and adaptable to arbitrary loss functions
453,B,P05-1045,N07-1042,0,Finkel et al. (2005),"In the context of information fusion of single relationships across multiple documents, Downey et al. (2005) propose a method that models the probabilities of positive and negative extracted classifications. More distantly related, Sutton and McCallum (2004) and Finkel et al. (2005) propose graphical models for combining information about a given entity from multiple mentions. In the field of question answering, Prager et al. (2004) answer a question about the list of compositions produced by a given subject by looking for related information about the subject’s birth and death.",the context of information fusion of single relationships across multiple documents propose method that models the probabilities of positive and negative extracted classifications More distantly related and propose graphical models for combining information about given entity from multiple mentions the field of question answering answer question about the list of compositions produced by given subject by looking for related information about the subject birth and death
454,B,P05-1045,N07-2046,0,"(Finkel et al, 2005; Ji & Grishman, 2005)","This might help alleviate the kind of error propagation with dualpass strategies that particularly afflicts long documents. Recent applications of statistical coreference models are beginning to show promise (Finkel et al, 2005; Ji & Grishman, 2005). Lastly, we can see this whole study as a particular challenge case for transfer learning, and indeed such work as Sutton and McCallum’s (2005) has looked at the name-tagging task from a transfer learning standpoint.",This might help alleviate the kind of error propagation with dualpass strategies that particularly afflicts long documents Recent applications of statistical coreference models are beginning to show promise Lastly we can see this whole study as particular challenge case for transfer learning and indeed such work as has looked at the name tagging task from transfer learning standpoint
455,B,P05-1045,N09-1037,1,"(Finkel et al., 2005)","The model we trained had 200 clusters, and we used it to assign each word in the training and test data to one of the clusters. For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005). For parse features, we used the exact same features as described in (Finkel and Manning, 2008).",The model we trained had clusters and we used it to assign each word in the training and test data to one of the clusters For the named entity features we used fairly standard feature set similar to those described in For parse features we used the exact same features as described in
456,B,P05-1045,N09-1068,1,"(Finkel et al., 2005)","The use of CRFs for sequence modeling has become standard so we will omit the model details; good explanations can be found in a number of places (Lafferty et al., 2001; Sutton and McCallum, 2007). Our features were based on those in (Finkel et al., 2005). We used three named entity datasets, from the CoNLL 2003, MUC-6 and MUC-7 shared tasks.",The use of for sequence modeling has become standard so we will omit the model details good explanations can be found in number of places Our features were based on those in used named entity datasets from the and shared tasks
457,B,P05-1045,N10-1068,0,"(Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009)","Bayesian learning is a wide-ranging field. We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009). Our overall plan is to give a generic algorithm for Bayesian training that is a “drop-in replacement” for EM training.",learning is wide ranging field focus on training using sampling because it has been popularly applied in the natural language literature Our overall plan is to give generic algorithm for training that is drop in replacement for training
458,B,P05-1045,N10-1117,0,"(Finkel et al., 2005)","In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008). In both cases we usually work in the framework of graphical models—in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials.",order to explore richer model structures the community has recently started to investigate the use of other well known machine learning techniques for marginal inference such technique is Markov chain Monte Carlo and in particular sampling another is sum product belief propagation both cases we usually work in the framework of graphical our case with factor graphs that describe our distributions through variables factors and factor potentials
459,B,P05-1045,N10-1121,1,"(Finkel et al., 2005)","The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008).",The documents were parsed using the Parser Namedentity information was obtained by the tagger roles were obtained by using the parser by
460,B,P05-1045,N12-1008,2,Finkel et al. (2005),"Constraints have been explored both at sentence and document level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency.",Constraints have been explored both at sentence and document level For example employ document level constraints to encourage global consistency of named entity assignments Likewise use constraints at multiple levels such as sentence level constraints to specify field boundaries and global constraints to ensure relation level consistency
461,B,P05-1045,N12-1013,1,"(Sutton and McCallum, 2005; Finkel et al., 2005)","By switching to ERMA training, we improve this result further to 85.1%. The second application, information extraction from seminar announcements, has been modeled previously with skip-chain CRFs (Sutton and McCallum, 2005; Finkel et al., 2005). The skip-chain CRF introduces loops and requires approximate inference, which motivates minimum risk training.",switching to training we improve this result further to The application information extraction from seminar announcements has been modeled previously with skip chain The skip chain introduces loops and requires approximate inference which motivates minimum risk training
462,B,P05-1045,N12-1065,1,"(Finkel et al., 2005)","Our experiments and their results are described in Section 4. Section 5 draws our conclusions and indicates avenues for future work. 2.1 Named entity recognition The Stanford named entity recognition (NER) software1 (Finkel et al., 2005) is an implementation of linear chain Conditional Random Field (CRF) sequence models, which includes a three class (person, organization, location and other) named entity recognizer for English. 2.2 Topic detection LDA (Blei et al., 2003) is a generative probabilistic model where documents are viewed as mixtures over underlying topics, and each topic is a distribution over words. Both the document-topic and the topicword distributions are assumed to have a Dirichlet prior.",Our experiments and their results are described in Section Section draws our conclusions and indicates avenues for future work Named entity recognition The named entity recognition is an implementation of linear chain Conditional Random Field sequence models which includes class named entity recognizer for Topic detection is generative probabilistic model where documents are viewed as mixtures over underlying topics and each topic is distribution over words Both the document topic and the topicword distributions are assumed to have prior
463,B,P05-1045,N12-1080,1,"(Finkel et al., 2005)","Our similarity measure is calculated using the number of shared named entities and nouns between sentences as seen in equation 9. For identification of named entities, we use Stanford NER (Finkel et al., 2005). It is straightforward to weight the resulting TextRank scores for each sentence using their cluster’s temporal importance. 4 Experimental Results We test on a set of 13 Wikipedia articles describing historical battles.",Our similarity measure is calculated using the number of shared named entities and nouns between sentences as seen in equation For identification of named entities we use is straightforward to weight the resulting scores for each sentence using their cluster temporal importance Experimental Results test on set of Wikipedia articles describing historical battles
464,B,P05-1045,N12-1084,1,"(Finkel, Grenager, and Manning, 2005; Ratinov and Roth, 2009; Ritter et al., 2011)","This sub-task labels each person-mention with a bullying role. It uses Named Entity Recognition (NER) (Finkel, Grenager, and Manning, 2005; Ratinov and Roth, 2009; Ritter et al., 2011) as a subroutine to identify named person entities, though we are also interested in unnamed persons such as “my teacher” and pronouns. It is related to Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002; Punyakanok, Roth, and Yih, 2008) but differs critically in that our roles are not tied to specific verb predicates.",This sub task labels each person mention with bullying role uses Named Entity Recognition as subroutine to identify named person entities though we are also interested in unnamed persons such as my teacher and pronouns is related to Semantic Role Labeling but differs critically in that our roles are not tied to specific verb predicates
465,B,P05-1045,N12-1085,0,"(Finkel et al., 2005)","While there are many approaches for inference in statistical models, we turn to MCMC methods (Neal, 1993) to discover the underlying structure of the model. More specifically, we seek a posterior distribution over latent variables that partition words in a sentence into flow and inert groups; we estimate this posterior using Gibbs sampling (Finkel et al., 2005). The sampler requires an initial state that respects the invariant.",While there are many approaches for inference in statistical models we turn to methods to discover the underlying structure of the model More specifically we seek posterior distribution over latent variables that partition words in sentence into flow and inert groups we estimate this posterior using sampling The sampler requires an initial state that respects the invariant
466,B,P05-1045,N13-1006,1,Finkel et al. (2005),"BerkeleyAligner also gives posterior probabilities Pa for each aligned word pair. We used the CRF-based Stanford NER tagger (using Viterbi decoding) as our baseline monolingual NER tool.6 English features were taken from Finkel et al. (2005). Table 1 lists the basic features of Chinese NER, where ◦ means string concatenation and yi is the named entity tag of the ith word wi.",also gives posterior probabilities for each aligned word pair used the based tagger as our baseline monolingual features were taken from Table lists the basic features of where means string concatenation and is the named entity tag of the ith word wi
467,B,P05-1045,N13-1007,1,"(Finkel et al., 2005)","Cohen’s kappa (Cohen, 1960) was 0.83 for English, 0.88 for Japanese, 5We filtered out phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore, Proposedlocal, SMT , and P&D, since most of them were not paraphrases. We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER. Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of Hashimoto et al. (2011)), and thus we did not apply the filter to them any further. and 0.85 for Chinese, all of which indicated reasonably good (Landis and Koch, 1977).",Cohen kappa was for for filtered out phrase pairs in which phrase contained named entity but the other did not contain the named entity from the output of ProposedScore Proposedlocal and since most of them were not paraphrases used for named entity recognition for and for Hashisup and did the named entity filtering of the same kind and thus we did not apply the filter to them any further and for all of which indicated reasonably good
468,B,P05-1045,N13-1037,0,"(Finkel et al., 2005)","In part-of-speech tagging, the accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to 85% accuracy on Twitter (Gimpel et al., 2011). In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test set (Finkel et al., 2005). In parsing, Foster et al. (2011) report double-digit decreases in accuracy for four different state-of-the-art parsers when applied to social media text.",part of speech tagging the accuracy of the tagger falls from on Wall Street Journal text to accuracy on named entity recognition the trained recognizer achieves measure down from on the test set parsing report double digit decreases in accuracy for different state of the art parsers when applied to social media text
469,B,P05-1045,P06-1059,2,"(Finkel et al., 2005)","In conventional CRFs and semi-CRFs, one can only use the information on the adjacent previous label when defining the features on a certain state or entity. In NER tasks, however, information about a distant entity is often more useful than information about the previous state (Finkel et al., 2005). For example, consider the sentence “... including Sp1 and CP1.” where the correct labels of “Sp1” and “CP1” are both “protein”.",conventional and semi one can only use the information on the adjacent previous label when defining the features on certain state or entity tasks however information about distant entity is often more useful than information about the previous state For example consider the sentence including and where the correct labels of and are both protein
470,B,P05-1045,P06-1141,3,Finkel et al. (2005),"Both these approaches use loopy belief propagation (Pearl, 1988; Yedidia et al., 2000) for approximate inference. Finkel et al. (2005) hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data. They then employ Gibbs sampling (Geman and Geman, 1984) for dealing with their local feature weights and their non-local penalties to do approximate inference.",Both these approaches use loopy belief propagation for approximate inference hand set penalties for inconsistency in entity labeling at different occurrences in the text based on some statistics from training data They then employ sampling for dealing with their local feature weights and their non local penalties to do approximate inference
471,B,P05-1045,P06-2054,0,"(Sutton and McCallum, 2004; Finkel et al., 2005)","One is to add edges to structure to allow higher-order dependencies and another is to add features (or observable variables) to encode the non-locality. An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (Sutton and McCallum, 2004; Finkel et al., 2005). However, this approach requires additional time complexity in inference/learning time and it is only suitable for representing constraints by enforcing label consistency.",is to add edges to structure to allow higher order dependencies and another is to add features to encode the non locality additional consistent edge of linear chain conditional random field explicitly models the dependencies between distant occurrences of similar words However this approach requires additional time complexity in time and it is only suitable for representing constraints by enforcing label consistency
472,B,P05-1045,P08-1031,0,Finkel et al. (2005),"Doing so analytically is intractable due to the complexity of the model, but sampling-based techniques can be used to estimate the posterior. We employ Gibbs sampling, previously used in NLP by Finkel et al. (2005) and Goldwater et al. (2006), among others. This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model (Gelman et al., 2004).",Doing so analytically is intractable due to the complexity of the model but sampling based techniques can be used to estimate the posterior employ Gibbs sampling previously used in by and among others This technique repeatedly samples from the conditional distributions of each hidden variable eventually converging on Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model
473,B,P05-1045,P08-2012,1,Finkel et al. (2005),"We used the MUC-6 formal training and test data, as well as theNWIRE andBNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER, but we found that several documents where too long forlp solve to find a solution.4 We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005). The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger.",used the formal training and test data as well as portions of the corpus This corpus had portion but we found that several documents where too long forlp solve to find added named entity tags to the data using the tagger of The data is already annotated with tags so when they conflicted they overrode the tags output by the tagger
474,B,P05-1045,P08-4003,1,"(Finkel et al., 2005)","Using a generic format for standoff annotation allows the use of the coreference resolution as part of a larger system, but also performing qualitative error analysis using integrated MMAX2 functionality (annotation chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution.",Using generic format for standoff annotation allows the use of the coreference resolution as part of larger system but also performing qualitative error analysis using integrated functionality annotation chunks and named entities as well as additional information such as part of speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper Starting out with chunking pipeline which uses classical combination of tagger and chunker with the tagger the chunker and the Named Entity Recognizer the desire to use richer syntactic representations led to the development of parsing pipeline which uses and reranking parser to assign tags and uses base as chunk equivalents while also providing syntactic trees that can be used by feature extractors also supports using the parser yielding an easy to use only solution
475,B,P05-1045,P09-1113,1,"(Finkel et al., 2005)","Thus each syntactic row in Table 3 represents a single syntactic feature. 5.3 Named entity tag features Every feature contains, in addition to the content described above, named entity tags for the two entities. We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005). The tagger provides each word with a label from {person, location, organization, miscellaneous, none}. 5.4 Feature conjunction Rather than use each of the above features in the classifier independently, we use only conjunctive features.",Thus each syntactic row in Table represents single syntactic feature Named entity tag features Every feature contains in addition to the content described above named entity tags for the entities perform named entity tagging using the class named entity tagger The tagger provides each word with label from person location organization miscellaneous none Feature conjunction Rather than use each of the above features in the classifier independently we use only conjunctive features
476,B,P05-1045,P09-2041,1,"(Finkel et al., 2005)","Our expectation is that true news stories will have been reported in various forums, and hence the number of web documents which include the same combination of entities will be higher than with satire documents. To implement this method, we first use the Stanford Named Entity Recognizer4 (Finkel et al., 2005) to identify the set of person and organisation entities, E, from each article in the corpus. From this, we estimate the validity of the combination of entities in the article as: where g is the set of matching documents returned by Google using a conjunctive query.",Our expectation is that true news stories will have been reported in various forums and hence the number of web documents which include the same combination of entities will be higher than with satire documents implement this method we first use the Stanford Named Entity to identify the set of person and organisation entities from each article in the corpus From this we estimate the validity of the combination of entities in the article as where is the set of matching documents returned by using conjunctive query
477,B,P05-1045,P10-1015,1,"(Finkel et al., 2005)","Holmes) from the text. We processed each novel with the Stanford NER tagger (Finkel et al., 2005) and extracted noun phrases that were categorized as persons or organizations. We then clustered the noun phrases into coreferents for the same entity (person or organization).",Holmes from the text processed each novel with the tagger and extracted noun phrases that were categorized as persons or organizations then clustered the noun phrases into coreferents for the same entity
478,B,P05-1045,P10-1056,1,"(Finkel et al., 2005)","In addition, first mentions of entities in text introduce the entity into the discourse and so must be informative and properly descriptive (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, andLOCATIONs. First mentions to peopleFeature exploration on our development set found that under-specified references to people are much more disruptive to a summary than short references to organizations or locations.",addition first mentions of entities in text introduce the entity into the discourse and so must be informative and properly descriptive run the Stanford Named Entity Recognizer and record the number of mentions to peopleFeature exploration on our development set found that under specified references to people are much more disruptive to summary than short references to organizations or locations
479,B,P05-1045,P10-1148,1,"(Finkel et al., 2005)","That is, a non-animate noun can hardly constitute an experience. In order to make a distinction, we use the dependency parser and a named-entity recognizer (Finkel et al., 2005) that can recognize person pronouns and person names. 3.2 Classification To train our classifier, we first crawled weblogs from Wordpress3, one of the most popular blog sites in use today. Worpress provides an interface to search blog posts with queries.",That is non animate noun can hardly constitute an experience order to make distinction we use the dependency parser and named entity recognizer that can recognize person pronouns and person names Classification train our classifier we first crawled weblogs from one of the most popular blog sites in use provides an interface to search blog posts with queries
480,B,P05-1045,P10-2049,1,"(Finkel et al., 2005)","We identified a few typical sources of errors in a preliminary error analysis. We therefore suggest three extensions to the algorithm which are on the one hand possible in the OM setting and on the other hand represent special features of the target discourse type: [1.] We observed that the Stanford Named Entity Recognizer (Finkel et al., 2005) is superior to the Person detection of the (MUC6 trained) CogNIAC implementation. We therefore filter out Person antecedent candidates which the Stanford NER detects for the impersonal and demonstrative pronouns and Location & Organization candidates for the personal pronouns.",identified few typical sources of errors in preliminary error analysis therefore suggest extensions to the algorithm which are on the one hand possible in the setting and on the other hand represent special features of the target discourse type observed that the Named Entity Recognizer is superior to the Person detection of the implementation therefore filter out Person antecedent candidates which the detects for the impersonal and demonstrative pronouns and Location Organization candidates for the personal pronouns
481,B,P05-1045,P11-1037,1,"(Finkel et al., 2005)","Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new genre of text, which are short, informal, ungrammatical and noise prone. For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary, which requires a lot of annotated tweets or rules.",Because of the domain mismatch current systems trained on non tweets perform poorly on tweets new genre of text which are short informal ungrammatical and noise prone For example the average of the which is trained on the shared task data set and achieves state of the art performance on that task drops from to on tweets Thus building domain specific for tweets is necessary which requires lot of annotated tweets or rules
482,B,P05-1045,P11-1082,1,"(Finkel et al., 2005)","On the other hand, if the CR model is used, the YAGO feature for an instance involvingNPk and preceding clusterc will have the value 1 if and only if NPk has a TYPE or MEANS relation with any of the NPs inc. Since knowledge extraction from webbased encyclopedia is typically noisy (Ponzetto and Poesio, 2009), we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer (Finkel et al., 2005) and the other NP is a common noun. FrameNet is a lexico-semantic resource focused on semantic frames (Baker et al., 1998).",the other hand if the model is used the feature for an instance involvingNPk and preceding will have the value if and only if has or relation with any of the inc Since knowledge extraction from webbased is typically noisy we use to determine whether have relation only if is named entity of type person organization or location according to the recognizer and the other is common noun is lexico semantic resource focused on semantic frames
483,B,P05-1045,P11-1113,0,Finkel et al. (2005),"They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.",They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction used sampling simple Monte Carlo method used to perform approximate inference in factored probabilistic models using simulated annealing in place of decoding in sequence models such as and it is possible to incorporate non local structure while preserving tractable inference
484,B,P05-1045,P11-1114,1,"(Finkel et al., 2005)","These patterns are similar to dependency relations in that they typically represent the syntactic role of the NP with respect to other constituents (e.g., subject-of, object-of, and noun arguments). Semantic features:we use the Stanford NER tagger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign semantic class labels to each NP’s head noun. One of our goals was to explore the use ofdocument genre to permit more aggressive strategies for extracting role fillers.",These patterns are similar to dependency relations in that they typically represent the syntactic role of the with respect to other constituents features we use the tagger to determine if the targeted is named entity and we use the parser to assign semantic class labels to each head noun of our goals was to explore the use ofdocument genre to permit more aggressive strategies for extracting role fillers
485,B,P05-1045,P11-3004,1,"(Finkel et al., 2005)","We represent both contexts as vectors of URIs. To create d we extract all NEs from the text using the Stanford NE Recognizer (Finkel et al., 2005) and represent each NE by its Wikipedia URI. If a surface form is ambiguous, we choose the most popular NE with the popularity metric described below.",represent both contexts as vectors of create we extract all from the text using the Stanford Recognizer and represent each by its Wikipedia surface form is ambiguous we choose the most popular with the popularity metric described below
486,B,P05-1045,P12-1042,1,"(Finkel et al., 2005)","In addition to this shallow parsing method, we also use named entity recognition (NER) to identify more entities. We use the Stanford Named Entity Recognizer (Finkel et al., 2005) for this purpose. It recognizes three types of entities: person, location, and organization.",addition to this shallow parsing method we also use named entity recognition to identify more entities use the Stanford Named Entity Recognizer for this purpose recognizes types of entities person location and organization
487,B,P05-1045,P12-1072,1,"(Finkel et al., 2005)","Association, National Football League, and Major League Baseball) in 2009. Due to the large size of the corpora, we uniformly sampled a subset of documents for each corpus and ran the Stanford NER tagger (Finkel et al., 2005), which tagged named entities mentions as person, location, and organization. We used named entity of type person from the political blogs corpus, while we are interested in person and organization entities for the sports news corpus.",Association National Football League and Major League Baseball in Due to the large size of the corpora we uniformly sampled subset of documents for each corpus and ran the tagger which tagged named entities mentions as person location and organization used named entity of type person from the political blogs corpus while we are interested in person and organization entities for the sports news corpus
488,B,P05-1045,P12-1073,1,"(Finkel et al., 2005)","In case of multiple possible labels, the most frequent one is denoted by * in the Figure. The Figure also shows the results of the Stanford NER tagger for English (Finkel et al., 2005) (we used the MUC-7 classifier). Table 2 reports the performance of the local (L Wiki-tagger), local+global (LG Wiki tagger) and the Stanford tagger.",case of multiple possible labels the most frequent one is denoted by in the Figure The Figure also shows the results of the tagger for Table reports the performance of the local and the tagger
489,B,P05-1045,P12-1075,1,"(Finkel et al., 2005)","We carry out experiments on New York Times articles from years 2000 to 2007 (Sandhaus, 2008). Following (Yao et al., 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al., 2005) and dependency parsing (Nivre et al., 2004). We extract dependency paths for each pair of named entities in one sentence.",carry out experiments on New York Times articles from to Following we filter out noisy documents and use natural language packages to annotate the documents including tagging and dependency parsing extract dependency paths for each pair of named entities in sentence
490,B,P05-1045,P12-1087,1,"(Finkel et al., 2005)","Our feature extraction process consists of three steps: 1. Run Stanford CoreNLP with POS tagging and named entity recognition (Finkel et al., 2005); 2. Run dependency parsing on TAC with the Ensemble parser (Surdeanu and Manning, 2010) and on ClueWeb with MaltParser (Nivre et al., 2007)8; and 3.",Our feature extraction process consists of steps Stanford with tagging and named entity recognition dependency parsing on with the parser and on with and
491,B,P05-1045,P12-1089,3,"(Sutton and McCallum, 2004; Finkel et al., 2005)","Interestingly, several researchers have attempted to model label consistency and high-level relational constraints using state-of-the-art sequential models of named entity recognition (NER). Mainly, predetermined word-level dependencies were represented as links in the underlying graphical model (Sutton and McCallum, 2004; Finkel et al., 2005). Finkel et al. (2005) further modelled high-level semantic constraints; for example, using the CMU seminar announcements dataset, spans labeled as start timeor end timewere required to be semantically consistent.",Interestingly several researchers have attempted to model label consistency and high level relational constraints using state of the art sequential models of named entity recognition Mainly predetermined word level dependencies were represented as links in the underlying graphical model further modelled high level semantic constraints for example using the seminar announcements dataset spans labeled as start timeor end timewere required to be semantically consistent
492,B,P05-1045,P12-2013,1,"(Finkel et al., 2005)","Sentiment toward an entity: We again adopt a simplifying view by modeling all the named entities in a sentence without heeding the roles these entities play, i.e. whether they are targets or not. Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005). We only focus on Person and Organization named entities.",Sentiment toward an entity again adopt simplifying view by modeling all the named entities in sentence without heeding the roles these entities play whether they are targets or not Accordingly we extract all the named entities in sentence using Stanford Name Entity Recognizer only focus on Person and Organization named entities
493,B,P05-1045,P12-2024,1,"(Faruqui and Padó, 2010; Finkel et al., 2005)","After tokenization, Giza++ and Moses were respectively used to align the corpora and extract a lexical phrase table (PT). Similarly, the semantic phrase table (SPT) has been ex2Recently, a new dataset including “Unknown” pairs has been used in the “Cross-Lingual Textual Entailment for Content Synchronization” task at SemEval-2012 (Negri et al., 2012). tracted from the same corpora annotated with the Stanford NE tagger (Faruqui and Padó, 2010; Finkel et al., 2005). Dependency relations (DR) have been extracted running the Stanford parser (Rafferty and Manning, 2008; De Marneffe et al., 2006).",After tokenization and Moses were respectively used to align the corpora and extract lexical phrase table Similarly the semantic phrase table has been new dataset including Unknown pairs has been used in the Cross Lingual Textual Entailment for Content Synchronization task at SemEval tracted from the same corpora annotated with the tagger Dependency relations have been extracted running the parser
494,B,P05-1045,P12-2064,1,"(Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005)","We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract the features. evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances.",adopted the feature set investigated in for article error correction use the to extract the features evaluated in terms of accuracy precision recall and score is the number of correct predictions divided by the total number of instances
495,B,P05-1045,P12-2069,1,"(Finkel et al., 2005)","Before tackling these, we perform some preprocessing on the cluster of documents. It includes: cleaning up and normalization of the input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE (Cunningham et al., 2002), syntactical parsing and dependency parsing (collapsed) using the Stanford Parser (de Marneffe et al., 2006), and Named Entity Recognition using Stanford NER (Finkel et al., 2005). We have also developed a date resolution engine that focuses on days of the week and relative terms.",Before tackling these we perform some preprocessing on the cluster of documents includes cleaning up and normalization of the input using regular expressions sentence segmentation tokenization and lemmatization using syntactical parsing and dependency parsing using the and Named Entity Recognition using have also developed date resolution engine that focuses on days of the week and relative terms
496,B,P05-1045,P12-3003,0,"(Finkel et al., 2005)","The implementation of QuickView requires adapting existing NLP components trained on formal texts, which often performs poorly on tweets. For example, the average F1 of the Stanford NER (Finkel et al., 2005) drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets, while Liu et al. (2010) report that the F1 score of a state-ofthe-art SRL system (Meza-Ruiz and Riedel, 2009) falls to 42.5% on tweets as apposed to 75.5% on news. However, the adaptation of those components is challenging, owing to the lack of annotated tweets and the inadequate signals provided by a noisy and short tweet.",The implementation of requires adapting existing components trained on formal texts which often performs poorly on tweets For example the average of the drops from to on tweets while report that the score of state ofthe art system falls to on tweets as apposed to on news However the adaptation of those components is challenging owing to the lack of annotated tweets and the inadequate signals provided by noisy and short tweet
497,B,P05-1045,P13-1106,1,"(Finkel et al., 2005)","We only use the dev set for model development. The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al., 2005). It also serves as a stateof-the-art monolingual baseline for both English and Chinese.",only use the dev set for model development The based tagger was used as the monolingual component in our models also serves as stateof the art monolingual baseline for both and
498,B,P05-1045,P13-1129,1,"(Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006)","Other preprocessing steps that are required for processing a new novel include standarizing the typographical conventions, and performing POS tagging, NER tagging, and dependency parsing. We utilize the Stanford tools (Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006). In this section, we describe experiments conducted to evaluate our speaker identification approach.",Other preprocessing steps that are required for processing new novel include standarizing the typographical conventions and performing tagging tagging and dependency parsing utilize the tools this section we describe experiments conducted to evaluate our speaker identification approach
499,B,P05-1045,P13-1146,0,(Finkel 2005),"Tagging mentions of named entities with lexical types has been pursued in previous work. Most well-known is the Stanford named entity recognition (NER) tagger (Finkel 2005) which assigns coarse-grained types like person, organization, location, and other to noun phrases that are likely to denote entities. There is fairly little work on finegrained typing, notable results being (Fleischmann 2002; Rahman 2010; Ling 2012; Yosef 2012).",Tagging mentions of named entities with lexical types has been pursued in previous work Most well known is the named entity recognition tagger which assigns coarse grained types like person organization location and other to noun phrases that are likely to denote entities There is fairly little work on finegrained typing notable results being
500,B,P05-1045,P13-1166,1,"(Finkel et al., 2005)","The complex features based on VIAF,7 GeoNames8 and WordNet do contribute to the classification in the Mallet setup as removing them and only using the focus token, window and generic features causes a slight drop in overall F-score from 49.45 to 47.25. When training the Stanford NER system (Finkel et al., 2005) on just the tokens from the Freire data set and the parameters from english.all.3class.distsim.prop (included in the Stanford NER release, see also Van Erp and Van der Meij (2013)), our F-scores come very close to those reported by Freire et al. (2012), but mostly with a higher recall and lower precision. It is puzzling that the Stanford system obtains such high results with only very simple features, whereas for Mallet the complex features show improvement over simpler features.",The complex features based on and do contribute to the classification in the setup as removing them and only using the focus token window and generic features causes slight drop in overall score from to When training the system on just the tokens from the data set and the parameters from our scores come very close to those reported by but mostly with higher recall and lower precision is puzzling that the system obtains such high results with only very simple features whereas for the complex features show improvement over simpler features
501,B,P05-1045,P13-2036,1,"(Finkel et al., 2005)","We consider entities occurring more than η times as nodes and entity pairs co-occurring more than σ times as edges. To identify entities, we use a CRF-based named entity tagger (Finkel et al., 2005) and a Chinese word breaker (Gao et al., 2003) for English and Chinese corpora, respectively. Given two graphs Ge = (Ve, Ee) and Gc = (Vc, Ec), we initialize |Ve|-by-|Vc| initial similarity matrix R0 using PH and CX for every pair (e, c) where e ∈ Ve and c ∈ Vc.",consider entities occurring more than times as nodes and entity pairs co occurring more than times as edges identify entities we use based named entity tagger and word breaker for and corpora respectively Given graphs and we initialize by initial similarity matrix using and for every pair where and
502,B,P05-1045,P13-2116,1,"(Finkel et al., 2005)","With p = 0.01, Joint has statistically significant improvement of F1 score over all three other approaches with each probability threshold. We validate the hypothesis that using linguistic features, e.g., part-of-speech tags (Toutanova and Manning, 2000), named-entity tags (Finkel et al., 2005), and dependency trees (de Marneffe et al., 2006), helps improve the quality of our approach, called Joint. There are different ways to use shallow and linguistic features; we select Type	   Features Shallow	   Regular	  Expressions	  (Dalvi	  et	  al.,	  2012) Term	  proximity	  (Matsuo	  et	  al.,	  2003) DicConary	  and	  Freebase	  (Mintz	  et	  al.,	  2009) We created the following variants of Joint.",With Joint has statistically significant improvement of score over all other approaches with each probability threshold validate the hypothesis that using linguistic features part of speech tags named entity tags and dependency trees helps improve the quality of our approach called Joint There are different ways to use shallow and linguistic features we select Type Features Shallow Regular Expressions Term proximity and created the following variants of Joint
503,B,P05-1045,P13-2128,0,"(Finkel et al., 2005)","In lexical semantics, smoothing is often achieved by backing off from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves.",lexical semantics smoothing is often achieved by backing off from words to semantic classes either adopted from resource such as or induced from data Similarly distributional features support generalization in Named Entity Recognition Although distributional information is often used for smoothing to our knowledge there is little work on smoothing distributional models themselves
504,B,P05-1045,P13-2141,1,"(Finkel et al., 2005)","We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component. We used Open NLP POS tagger, Standford NER (Finkel et al., 2005) and MaltParser (Nivre et al., 2006) to label/tag sentences. We employed liblinear (Fan et al., 2008) as classifiers for NED and relation extraction and the solver is L2LR. 4.2 Performance of Relation Extraction Held-out Evaluation.",used Wikipedia articles for generating training data for the component used Open tagger and to sentences employed liblinear as classifiers for and relation extraction and the solver is Performance of Relation Extraction Held out Evaluation
505,B,P05-1045,P13-4023,0,"(Finkel et al., 2005)","In order to perform “live” entity type classification based on ad-hoc text inputs, several performance optimizations have been undertaken to operate under real-time conditions. State-of-the-art tools for named entity recognition such as the Stanford NER Tagger (Finkel et al., 2005) compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). However, we are not aware of any online tool that performs fine-grained typing of entity mentions.",order to perform live entity type classification based on ad hoc text inputs several performance optimizations have been undertaken to operate under real time conditions State of the art tools for named entity recognition such as the compute semantic tags only for small set of coarse grained types Person Location and Organization However we are not aware of any online tool that performs fine grained typing of entity mentions
506,B,P05-1045,Q13-1028,1,"(Finkel et al., 2005)","We approximate this facet by computing the number of explicit references to people, relying on three sources of information about animacy of words. The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005). We also created a list of personal pronouns such as ‘he’, ‘myself’ etc. which standardly indicate animate entities (animate pronouns).",approximate this facet by computing the number of explicit references to people relying on sources of information about animacy of words The is named entity tags returned by the recognition tool also created list of personal pronouns such as he myself etc which standardly indicate animate entities
507,B,P05-1045,S10-1020,1,"(Finkel et al., 2005)","Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Manning, 2003) and NE-tagging (Finkel et al., 2005), Wordnet for semantic classes and the U. S. census data for assigning gender values to person names.",Corry relies on rich linguistically motivated feature set which has however been manually reduced to features for efficiency reasons Corry has only participated in the open setting as it has already number of preprocessing modules integrated into the system the toolkit for parsing and tagging Wordnet for semantic classes and the census data for assigning gender values to person names
508,B,P05-1045,S12-1035,1,"(Finkel et al., 2005)","We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charsrlconll-1.1.tgz 7The original focus annotation targeted the 3,993 negations Train Devel Test 1 role 2,210 515 672 2 roles 89 15 38 3 roles 3 0 2 All 2,302 530 712 • Semantic roles using the labeler described by (Punyakanok et al., 2008); and • Verbal negation, indicates with ‘N’ if that token correspond to a verbal negation for which focus must be predicted. Figure 2 provides a sample of PB-FOC.",gathered all negations from sections and and discarded negations for which the focus or PropBank annotations were not sound leaving For each verbal negation provides the current sentence and the previous and next sentences as context For each sentence along with the gold focus annotations contains the following additional annotations number tags using the Brill tagger Named Entities using the named entity recognizer recognizer Chunks using the chunker by tree using the parser original focus annotation targeted the negations Train Devel Test role roles roles All Semantic roles using the labeler described by and negation indicates with if that token correspond to verbal negation for which focus must be predicted Figure provides sample of
509,B,P05-1045,S12-1082,1,"(Finkel et al., 2005; Toutanova et al., 2003)","In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 =∞. Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming. The exact type of pre-processing used depends on the metric used.",this work we use models with Initially all sentences are pre processed by the suite of tools process that includes named entity recognition normalization part of speech tagging lemmatization and stemming The exact type of pre processing used depends on the metric used
510,B,P05-1045,S12-1085,1,"(Finkel et al., 2005)","This tokenization strategy gives us the best results among all our three runs. For capturing and normalizing the above mentioned expressions, we make use of the Stanford NER Toolkit (Finkel et al., 2005). Some normalized samples are mentioned in figure 2.",This tokenization strategy gives us the best results among all our runs For capturing and normalizing the above mentioned expressions we make use of the Toolkit Some normalized samples are mentioned in figure
511,B,P05-1045,S13-1010,1,"(Finkel et al., 2005)","Wikipedia pages for nouns with senses (according to the disambiguation page) in a set of predefined categories were identified to form the list Wikipedia. Named entity recognition The Stanford Named Entity Recogniser (Finkel et al., 2005) was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are extracted to form the list Stanford. WordNet The final heuristic makes use of WordNet (Fellbaum, 1998) which lists nouns that are often used as proper nouns with capitalisation.",Wikipedia pages for nouns with senses in set of predefined categories were identified to form the list Wikipedia Named entity recognition The Stanford Named Entity Recogniser was run over the and any nouns that occur in the corpus with both named entity and non named entity tags are extracted to form the list Stanford The final heuristic makes use of which lists nouns that are often used as proper nouns with capitalisation
512,B,P05-1045,S13-1018,1,"(Finkel et al., 2005; Toutanova et al., 2003)","They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.",They can not be redistributed so we used the urls and scripts provided by the organizers to extract the corresponding metadata analysed the text in the metadata performing lemmatization tagging named entity recognition and classification and date detection using preliminary score for each similarity type was then calculated as follows General cosine similarity of
513,B,P05-1045,S13-1023,1,"(Finkel et al., 2005)","In our experiments, k was set to 0.1, the default value in the original model. In addition to the above text similarity measures, we used also the following common measures: (wq1 , . . . , wqn) the vectors of tf.idf weights associated to sentences p and q, the cosine distance is calculated as: where Lev(p, q) is the Levenshtein distance between the two sentences, taking into account the characters. 2.7.3 Named Entity Overlap We used the Stanford Named Entity Recognizer by (Finkel et al., 2005), with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date. Then we calculated a per-class overlap measure (in this way, “France” as an Organization does not match “France” as a Location): where Np and Nq are the sets of NEs found, respectively, in sentences p and q.",our experiments was set to the default value in the original model addition to the above text similarity measures we used also the following common measures the vectors of weights associated to sentences and the cosine distance is calculated as where is the distance between the sentences taking into account the characters Named Entity Overlap used the Stanford Named Entity Recognizer by with the class model trained for Time Location Organization Person Money Percent Date Then we calculated per class overlap measure where and are the sets of found respectively in sentences and
514,B,P05-1045,W06-1643,0,"(Finkel et al., 2005)","In the case of BNs, we write: We can reduce a particular skip-chain CRF to represent only the set of cliques along (yt−1, yt) adjacency edges and (yst , yt) skip edges, resulting in only two potential functions: Our CRF and BN models were designed using MALLET (McCallum, 2002), which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training dataD = (x(i),y(i)) i=1, and provides probabilistic inference algorithms for linear-chain BNs and CRFs. Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al., 2005). Approximation is needed when the junction tree of a graphical model is associated with prohibitively large cliques.",the case of we write can reduce particular skip chain to represent only the set of cliques along adjacency edges and skip edges resulting in only two potential functions Our and models were designed using which provides tools for training log linear models with optimization techniques and maximize the loglikelihood of our training and provides probabilistic inference algorithms for linear chain and Most previous work with containing nonlocal dependencies used approximate probabilistic inference techniques including and Gibbs sampling Approximation is needed when the junction tree of graphical model is associated with prohibitively large cliques
515,B,P05-1045,W06-1655,0,"(Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003)","We have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds. There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003). The current work indicates that these systems might be improved by moving to a semi-CRF model.",have presented the intuitive argument that the log odds may be advantageous because it does not exhibit the asymmetry of the log probability but it would be satisfying to justify the choice on more theoretical grounds There is significant volume of work exploring the use of for variety of chunking tasks including named entity recognition gene prediction shallow parsing and others The current work indicates that these systems might be improved by moving to semi model
516,B,P05-1045,W09-0422,1,"(Finkel et al., 2005)","Unlike in PDT 2.0, the information about the original syntactic form is stored with each tnode (values such asv:inf for an infinitive verb form, v:since+fin for the head of a subordinate clause of a certain type,adj:attr for an adjective in attribute position,n:for+X for a given prepositional group are distinguished). 6We used the full development set of 2k sentences for “Moses T” and a subset of 1k sentences for the other two setups due to time constraints. One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005). The nodes in the English t-layer are grouped according to the detected named entities and they are assigned the type of entity (location, person, or organization).",Unlike in the information about the original syntactic form is stored with each tnode used the full development set of sentences for Moses and subset of sentences for the other setups due to time constraints of the steps in the analysis of is named entity recognition using Stanford Named Entity Recognizer The nodes in the layer are grouped according to the detected named entities and they are assigned the type of entity
517,B,P05-1045,W09-1119,2,"(Finkel et al., 2005)","We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features. The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005). Our goal was to compare the performance of the taggers across several datasets.",have downloaded the tagger and used the strongest provided model trained on the data with distributional similarity features The results we obtained on the test set were consistent with what was reported in Our goal was to compare the performance of the taggers across several datasets
518,B,P05-1045,W09-1218,0,"(Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007)","We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information.",explore different approach to deal with such information using global features Use of global features for structured prediction problem has been explored by several applications such as sequential labeling and dependency parsing with great deal of success attempt to use global features for argument classification in which the most plausible semantic role assignment is selected using both local and global information
519,B,P05-1045,W10-0725,1,"(Finkel et al., 2005)","We give Turkers one example as a guide along with the instructions. The texts we use in our experiments are the development set of the RTE-5 challenge (Bentivogli et al., 2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al., 2005). In all, it contains 600 T-H pairs, and we use the texts to generate facts and counter-facts and hypotheses as references.",give Turkers example as guide along with the instructions The texts we use in our experiments are the development set of the challenge and we preprocess the data using the named entity recognizer all it contains pairs and we use the texts to generate facts and counter facts and hypotheses as references
520,B,P05-1045,W10-1902,0,Finkel et al. (2005),"Huang et al. (2007) combines a linear-chain CRF and two SVM models to enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction. However, the CRF models used in these systems were all linear-chain CRFs.",combines linear chain and models to enhance the recall used Gibbs Sampling to add non local dependencies into linear chain model for information extraction However the models used in these systems were all linear chain
521,B,P05-1045,W10-2906,1,"(Finkel et al., 2005)","Neither of these data sets were annotated with named entities, so we manually annotated 200 sentences from each of them. We used the Stanford NER tagger (Finkel et al., 2005) with its default configuration as our full monolingual model for each language. We weakened both the English and German models by removing several non-lexical and word-shape features.",Neither of these data sets were annotated with named entities so we manually annotated sentences from each of them used the tagger with its default configuration as our full monolingual model for each language weakened both the and models by removing several non lexical and word shape features
522,B,P05-1045,W10-3102,2,"(Finkel et al., 2005)","In the cases where the two student annotators agreed on the annotation, that annotation was chosen for the final corpus. In the cases where they did not agree, an annotation made by the chief annotator was chosen. 2.3 The Stanford NER based on CRF The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Conditional Random Fields (Finkel et al., 2005) and has been used extensively for identifying named entities in news text. For example in the CoNLL-2003, where the topic was language-independent named entity recognition, Stanford NER CRF was used both on English and German news text for training and evaluation.",the cases where the student annotators agreed on the annotation that annotation was chosen for the final corpus the cases where they did not agree an annotation made by the chief annotator was chosen The based on The Named Entity Recognizer is based on the machine learning algorithm Conditional Random Fields and has been used extensively for identifying named entities in news text For example in the where the topic was language independent named entity recognition was used both on and news text for training and evaluation
523,B,P05-1045,W11-0219,1,"(Finkel et al., 2005)","To support  more robust processing as well as domain configurability, the core system is informed by a variety of statistical and symbolic preprocessors. These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger (Toutanova and Manning, 2000), the Stanford named-entity recognizer (NER) (Finkel et al., 2005) and the Stanford Parser (Klein and Manning, 2003). The output of these and other specialized preprocessors (such as a street address recognizer) are sent  to the parser as advice.",support more robust processing as well as domain configurability the core system is informed by variety of statistical and symbolic preprocessors These include several off the shelf statisical tools such as the tagger the named entity recognizer and the Parser The output of these and other specialized preprocessors are sent to the parser as advice
524,B,P05-1045,W11-0810,2,"(Finkel et al., 2005)","We compared three state-of-the-art NER taggers: one from Stanford University (henceforth, Stanford tagger), one from the University of Illinois (henceforth, the LBJ tagger) and BBN IdentiFinder (henceforth, IdentiFinder). The Stanford Tagger is based on Conditional Random Fields (Finkel et al., 2005). It was trained on 100 million words from the English Gigawords corpus.",compared state of the art taggers from University from the University of Illinois and IdentiFinder The is based on Conditional Random Fields was trained on words from the English Gigawords corpus
525,B,P05-1045,W11-0902,1,"(Finkel et al., 2005)","Note however that the domain customization discussion in Section 5 is independent of the system architecture or classifiers used for EMD and RMD, and we expect the proposed ideas to apply to other IE approaches as well. We performed all pre-processing (tokenization, part-of-speech (POS) tagging) with the Stanford CoreNLP toolkit.2 For EMD we used the Stanford named entity recognizer (Finkel et al., 2005). In all our experiments we used a generic set of features (“macro”) and the IO notation3 for entity mention labels (e.g., the labels for the tokens “over the Seattle Seahawks on Sunday” (from Figure 1) are encoded as “O O NFLTEAM NFLTEAM O DATE”). 3The IO notation facilitates faster inference than the IOB or IOB2 notations with minimal impact on performance, when there are fewer adjacent mentions with the same type. – Head words of the two arguments and their combination – Entity mention labels of the two arguments and their combination – Sequence of dependency labels in the dependency path linking the heads of the two arguments – Lemmas of all words in the dependency path – Syntactic path in the constituent parse tree between the largest constituents headed by the same words as the two arguments (similar to Gildea and Jurafsky (2002)) The RMD model was built from scratch as a multi-class classifier that extracts binary relations between entity mentions in the same sentence.",Note however that the domain customization discussion in is independent of the system architecture or classifiers used for and and we expect the proposed ideas to apply to other approaches as well performed all pre processing tokenization part of speech tagging with the For we used the named entity recognizer all our experiments we used generic set of features and the for entity mention labels the labels for the tokens over the Seattle Seahawks on are encoded as notation facilitates faster inference than the or notations with minimal impact on performance when there are fewer adjacent mentions with the same type Head words of the arguments and their combination Entity mention labels of the arguments and their combination Sequence of dependency labels in the dependency path linking the heads of the arguments of all words in the dependency path path in the constituent parse tree between the largest constituents headed by the same words as the arguments The model was built from scratch as multi class classifier that extracts binary relations between entity mentions in the same sentence
526,B,P05-1045,W11-1907,1,"(Finkel et al., 2005)","These annotations are created by a pipeline of preprocessing components. We use theStanford MaxentTagger (Toutanova et al., 2003) for partof-speech tagging, and theStanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities. In order to derive syntactic information, we use theCharniak/Johnson reranking parser (Charniak and Johnson, 2005) combined with a constituent-to-dependency conversion Tool ( treebank_converter ).",These annotations are created by pipeline of preprocessing components use theStanford MaxentTagger for partof speech tagging and theStanford Named Entity Recognizer for annotating named entities order to derive syntactic information we use reranking parser combined with constituent to dependency conversion Tool
527,B,P05-1045,W11-1908,1,"(Finkel et al., 2005)","First, we create a list ofcandidate mentionsby merging basic NP chunks with named entities. NP chunks are computed from the parse trees provided in the CoNLL distribution, Named entities are extracted with the Stanford NER tool (Finkel et al., 2005). For each candidate mention, we store it minimal and maximal span.",we create list ofcandidate mentionsby merging basic chunks with named entities chunks are computed from the parse trees provided in the distribution Named entities are extracted with the tool For each candidate mention we store it minimal and maximal span
528,B,P05-1045,W11-2202,1,"(Finkel et al., 2005)","Data Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people. We then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens (these were usually errors).",Data Evaluation was performed on corpus of blogs describing United States politics in ran the Stanford Named Entity Recognition system to obtain set of candidate mentions which the system judged to be names of people then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens
529,B,P05-1045,W11-2705,1,"(Finkel et al., 2005)","For question classification, we used Li and Roth (2002) taxonomy and a machine learning-based classifier fed with features derived from a rule-based classifier (Silva et al., 2011). For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford’s Conditional Random-Field-based named entity recognizer (Finkel et al., 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION. For the generation of questions we used the top 16 documents retrieved by the Google for 9 personalities from several domains, like literature (e.g., Jane Austen) and politics (e.g., Adolf Hitler).",For question classification we used taxonomy and machine learning based classifier fed with features derived from rule based classifier For the learning of patterns we used the top documents retrieved by and to recognize the named entities in the pattern we apply several strategies namely the Stanford Conditional Random Field based named entity recognizer to detect entities of type regular expressions to detect and type entities gazetteers to detect entities of type For the generation of questions we used the top documents retrieved by the for personalities from several domains like literature and politics
530,B,P05-1045,W12-0102,1,"(Finkel et al., 2005)","If more named entities cooccur in two documents, they are very likely to talk about the same event or subject and thus should be more comparable. We use Stanford named entity recognizer7 to extract named entities from the texts (Finkel et al., 2005). Again, cosine is then applied to measure the similarity of named entities (denoted by WN ) between a document pair.",more named entities cooccur in documents they are very likely to talk about the same event or subject and thus should be more comparable use named entity to extract named entities from the texts Again cosine is then applied to measure the similarity of named entities between document pair
531,B,P05-1045,W12-0103,1,"(Finkel et al., 2005)","In order to obtain the Level2 representation of these corpora, the documents and the test sets must be annotated. For the annotation pipeline we use the TnT POS tagger (Brants, 2000), WordNet (Fellbaum, 1998), the YamCha chunker (Kudo and Matsumoto, 2003), the Stanford NERC (Finkel et al., 2005), and an in-house temporal expressions recogniser. Table 2 shows some statistics for the parallel corpus and the two different levels of annotation.",order to obtain the representation of these corpora the documents and the test sets must be annotated For the annotation pipeline we use the tagger WordNet the chunker the and an in house temporal expressions recogniser Table shows some statistics for the parallel corpus and the different levels of annotation
532,B,P05-1045,W12-0604,1,"(Finkel et al., 2005)","For the multi-class experiment, we use the 350 blogs corresponding to the 7 categories. Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recognizer (Finkel et al., 2005), which labels the entities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous. After several tests, we found that Location, Organization, Person and Miscellaneous were the most useful for topic classification, and we thus ignored the rest for the results presented here.",For the multi class experiment we use the blogs corresponding to the categories Both the blogs and the articles were tagged using the Stanford Named Entity Recognizer which labels the entities according to these types Time Location Organization Person Money Percent Date and Miscellaneous After several tests we found that Location Organization Person and Miscellaneous were the most useful for topic classification and we thus ignored the rest for the results presented here
533,B,P05-1045,W12-2007,1,"(Finkel et al., 2005)","We use the pattern of predicate matches against the TextRunner database to assess the degree and the equivocality of the connection between NE and NP. We use the Stanford Named Entity Recognizer (Finkel et al., 2005) that tags named entities as people, locations, organizations, and miscellaneous. We annotated a sample of 90 essays for named entities; the sample yielded 442 tokens, which we classified as shown in Table 2.",use the pattern of predicate matches against the database to assess the degree and the equivocality of the connection between and use the Stanford Named Entity Recognizer that tags named entities as people locations organizations and miscellaneous annotated sample of essays for named entities the sample yielded tokens which we classified as shown in Table
534,B,P05-1045,W12-2408,1,"(Finkel et al., 2005)","NER has long been studied by the research community and many different approaches have been developed (Tjong Kim Sang and De Meulder, 2003; Doddington et al., 2004). One successful and freely available named entity recognizer is the Stanford NER system (Finkel et al., 2005), which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents. As already mentioned, we first selected and ran several existing de-identification and NER systems detecting person names in our clinical documents.",has long been studied by the research community and many different approaches have been developed successful and freely available named entity recognizer is the system which provides an implementation of linear chain sequence models coupled with well engineered feature extractors for and trained with newswire documents already mentioned we selected and ran several existing de identification and systems detecting person names in our clinical documents
535,B,P05-1045,W12-2903,1,"(Finkel et al., 2005)","Each dialog was processed using the Stanford Core NLP tools. The Stanford tools perform part of speech tagging (Toutanova et al., 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al., 2005), and coreference resolution (Lee et al., 2011). From the output of the Stanford tools, the following features were extracted for each utterance: word bigrams (pairs of adjacent words); dependency-head relations, along with the type of dependency relation (basically, governors — e.g., verbs — and their dependents — e.g., nouns); named entities (persons, organizations, etc.); and the whole utterance.",Each dialog was processed using the Core tools The tools perform part of speech tagging constituent and dependency parsing named entity recognition and coreference resolution From the output of the tools the following features were extracted for each utterance word bigrams dependency head relations along with the type of dependency relation named entities and the whole utterance
536,B,P05-1045,W12-3109,1,"(Finkel et al., 2005)","Another aspect that might pose a potential problem to MT is the occurrence of words that were only observed a few times or in very particular contexts, as it is often the case for Named Entities. We used the Stanford NER Tagger (Finkel et al., 2005) to detect words that belong to one of four groups: Person, Location, Organization and Misc. Each group is represented by a binary feature.",Another aspect that might pose potential problem to is the occurrence of words that were only observed few times or in very particular contexts as it is often the case for Named used the to detect words that belong to of groups Person Location Organization and Misc Each group is represented by binary feature
537,B,P05-1045,W13-0905,1,"(Finkel et al., 2005)","The selectional restrictions, however, are not linked to any lexicons so a mapping was constructed in order to allow for automated detection of preference violations. Our first experiment utilizes WN, VN, and the Stanford Parser (de Marneffe et al., 2006) and Named Entity Recognizer (Finkel et al., 2005). The Stanford Parser identifies the verbs, as well as their corresponding subjects and direct objects.",The selectional restrictions however are not linked to any lexicons so mapping was constructed in order to allow for automated detection of preference violations Our experiment utilizes and the and Named Entity Recognizer The identifies the verbs as well as their corresponding subjects and direct objects
538,B,P05-1045,W13-1101,0,"(e.g., Finkel et al., 2005)","In general, language models could be used for more context-sensitive spelling correction. Given the preponderance of terms on the web, using a named entity recognizer (e.g., Finkel et al., 2005) for preprocessing may also provide benefits. We would like to thank Giuseppe Attardi for his help in using DeSR; Can Liu, Shoshana Berleant, and the IU CL discussion group for discussion; and the three anonymous reviewers for their helpful comments.",general language models could be used for more context sensitive spelling correction Given the preponderance of terms on the web using named entity recognizer for preprocessing may also provide benefits would like to thank Attardi for his help in using Can Liu Shoshana Berleant and the discussion group for discussion and the anonymous reviewers for their helpful comments
539,B,P05-1045,W13-2234,1,"(Finkel et al., 2005)","Semantic. We extract further information indicating whether a named entity, as identified by the Stanford NE Recognizer (Finkel et al., 2005) begins at wi. These features are relevant as there 5Realization of the classes D and N as lexical items is straightforward.",Semantic extract further information indicating whether named entity as identified by the begins at wi These features are relevant as there of the classes and as lexical items is straightforward
540,B,P05-1045,W13-2414,0,"(Finkel et al., 2005)","We have evaluated two schemas with a limited number of the NE categories. In the first more common (Finkel et al., 2005) schema, all PNs are divided into four MUC categories, i.e. person, organization, location and other. In the other schema, assuming a separate phases for PN recognition and classification (Al-Rfou’ and Skiena, 2012), we mapped all the PN categories to a single category, namely NAM.",have evaluated schemas with limited number of the categories the more common schema all are divided into categories person organization location and other the other schema assuming separate phases for recognition and classification we mapped all the categories to single category namely
541,B,P05-1045,W13-3516,1,"(Finkel et al., 2005)","In addition to automatically predicting the arguments, we also trained the IMS system to tag PropBank frameset IDs. English BC 1671 80.17 77.20 78.66 BN 2180 88.95 85.69 87.29 MZ 1161 82.74 82.17 82.45 NW 4679 86.79 84.25 85.50 TC 362 74.09 61.60 67.27 WB 1133 77.72 68.05 72.56 Overall 11186 84.04 80.86 82.42 Chinese BC 667 72.49 58.47 64.73 BN 3158 82.17 71.50 76.46 NW 1453 86.11 76.39 80.96 MZ 1043 65.16 56.66 60.62 TC 200 48.00 60.00 53.33 WB 886 80.60 51.13 62.57 Overall 7407 78.20 66.45 71.85 4.4 Named Entities We retrained the Stanford named entity recognizer20 (Finkel et al., 2005) on the OntoNotes data. Table 6 shows the performance details for all the languages across all 18 name types broken down by genre.",addition to automatically predicting the arguments we also trained the system to tag PropBank frameset Overall Chinese Overall Named Entities retrained the named entity on the data Table shows the performance details for all the languages across all name types broken down by genre
542,B,P05-1045,W13-4008,1,"(Finkel et al., 2005)","For example, ‘Joker’,‘Clarke Kent’ are related to ‘Batman’ and ‘Darth Vader’, ‘Yoda’ to ‘Star Wars’. To extract the extended targets, we capture named entities (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer (Finkel et al., 2005) and sort them based on their page occurrence count. Out of top-k (k = 20) NEs, some can belong to both of the debate topics.",For example Joker Clarke Kent are related to and Darth Vader Yoda to Star Wars extract the extended targets we capture named entities from the Wikipedia page of the debate topic using the Stanford Named Entity Recognizer and sort them based on their page occurrence count Out of top some can belong to both of the debate topics
543,A,N04-4028,N06-1038,2,Culotta and McCallum (2004) ,Culotta and McCallum (2004) describe the constrained forward-backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF.,describe the constrained forward backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a
544,A,P09-1113,P12-1076,3,Mintz et al. (2009),"Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base.",Following  we carried out our experiments using as the target corpus and Freebase  as the knowledge base
545,A,P96-1025,E06-1012,2,Collins (1996),"The distance between the dependent units plays an important role in the computation of the depen dency probabilities. Collins (1996) employs this
distance in the computation of word-to word dependency probabilities.",The distance between the dependent units plays an important role in the computation of the dependency probabilities.  employs this distance in the computation of word to word dependency probabilities
546,A,W02-1001,E06-1011,2, Collins (2002),"This algorithm can thus be
viewed as a large-margin version of the perceptron
algorithm for structured outputs Collins (2002).",This algorithm can thus be viewed as a large margin version of the perceptron algorithm for structured outputs
547,B,P05-1045,D13-1159,2,"(Finkel et al., 2005)","Two ER (i.e., ours and Freebase) are employed in entity extraction for comparison. In addition, we compare our approach with Stanford NER (Finkel et al., 2005) and fine-grained entity recognition (FIGER) (Ling and Weld, 2012).","Two  are employed in entity extraction for comparison, in addition, we compare our approach with Stanford NER  and fine grained entity recognition"
548,A,W96-0213,W03-0424,2,Ratnaparkhi (1996),The ME tagger is based on Ratnaparkhi (1996)s POS tagger and is described in Curran and Clark (2003) .,tagger is based on  tagger and is described in  
549,A,W99-0613,P01-1008,3,"(Collins and Singer, 1999)","Our model is based on the DLCoTrain algorrithm proposed by (Collins and Singer, 1999), which applies a co-training procedure to decision list classifiers for two independent sets of features.",Our model is based on the algorrithm proposed by  which applies a co training procedure to decision list classifiers for two independent sets of features
550,A,P05-1045,P06-1089,2,"(Chieu and Ng, 2002; Finkel et al., 2005)","Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (Chieu and Ng, 2002; Finkel et al., 2005).",Global information is known to be useful in other  tasks especially in the named entity recognition task  and several studies successfully used global features
551,B,P05-1045,P06-1089,2,"(Chieu and Ng, 2002; Finkel et al., 2005)","Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (Chieu and Ng, 2002; Finkel et al., 2005).",Global information is known to be useful in other  tasks especially in the named entity recognition task  and several studies successfully used global features
